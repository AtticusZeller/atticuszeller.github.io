{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"HomeHi there, I'm Atticus Zeller \ud83d\udc4b","text":"<p>\ud83d\udd2d I'm currently working on computer vision, deep learning, and 3D-reconstruction research projects.</p> <p>\ud83c\udf31 Currently completing my undergraduate studies and preparing for postgraduate studies in Artificial Intelligence.</p> <p>\ud83d\udee0\ufe0f Tech Stack:</p> <ul> <li>Programming languages: Python | C++</li> <li>Frontend: Streamlit</li> <li>Backend: FastAPI | SQL databases</li> <li>DevOps: Linux | Shell | Git | Docker</li> <li>Deep Learning: PyTorch | NumPy | Weights &amp; Biases</li> </ul> <p>\u26a1 Fun fact: I'm also an avid reader and a sports aficionado</p> <p>\ud83d\udceb How to reach me:</p> <ul> <li>Email: hello@atticux.me</li> </ul> <p>\ud83c\udf93 Academic Profile:</p> <ul> <li>ORCID: https://orcid.org/0009-0008-5460-325X</li> <li>GSplatLoc: Ultra-Precise Camera Localization via 3D Gaussian Splatting</li> </ul>"},{"location":"Backend/Docker/","title":"Docker","text":""},{"location":"Backend/Docker/#install-docker-and-compose","title":"Install Docker and Compose","text":"<pre><code># Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl\nsudo install -m 0755 -d /etc/apt/keyrings\nsudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\nsudo chmod a+r /etc/apt/keyrings/docker.asc\n# Add the repository to Apt sources:\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\") stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n# install docker engine\nsudo apt update\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> <pre><code># tests\nsudo docker run hello-world\ndocker compose version\n</code></pre>"},{"location":"Backend/Docker/#terminology","title":"Terminology","text":"<ul> <li>Containers Created from Docker images and run the actual application. We create a container using <code>docker run</code>. A list of running containers can be seen using the <code>docker ps</code> command.</li> <li>Images The blueprints of our application which form the basis of containers.</li> <li>Docker Daemon The background service running on the host that manages building, running and distributing Docker containers. The daemon is the process that runs in the operating system which clients talk to.</li> <li>Docker Client The command line tool that allows the user to interact with the daemon. More generally, there can be other forms of clients too - such as docker-desktop which provide a GUI to the users.</li> <li>Docker Hub A registry of Docker images. You can think of the registry as a directory of all available Docker images. If required, one can host their own Docker registries and can use them for pulling images.</li> <li>The <code>TAG</code> refers to a particular snapshot of the image and the <code>IMAGE ID</code> is the corresponding unique identifier for that image.</li> </ul>"},{"location":"Backend/Docker/#docker-image","title":"Docker Image","text":"<p>An important distinction to be aware of when it comes to images is the difference between base and child images.</p> <ul> <li>Base images are images that have no parent image, usually images with an OS like ubuntu, busybox or debian.</li> <li>Child images are images that build on base images and add additional functionality.</li> </ul> <p>Then there are official and user images, which can be both base and child images.</p> <ul> <li>Official images are images that are officially maintained and supported by the folks at Docker. These are typically one word long. In the list of images above, the <code>python</code>, <code>ubuntu</code>, <code>busybox</code> and <code>hello-world</code> images are official images.</li> <li>User images are images created and shared by users like you and me. They build on base images and add additional functionality. Typically, these are formatted as <code>user/image-name</code>.</li> </ul>"},{"location":"Backend/Docker/#image-layers","title":"Image Layers","text":"<p>container images are composed of layers. And each of these layers, once created, are immutable.  Each layer in an image contains a set of file system changes - additions, deletions, or modifications.</p> <p> This is beneficial because it allows layers to be reused between images. Layers let you extend images of others by reusing their base layers, allowing you to add only the data that your application needs.</p> <p>The new layer is able to be stacked by <code>Dockerfile</code> or <code>docker container commit -m &lt;container&gt; &lt;image&gt;</code> manually. And <code>docker image history &lt;image&gt;</code> offers image layers info.<sup>1</sup></p> <pre><code>IMAGE          CREATED              CREATED BY                               SIZE      COMMENT\nc1502e2ec875   About a minute ago   /bin/bash                                33B       Add app\n5310da79c50a   4 minutes ago        /bin/bash                                126MB     Add node\n2b7cc08dcdbb   5 weeks ago          /bin/sh -c #(nop)  CMD [\"/bin/bash\"]            0B\n&lt;missing&gt;      5 weeks ago          /bin/sh -c #(nop) ADD file:07cdbabf782942af0\u00e2\u0080\u00a6   69.2MB\n&lt;missing&gt;      5 weeks ago          /bin/sh -c #(nop)  LABEL org.opencontainers.\u00e2\u0080\u00a6   0B\n&lt;missing&gt;      5 weeks ago          /bin/sh -c #(nop)  LABEL org.opencontainers.\u00e2\u0080\u00a6   0B\n&lt;missing&gt;      5 weeks ago          /bin/sh -c #(nop)  ARG LAUNCHPAD_BUILD_ARCH     0B\n&lt;missing&gt;      5 weeks ago          /bin/sh -c #(nop)  ARG RELEASE                  0B\n</code></pre>"},{"location":"Backend/Docker/#dockerfile","title":"Dockerfile","text":"<p>A Dockerfile is a simple text file that contains a list of commands that the Docker client calls while creating an \\(child\\) image.</p> <p><code>FROM &lt;image&gt;</code> - this specifies the base image that the build will extend.</p> <pre><code># base image from bookworm=debian12 with slim python\nFROM python:3.12-slim-bookworm\n</code></pre> <p><code>WORKDIR &lt;path&gt;</code> - this instruction specifies the \"working directory\" or the path in the image where files will be copied and commands will be executed.</p> <pre><code># set a directory for the app\nWORKDIR /app\n</code></pre> <p><code>COPY &lt;host-path&gt; &lt;image-path&gt;</code> - this instruction tells the builder to copy files from the host and put them into the container image.</p> <pre><code># copy all the files to the $(pwd)\nCOPY . .\n</code></pre> <p><code>ENV &lt;name&gt; &lt;value&gt;</code> - this instruction sets an environment variable that a running container will use.</p> <pre><code># Ensure virtual environment binaries are in PATH\uff1a https://docs.astral.sh/uv/guides/integration/docker/#using-the-environment\nENV PATH=\"/app/.venv/bin:$PATH\"\n</code></pre>"},{"location":"Backend/Docker/#shell-and-exec-form","title":"Shell and exec form","text":"<p>The <code>RUN</code>, <code>CMD</code>, and <code>ENTRYPOINT</code> instructions all have two possible forms:</p> <ul> <li><code>INSTRUCTION [\"executable\",\"param1\",\"param2\"]</code> (exec form)</li> <li><code>INSTRUCTION command param1 param2</code> (shell form)</li> </ul> <p>Note</p> <ul> <li>The exec form makes it possible to avoid shell string munging, because it is parsed as a JSON array.</li> <li>The shell form always use a command shell and is parsed as a regular string, so it's more relaxed and inherits shell feature directly.</li> </ul> <p>for example, <code>$HOME</code> can not be parsed as string <code>\"$HOME\"</code> but replaced by shell.  on the contrary, we need to specify a command shell, or any other executable while using environment variables</p> <pre><code>ENTRYPOINT [\"/bin/bash\", \"-c\", \"echo hello\"]\n</code></pre> <p>Tip</p> <ul> <li>The Exec form is best used to specify <code>ENTRYPOINT</code> and <code>CMD</code> instructions for settings of default arguments that can be overridden at runtime.And it also stops container quicker while running <code>docker compose stop</code>.</li> <li>The Shell form is most commonly used in <code>RUN</code> command, it lets you easier break long command in multiple lines with backslash</li> </ul> <pre><code>RUN source $HOME/.bashrc &amp;&amp; \\\necho $HOME\n</code></pre> <p><code>RUN [OPTIONS] &lt;command&gt;</code> - this instruction tells the builder to run the specified command. and creates a new layer on top of current layer. The available <code>[OPTIONS]</code> is helpful for accelerating the building process.</p> <pre><code># Install dependencies\n# Ref: https://docs.astral.sh/uv/guides/integration/docker/#intermediate-layers\nRUN --mount=type=cache,target=/root/.cache/uv \\\n--mount=type=bind,source=uv.lock,target=uv.lock \\\n--mount=type=bind,source=pyproject.toml,target=pyproject.toml \\\nuv sync --frozen --no-install-project\n</code></pre> <p><code>ENTRYPOINT [\"executable\", \"param1\", \"param2\"]</code> - it allows you to configure a container that run as executable. command line arguments to <code>docker run &lt;image&gt;</code> will be appended after all elements in an exec form of <code>ENTRYPONIT</code>, and will override all elements specified using <code>CMD</code>, so will the commands in <code>compose.yml</code>.</p> <p><code>CMD</code> instruction sets the command to be executed when running a container from an image. - <code>CMD [\"executable\",\"param1\",\"param2\"]</code> (exec form) - <code>CMD [\"param1\",\"param2\"]</code> (exec form, as default parameters to <code>ENTRYPOINT</code>)</p> <p>Tip</p> <p>we primarily use <code>CMD</code> to . Because it's much clearer for us to override it while debugging. Nobody wanna see something ugly like: <pre><code>services:\n  backend:\n      restart: \"no\"\n      ports:\n        - \"8000:8000\"\n      build:\n        context: ./backend\n      command:\n        - run\n        - --reload\n        - \"app/main.py\"\n</code></pre> there is no head of the <code>command</code>, quite scary.</p>"},{"location":"Backend/Docker/#docker-build","title":"Docker Build","text":"<p>This username should be the same one you created when you registered on Docker hub. it takes an optional tag name with <code>-t</code> and a location of the directory containing the <code>Dockerfile</code>.</p> <pre><code>docker build -t yourusername/hello-world .\n</code></pre> <p>copy <code>.gitignore</code> to <code>.dockerignore</code> to slim the <code>/app</code></p>"},{"location":"Backend/Docker/#cache","title":"Cache","text":"<p>Using the build cache</p> <p>Using the build cache effectively lets you achieve faster builds by reusing results from previous builds and skipping unnecessary work. - Any changes to the command of a <code>RUN</code> instruction invalidates that layer. - Any changes to files copied into the image with the <code>COPY</code> or <code>ADD</code> instructions. - Once one layer is invalidated, all following layers are also invalidated. <sup>2</sup></p> <p>How it works?</p> <pre><code># syntax=docker/dockerfile:1\nFROM ubuntu:latest\n\nRUN apt-get update &amp;&amp; apt-get install -y build-essentials\nCOPY main.c Makefile /src/\nWORKDIR /src/\nRUN make build\n</code></pre> <p>Each instruction in this Dockerfile translates to a layer in your final image.</p> <p>If a layer changes, all other layers that come after it are also affected. </p> <p>Best practice ?</p> <p>we hope the layer frequently modified to be last one,which aims to prevent to take much time on rebuilding the other layers such as installing dependency.</p> <p>Intermediate layers and template</p>"},{"location":"Backend/Docker/#multi-stage-builds","title":"Multi-stage Builds","text":"<p>For compiled languages,like C or Go or Rust, multi-stage builds let you compile in one stage and copy the compiled binaries into a final runtime image. No need to bundle the entire compiler in your final image.<sup>3</sup></p> <pre><code># Stage 1: Build Environment\nFROM builder-image AS build-stage\n# Install build tools (e.g., Maven, Gradle)\n# Copy source code\n# Build commands (e.g., compile, package)\n\n# Stage 2: Runtime environment\nFROM runtime-image AS final-stage\n#  Copy application artifacts from the build stage (e.g., JAR file)\nCOPY --from=build-stage /path/in/build/stage /path/to/place/in/final/stage\n# Define runtime configuration (e.g., CMD, ENTRYPOINT)\n</code></pre>"},{"location":"Backend/Docker/#docker-push","title":"Docker Push","text":"<p>If this is the first time you are pushing an image, the client will ask you to login. Provide the same credentials that you used for logging into Docker Hub.</p> <pre><code>docker login\n</code></pre> <p>It is important to have the format of <code>yourusername/image_name</code> so that the client knows where to publish.</p> <pre><code>docker push yourusername/hello-world\n</code></pre> <p>Now that your image is online, anyone who has docker installed can play with your app by typing just a single command.</p>"},{"location":"Backend/Docker/#docker-container","title":"Docker Container","text":""},{"location":"Backend/Docker/#run","title":"Run","text":"<p><code>docker run</code> is aliases of <code>docker container run [OPTIONS] IMAGE [COMMAND] [ARG\u2026]</code>,which defines how to create and run a new container from image.</p> <p>Important</p> <p>While <code>docker run</code> is a convenient tool for launching container,it becomes difficult to manage a growing application stack with it. That's where <code>Docker Compose</code> comes to rescue.</p>"},{"location":"Backend/Docker/#network","title":"Network","text":""},{"location":"Backend/Docker/#bridge","title":"Bridge","text":"<p>When docker is installed, it creates three networks automatically.</p> <pre><code>docker network ls\nNETWORK ID     NAME      DRIVER    SCOPE\n1f50a9c659fd   bridge    bridge    local\naf3e60c1b1c0   host      host      local\n534df30aa391   none      null      local\n</code></pre> host and none network <p>The latter two are not fully-fledged networks, but are used to start a container connected directly to the Docker daemon host's networking stack, or to start a container with no network devices. This tutorial will connect two containers to the <code>bridge</code> network.<sup>4</sup></p> <p>if you have not specified any <code>--network</code> flags, the containers connect to the default <code>bridge</code> network, which can not resolve a container name to an IP address,but it works for user-defined networks,<sup>5</sup>and containers will connect to the same user-defined networks bridge which created by docker compose.</p> <p>Note</p> <p>Automatic service discovery can only resolve custom container names, not default automatically generated container names<sup>5</sup> <code>docker run --name &lt;custom_container_name&gt;</code> == the service name of <code>compose.yml</code> <sup>6</sup></p> <p>docker container is designed as network isolation, publishing a port provides the ability to connect or communicate other containers or be accessed by host machine.</p> <pre><code>services:\n  app:\n    image: docker/welcome-to-docker\n    ports:\n      #- HOST_PORT:CONTAINER_PORT\n      - 8080:80\n</code></pre>"},{"location":"Backend/Docker/#host","title":"Host","text":"<p>If you use the <code>host</code> network mode for a container, that container's network stack isn't isolated from the Docker host (the container shares the host's networking namespace), and the container doesn't get its own IP-address allocated.<sup>7</sup></p>"},{"location":"Backend/Docker/#volume","title":"Volume","text":"<p>Volumes are a storage mechanism that provide the ability to persist data beyond the lifecycle of an individual container.<sup>8</sup>so it can be shared by other or old containers.</p>"},{"location":"Backend/Docker/#sharing-files-between-a-host-and-container","title":"Sharing files between a host and container","text":"<pre><code>docker run -v /HOST/PATH:/CONTAINER/PATH -it nginx\n</code></pre>"},{"location":"Backend/Docker/#docker-compose","title":"Docker Compose","text":"<p><sup>9</sup> Docker Compose defines your entire multi-container application in a single <code>YAML</code> file called <code>commpose.yml</code>. The file specifies configuration for all your containers,their dependencies, environment variables, and even volumes and networks.<sup>10</sup></p>"},{"location":"Backend/Docker/#configuration","title":"Configuration","text":"<p>There are 3 ways for <code>docker compose</code> to read configs, merge, extend, include.,<sup>11</sup> merge configs is most widely used while developing, extend and include are suitable for multiple services or teams.</p> <p>Default behavior of <code>docker compose up</code>? Compose read 2 files, a <code>compose.yaml</code> and an optional <code>compose.override.yaml</code> file. By convention, the <code>compose.yaml</code> contains your base configuration. The override file can contain configuration overrides for existing services or entirely new services.<sup>12</sup></p> <p>Strategy Use a two-file approach for different environments</p> <ol> <li><code>compose.yml</code>: Main configuration for production</li> <li>Run with: <code>docker compose -f compose.yml up -d</code></li> <li><code>compose.override.yml</code>: Override file for development</li> <li>Automatically used with: <code>docker compose up -d</code></li> </ol> <p>This strategy allows you to maintain a base configuration for production while easily switching to a development setup with overrides.<sup>13</sup></p> <p>what if i don't like default behavior? To specify or merge multiple configs use <code>docker compose -f &lt;config_path&gt; up</code>.<sup>14</sup></p> <p>Note</p> <p>Paths are evaluated relative to the base file. When you use multiple Compose files, you must make sure all paths in the files are relative to the base Compose file (the first Compose file specified with <code>-f</code>).<sup>14</sup></p> <p>how to merge? precedence? - For single-value options like <code>image</code>, <code>command</code> or <code>mem_limit</code>, the new value replaces the old value. - For the multi-value\\(list\\) options <code>ports</code>, <code>expose</code>, <code>external_links</code>, <code>dns</code>, <code>dns_search</code>, and <code>tmpfs</code>, Compose concatenates both sets of values. - For key-value options <code>environment</code>, <code>labels</code>, <code>volumes</code>, and <code>devices</code>, Compose \"merges\" entries together with locally defined values taking precedence.<sup>15</sup></p> <p><code>compose.yaml</code> or <code>docker-compose.yaml</code>?</p> <p>The default path for a Compose file is <code>compose.yaml</code> (preferred) or <code>compose.yml</code> that is placed in the working directory. Compose also supports <code>docker-compose.yaml</code> and <code>docker-compose.yml</code> for backwards compatibility of earlier versions. If both files exist, Compose prefers the canonical <code>compose.yaml</code>.<sup>16</sup></p> <p>Dockerfile versus Compose file</p> <p>A Dockerfile provides instructions to build a container image while a Compose file defines your running containers. Quite often, a Compose file references a Dockerfile to build an image to use for a particular service.<sup>17</sup></p>"},{"location":"Backend/Docker/#project-name","title":"Project name","text":""},{"location":"Backend/Docker/#docker-commands","title":"Docker Commands","text":""},{"location":"Backend/Docker/#use-docker-by-user-without-sudo","title":"Use Docker by User without <code>sudo</code>","text":"<p>Post-installation steps | Docker Docs</p> <pre><code>sudo groupadd docker\nsudo usermod -aG docker $USER\nnewgrp docker\n# tests\ndocker run hello-world\n</code></pre>"},{"location":"Backend/Docker/#auto-start","title":"Auto Start","text":"<pre><code>sudo systemctl enable docker.service\nsudo systemctl enable containerd.service\n</code></pre>"},{"location":"Backend/Docker/#check-the-running-container","title":"Check the Running Container","text":"<pre><code># ps= process status,show running containers\ndocker ps\n# show containers that are running or exited\ndocker ps -a\n</code></pre>"},{"location":"Backend/Docker/#usage-and-rate-limits-docker-docs","title":"Usage and rate limits | Docker Docs","text":"<pre><code>docker login\n</code></pre>"},{"location":"Backend/Docker/#remove-containers","title":"Remove Containers","text":"<p>deletes all containers that have a status of `exited</p> <pre><code>docker container prune\n</code></pre>"},{"location":"Backend/Docker/#clean-up","title":"Clean up","text":"<pre><code>docker system prune -f\n</code></pre>"},{"location":"Backend/Docker/#remove-image","title":"Remove Image","text":"<pre><code>docker rmi &lt;image_id&gt;\n</code></pre>"},{"location":"Backend/Docker/#linter","title":"Linter","text":"<pre><code>brew install hadolint\n</code></pre> <ol> <li> <p>https://docs.docker.com/get-started/docker-concepts/building-images/understanding-image-layers/#stacking-the-layers\u00a0\u21a9</p> </li> <li> <p>https://docs.docker.com/get-started/docker-concepts/building-images/using-the-build-cache/\u00a0\u21a9</p> </li> <li> <p>https://docs.docker.com/get-started/docker-concepts/building-images/multi-stage-builds/#explanation)\u00a0\u21a9</p> </li> <li> <p>https://docs.docker.com/engine/network/tutorials/standalone/#use-the-default-bridge-network\u00a0\u21a9</p> </li> <li> <p>https://docs.docker.com/engine/network/tutorials/standalone/#use-user-defined-bridge-networks\u00a0\u21a9\u21a9</p> </li> <li> <p>https://docs.docker.com/compose/how-tos/networking/\u00a0\u21a9</p> </li> <li> <p>https://docs.docker.com/engine/network/drivers/host/\u00a0\u21a9</p> </li> <li> <p>https://docs.docker.com/get-started/docker-concepts/running-containers/persisting-container-data/\u00a0\u21a9</p> </li> <li> <p>https://docs.docker.com/compose/images/compose-application.webp\u00a0\u21a9</p> </li> <li> <p>https://docs.docker.com/get-started/docker-concepts/running-containers/multi-container-applications/#explanation\u00a0\u21a9</p> </li> <li> <p>https://docs.docker.com/compose/how-tos/multiple-compose-files/\u00a0\u21a9</p> </li> <li> <p>https://docs.docker.com/compose/how-tos/multiple-compose-files/merge/\u00a0\u21a9</p> </li> <li> <p>https://github.com/fastapi/full-stack-fastapi-template\u00a0\u21a9</p> </li> <li> <p>https://docs.docker.com/compose/how-tos/multiple-compose-files/merge/#how-to-merge-multiple-compose-files\u00a0\u21a9\u21a9</p> </li> <li> <p>https://docs.docker.com/compose/how-tos/multiple-compose-files/merge/#merging-rules\u00a0\u21a9</p> </li> <li> <p>https://docs.docker.com/compose/intro/compose-application-model/#the-compose-file\u00a0\u21a9</p> </li> <li> <p>https://docs.docker.com/get-started/docker-concepts/the-basics/what-is-docker-compose\u00a0\u21a9</p> </li> </ol>"},{"location":"Backend/FastApi/","title":"FastApi","text":"<p>The function parameters will be recognized as follows:</p> <ol> <li>If the parameter is also declared in the path, it will be used as a path parameter.</li> <li>If the parameter is of a singular type (like int, float, str, bool, etc) it will be interpreted as a query parameter.</li> </ol> <p>Info</p> <p>FastAPI will know that the value of q is not required because of the default value = None.</p> <ol> <li>If the parameter is declared to be of the type of a Pydantic model, it will be interpreted as a request body.</li> </ol>"},{"location":"Backend/DB/PostgreSQL/","title":"PostgreSQL","text":"<p>PostgreSQL Tutorial DBA Roadmap: Learn to become a database administrator with PostgreSQL</p>"},{"location":"Backend/DB/PostgreSQL/#initial-config","title":"Initial Config","text":""},{"location":"Backend/DB/PostgreSQL/#postgresql_1","title":"PostgreSQL","text":""},{"location":"Backend/DB/PostgreSQL/#install","title":"Install","text":"<p>Automated repository configuration:</p> <pre><code>sudo apt install -y postgresql-common\nsudo /usr/share/postgresql-common/pgdg/apt.postgresql.org.sh\n</code></pre> <pre><code># Update the package lists:\nsudo apt update\n\n# Install the latest version of PostgreSQL:\n# If you want a specific version, use 'postgresql-16' or similar instead of 'postgresql'\nsudo apt -y install postgresql-16\n</code></pre> <p>start/stop app</p> <pre><code>sudo systemctl start/stop postgresql\n</code></pre> <p>start/stop as start up</p> <pre><code>sudo systemctl enable/disable postgresql\n</code></pre>"},{"location":"Backend/DB/PostgreSQL/#configure-postgresql-server-remote","title":"Configure PostgreSQL Server (remote)","text":"<p>Install PostgreSQL on Linux (Ubuntu) set listening port as <code>listen_addresses = '*'</code></p> <pre><code>sudo nano /etc/postgresql/16/main/postgresql.conf\n</code></pre> <p>enable remote connections</p> <pre><code>sudo sed -i '/^host/s/ident/md5/' /etc/postgresql/16/main/pg_hba.conf sudo sed -i '/^local/s/peer/trust/' /etc/postgresql/16/main/pg_hba.conf echo \"host all all 0.0.0.0/0 md5\" | sudo tee -a /etc/postgresql/16/main/pg_hba.conf\nsudo systemctl restart postgresql\nsudo ufw allow 5432/tcp\n</code></pre>"},{"location":"Backend/DB/PostgreSQL/#connection","title":"Connection","text":"<p>switch to <code>postgres</code> user and get into <code>psql</code></p> <pre><code>sudo -u postgres psql\n</code></pre> <p>input your password then with <code>\\q</code> to quit</p> <pre><code>\\password postgres\n</code></pre> <p>Default User</p> <p>username: postgres pswd: postgres port: 5423 ip: localhost</p>"},{"location":"Backend/DB/PostgreSQL/#pgadmin","title":"pgAdmin","text":""},{"location":"Backend/DB/PostgreSQL/#install_1","title":"Install","text":"<p>a web-based tool to connect to the PostgreSQL server</p> <p>PostgreSQL Administration</p> <p>Setup the Repository</p> <pre><code># Install the public key for the repository (if not done previously):\ncurl -fsS https://www.pgadmin.org/static/packages_pgadmin_org.pub | sudo gpg --dearmor -o /usr/share/keyrings/packages-pgadmin-org.gpg\n\n# Create the repository configuration file:\nsudo sh -c 'echo \"deb [signed-by=/usr/share/keyrings/packages-pgadmin-org.gpg] https://ftp.postgresql.org/pub/pgadmin/pgadmin4/apt/$(lsb_release -cs) pgadmin4 main\" &gt; /etc/apt/sources.list.d/pgadmin4.list &amp;&amp; apt update'\n</code></pre> <p>Install pgAdmin</p> <pre><code># Install for both desktop and web modes:\nsudo apt install pgadmin4\n\n# Install for desktop mode only: for local development\nsudo apt install pgadmin4-desktop\n\n# Install for web mode only:  for remote server\nsudo apt install pgadmin4-web\n\n# Configure the webserver, if you installed pgadmin4-web:\nsudo /usr/pgadmin4/bin/setup-web.sh\n</code></pre> <p>Dark model <code>Preferences --&gt; User Interface --&gt; Themes --&gt; Dark</code></p> <p>Register Server Dialog \u2014 pgAdmin 4 8.11 documentation</p>"},{"location":"Backend/DB/Sqlite/","title":"Sqlite","text":""},{"location":"Backend/DB/ORM/SQLModel/","title":"SQLModel","text":"<p>Tutorial - User Guide - SQLModel</p>"},{"location":"Backend/Proxy/Traefik/","title":"Traefik","text":"<p>Traefik is an application proxy that routes requests to the right service.</p>"},{"location":"Language/C%2B%2B/Pointer/","title":"Pointer","text":""},{"location":"Language/C%2B%2B/Pointer/#c","title":"C++ \u667a\u80fd\u6307\u9488\u7b80\u4ecb","text":"<p>\u667a\u80fd\u6307\u9488\u662f\u73b0\u4ee3 C++ \u4e2d\u7528\u4e8e\u81ea\u52a8\u7ba1\u7406\u5185\u5b58\u7684\u5bf9\u8c61\uff0c\u901a\u8fc7\u5c01\u88c5\u539f\u59cb\u6307\u9488\uff0c\u5b83\u4eec\u5e2e\u52a9\u7a0b\u5e8f\u5458\u907f\u514d\u5185\u5b58\u6cc4\u6f0f\u5e76\u63d0\u4f9b\u5f02\u5e38\u5b89\u5168\u3002C++11 \u6807\u51c6\u5f15\u5165\u4e86\u51e0\u79cd\u667a\u80fd\u6307\u9488\u7c7b\u578b\uff0c\u4f7f\u5f97\u8d44\u6e90\u7ba1\u7406\u66f4\u52a0\u7b80\u5355\u548c\u5b89\u5168\u3002</p>"},{"location":"Language/C%2B%2B/Pointer/#_1","title":"\u667a\u80fd\u6307\u9488\u7c7b\u578b","text":"<ol> <li> <p><code>std::unique_ptr</code></p> </li> <li> <p>\u7279\u6027\uff1a\u63d0\u4f9b\u5bf9\u5185\u5b58\u7684\u72ec\u5360\u6240\u6709\u6743\uff0c\u5373\u540c\u4e00\u65f6\u95f4\u5185\u53ea\u6709\u4e00\u4e2a <code>std::unique_ptr</code> \u53ef\u4ee5\u62e5\u6709\u8d44\u6e90\u3002</p> </li> <li>\u5178\u578b\u7528\u9014\uff1a\u7528\u4e8e\u9700\u8981\u660e\u786e\u5355\u4e00\u6240\u6709\u6743\u7684\u60c5\u51b5\uff0c\u4f8b\u5982\u5728\u51fd\u6570\u5185\u90e8\u521b\u5efa\u4e34\u65f6\u5bf9\u8c61\u3002</li> <li>\u4e0d\u53ef\u590d\u5236\uff1a\u53ea\u80fd\u79fb\u52a8\uff0c\u786e\u4fdd\u6240\u6709\u6743\u7684\u552f\u4e00\u6027\u3002</li> <li> <p>\u5f15\u5165\u7248\u672c\uff1aC++11\u3002</p> </li> <li> <p><code>std::shared_ptr</code></p> </li> <li> <p>\u7279\u6027\uff1a\u652f\u6301\u591a\u91cd\u6240\u6709\u6743\u6982\u5ff5\uff0c\u5373\u591a\u4e2a <code>std::shared_ptr</code> \u5bf9\u8c61\u53ef\u4ee5\u5171\u4eab\u540c\u4e00\u4e2a\u8d44\u6e90\u3002</p> </li> <li>\u5178\u578b\u7528\u9014\uff1a\u7528\u4e8e\u5bf9\u8c61\u751f\u547d\u5468\u671f\u9700\u8981\u7531\u591a\u4e2a\u6240\u6709\u8005\u5171\u540c\u7ba1\u7406\u7684\u60c5\u51b5\uff0c\u4f8b\u5982\u5728\u591a\u4e2a\u6570\u636e\u7ed3\u6784\u4e4b\u95f4\u5171\u4eab\u6570\u636e\u3002</li> <li> <p>\u5f15\u5165\u7248\u672c\uff1aC++11\u3002</p> </li> <li> <p><code>std::weak_ptr</code></p> </li> <li>\u7279\u6027\uff1a\u63d0\u4f9b\u4e00\u79cd\u975e\u62e5\u6709\u6027\u7684\u667a\u80fd\u6307\u9488\uff0c\u7528\u6765\u89c2\u5bdf <code>std::shared_ptr</code>\uff0c\u4f46\u4e0d\u5f71\u54cd\u5176\u5f15\u7528\u8ba1\u6570\u3002</li> <li>\u5178\u578b\u7528\u9014\uff1a\u7528\u4e8e\u89e3\u51b3 <code>std::shared_ptr</code> \u53ef\u80fd\u5bfc\u81f4\u7684\u5faa\u73af\u5f15\u7528\u95ee\u9898\u3002</li> <li>\u5f15\u5165\u7248\u672c\uff1aC++11\u3002</li> </ol> <pre><code>#include &lt;iostream&gt;\n#include &lt;memory&gt;\n\nclass Child;\nclass Parent;\n\nclass Parent : public std::enable_shared_from_this&lt;Parent&gt; {\npublic:\n    std::shared_ptr&lt;Child&gt; child;\n    ~Parent() { std::cout &lt;&lt; \"Parent destroyed\\n\"; }\n\n    void setChild(std::shared_ptr&lt;Child&gt; c) {\n        child = c;\n        child-&gt;setParent(shared_from_this());  // \u4f7f\u7528 shared_from_this() \u83b7\u53d6\u5f53\u524d\u5bf9\u8c61\u7684 shared_ptr\n    }\n};\n\nclass Child {\npublic:\n    std::weak_ptr&lt;Parent&gt; parent;  // \u4f7f\u7528 weak_ptr \u907f\u514d\u5faa\u73af\u5f15\u7528\n    ~Child() { std::cout &lt;&lt; \"Child destroyed\\n\"; }\n\n    void setParent(std::shared_ptr&lt;Parent&gt; p) {\n        parent = p;  // \u8bbe\u7f6e parent \u4e3a\u4f20\u5165\u7684 shared_ptr\n    }\n};\n\nint main() {\n    std::shared_ptr&lt;Parent&gt; p = std::make_shared&lt;Parent&gt;();\n    std::shared_ptr&lt;Child&gt; c = std::make_shared&lt;Child&gt;();\n    p-&gt;setChild(c);\n\n    std::cout &lt;&lt; \"Parent use count: \" &lt;&lt; p.use_count() &lt;&lt; std::endl;  // \u8f93\u51fa\u4f1a\u662f 2\uff0c\u56e0\u4e3a child \u4e5f\u6301\u6709 parent \u7684 shared_ptr\n    std::cout &lt;&lt; \"Child use count: \" &lt;&lt; c.use_count() &lt;&lt; std::endl;   // \u8f93\u51fa 1\n}\n</code></pre>"},{"location":"Language/C%2B%2B/Pointer/#_2","title":"\u521b\u5efa\u548c\u4f7f\u7528\u667a\u80fd\u6307\u9488","text":"<ul> <li><code>std::make_shared</code></li> <li>\u63cf\u8ff0\uff1a\u7528\u4e8e\u6784\u9020 <code>std::shared_ptr</code> \u7684\u4f18\u9009\u65b9\u6cd5\uff0c\u56e0\u4e3a\u5b83\u53ef\u4ee5\u5728\u5355\u4e00\u64cd\u4f5c\u4e2d\u5206\u914d\u5185\u5b58\uff0c\u63d0\u9ad8\u6548\u7387\u5e76\u51cf\u5c11\u5f02\u5e38\u98ce\u9669\u3002</li> <li>\u5f15\u5165\u7248\u672c\uff1aC++11\u3002</li> <li><code>std::make_unique</code></li> <li>\u63cf\u8ff0\uff1a\u7528\u4e8e\u6784\u9020 <code>std::unique_ptr</code> \u7684\u65b9\u6cd5\u3002\u7c7b\u4f3c\u4e8e <code>std::make_shared</code>\uff0c\u4f46\u4e13\u4e3a <code>std::unique_ptr</code> \u8bbe\u8ba1\u3002</li> <li>\u5f15\u5165\u7248\u672c\uff1aC++14\u3002</li> <li><code>shared_from_this</code></li> <li>\u63cf\u8ff0\uff1a\u5f53\u7c7b\u7ee7\u627f\u81ea <code>std::enable_shared_from_this</code> \u65f6\uff0c\u4f60\u53ef\u4ee5\u5b89\u5168\u5730\u5728\u7c7b\u5185\u90e8\u83b7\u53d6\u4e00\u4e2a\u6307\u5411\u81ea\u5df1\u7684 <code>std::shared_ptr</code>\u3002</li> <li>\u5f15\u5165\u7248\u672c\uff1aC++11\u3002</li> </ul> <p><code>shared_from_this()</code> \u53ea\u80fd\u5728 <code>std::shared_ptr</code> \u5df2\u7ecf\u62e5\u6709\u8be5\u5bf9\u8c61\u65f6\u624d\u80fd\u5b89\u5168\u8c03\u7528\u3002\u5982\u679c\u5728\u4efb\u4f55 <code>std::shared_ptr</code> \u62e5\u6709\u5bf9\u8c61\u4e4b\u524d\u8c03\u7528\u5b83\uff0c\u7a0b\u5e8f\u5c06\u629b\u51fa <code>std::bad_weak_ptr</code> \u5f02\u5e38\u3002</p>"},{"location":"Language/C%2B%2B/Pointer/#_3","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>#include &lt;memory&gt;\n#include &lt;iostream&gt;\n\nclass MyClass : public std::enable_shared_from_this&lt;MyClass&gt; {\npublic:\n    void show() {\n        auto sharedPtr = shared_from_this();\n        std::cout &lt;&lt; \"MyClass instance has \" &lt;&lt; sharedPtr.use_count() &lt;&lt; \" references\\n\";\n    }\n};\n\nint main() {\n    auto myObject = std::make_shared&lt;MyClass&gt;();\n    myObject-&gt;show();  // \u663e\u793a\u5f15\u7528\u8ba1\u6570\n}\n</code></pre>"},{"location":"Language/C%2B%2B/packages/","title":"Packages","text":""},{"location":"Language/C%2B%2B/packages/#pcl","title":"Pcl","text":"<pre><code>sudo apt update\nsudo apt install libpcl-dev\n</code></pre>"},{"location":"Language/C%2B%2B/packages/#eigen3","title":"Eigen3","text":"<pre><code>sudo apt install libeigen3-dev\n</code></pre>"},{"location":"Language/C%2B%2B/packages/#install-assimp","title":"Install ASSIMP","text":"<p>If ASSIMP is not already installed on your system, you will need to install it. On Ubuntu, you can typically install it using the package manager with the following command:</p> <pre><code>sudo apt update\nsudo apt install libassimp-dev\n</code></pre>"},{"location":"Language/C%2B%2B/packages/#_1","title":"Packages","text":"<pre><code>sudo apt update\nsudo apt install libembree-dev\n</code></pre> <pre><code>sudo apt update\nsudo apt install libopencv-dev\n</code></pre> <pre><code>sudo apt update\nsudo apt install libglfw3-dev\n</code></pre> <pre><code>sudo apt update\nsudo apt install libgtk-3-dev\nsudo apt install nvidia-cuda-toolkit\nsudo apt install libxxf86vm-dev\nsudo apt install ffmpeg libavdevice-dev\n</code></pre>"},{"location":"Language/C%2B%2B/syntax/","title":"Basic","text":"<ul> <li>\"a\"-&gt;string, 'a'-&gt;char</li> </ul>"},{"location":"Language/C%2B%2B/syntax/#value-range","title":"Value Range","text":""},{"location":"Language/C%2B%2B/syntax/#_1","title":"\u6709\u7b26\u53f7\u6574\u6570\u7c7b\u578b","text":"\u7c7b\u578b \u6700\u5c0f\u503c \u6700\u5927\u503c \u5907\u6ce8 <code>char</code> -128 127 \u81f3\u5c11 8 \u4f4d <code>short</code> -32,768 32,767 \u81f3\u5c11 16 \u4f4d <code>int</code> -2,147,483,648 2,147,483,647 \u901a\u5e38\u662f 32 \u4f4d <code>long</code> -2,147,483,648 2,147,483,647 \u81f3\u5c11 32 \u4f4d\uff0c\u4f46\u53ef\u80fd\u66f4\u957f <code>long long</code> -9,223,372,036,854,775,808 9,223,372,036,854,775,807 \u81f3\u5c11 64 \u4f4d"},{"location":"Language/C%2B%2B/syntax/#_2","title":"\u65e0\u7b26\u53f7\u6574\u6570\u7c7b\u578b\uff08\u975e\u8d1f\u6570\uff09","text":"\u7c7b\u578b \u6700\u5c0f\u503c \u6700\u5927\u503c \u5907\u6ce8 <code>unsigned char</code> 0 255 \u81f3\u5c11 8 \u4f4d <code>unsigned short</code> 0 65,535 \u81f3\u5c11 16 \u4f4d <code>unsigned int</code> 0 4,294,967,295 \u901a\u5e38\u662f 32 \u4f4d <code>unsigned long</code> 0 4,294,967,295 \u81f3\u5c11 32 \u4f4d\uff0c\u4f46\u53ef\u80fd\u66f4\u957f <code>unsigned long long</code> 0 18,446,744,073,709,551,615 \u81f3\u5c11 64 \u4f4d"},{"location":"Language/C%2B%2B/syntax/#uniform-initialization","title":"Uniform Initialization","text":"<pre><code>int a{5}; // \u76f4\u63a5\u521d\u59cb\u5316\nstd::vector&lt;int&gt; v{1, 2, 3}; // \u7edf\u4e00\u521d\u59cb\u5316\n</code></pre>"},{"location":"Language/C%2B%2B/syntax/#auto-keyword-for-type-inference","title":"Auto Keyword for Type Inference","text":"<pre><code>auto x = 42;  // int\nauto y = 42.0;  // double\n</code></pre>"},{"location":"Language/C%2B%2B/syntax/#range-based-for-loops","title":"Range-based For Loops","text":"<pre><code>for (const auto&amp; elem : vec) {\n    // Do something with elem\n}\n</code></pre>"},{"location":"Language/C%2B%2B/syntax/#lambda-expressions","title":"Lambda Expressions","text":"<pre><code>auto add = [](int a, int b) -&gt; int {\n    return a + b;\n};\n</code></pre>"},{"location":"Language/C%2B%2B/syntax/#smart-pointers","title":"Smart Pointers","text":"<pre><code>std::unique_ptr&lt;int&gt; p1(new int(42));\nstd::shared_ptr&lt;int&gt; p2 = std::make_shared&lt;int&gt;(42);\n</code></pre>"},{"location":"Language/C%2B%2B/syntax/#template-functions","title":"Template Functions","text":"<pre><code>template &lt;typename T&gt;\nT getMax(T a, T b) {\n    return (a &gt; b) ? a : b;\n}\n</code></pre>"},{"location":"Language/C%2B%2B/syntax/#move-semantics","title":"Move Semantics","text":"<pre><code>std::vector&lt;int&gt; vec1 = {1, 2, 3};\nstd::vector&lt;int&gt; vec2 = std::move(vec1);  // Moves data from vec1 to vec2\n</code></pre>"},{"location":"Language/C%2B%2B/syntax/#structured-binding-c17","title":"Structured Binding (C++17)","text":"<pre><code>std::pair&lt;int, std::string&gt; p = {1, \"one\"};\nauto [num, word] = p;\n</code></pre>"},{"location":"Language/C%2B%2B/syntax/#variadic-templates","title":"Variadic Templates","text":"<pre><code>template&lt;typename... Args&gt;\nvoid printAll(Args... args) {\n    // Do something\n}\n</code></pre>"},{"location":"Language/C%2B%2B/syntax/#pattern-match","title":"Pattern Match","text":""},{"location":"Language/C%2B%2B/syntax/#switch","title":"Switch","text":"<pre><code>int value = /* some value */;\nswitch (value) {\n    case 1:\n        // do something\n        break;\n    case 2:\n        // do something else\n        break;\n    default:\n        // default case\n        break;\n}\n</code></pre>"},{"location":"Language/C%2B%2B/syntax/#unordered_map","title":"unordered_map","text":"<pre><code>unordered_map&lt;string, function&lt;int(int, int)&gt;&gt; operations = {\n            {\"+\", [](int a, int b) { return a + b; }},\n            {\"-\", [](int a, int b) { return a - b; }},\n            {\"*\", [](int a, int b) { return a * b; }},\n            {\"/\", [](int a, int b) { return a / b; }}\n        };\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/Queue/","title":"Queue","text":"<p>A queue is a container adaptor that provides a FIFO (First-In-First-Out) data structure. The standard library's <code>std::queue</code> is defined in the header <code>&lt;queue&gt;</code>.</p>"},{"location":"Language/C%2B%2B/Data_model/Queue/#initialization","title":"Initialization","text":"<ul> <li>Empty Queue:</li> </ul> <pre><code>std::queue&lt;int&gt; q;\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/Queue/#adding-elements","title":"Adding Elements","text":"<ul> <li>Append to End:</li> </ul> <pre><code>q.push(4);\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/Queue/#access-elements","title":"Access Elements","text":"<ul> <li>Access Next Element:</li> </ul> <pre><code>int next = q.front();\n</code></pre> <ul> <li>Access Last Element:</li> </ul> <pre><code>int last = q.back();\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/Queue/#removing-elements","title":"Removing Elements","text":"<ul> <li>Remove Next Element:</li> </ul> <pre><code>q.pop();  // Removes the element at the front of the queue\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/Queue/#query-attributes","title":"Query Attributes","text":"<ul> <li>Get Queue Size:</li> </ul> <pre><code>size_t size = q.size();\n</code></pre> <ul> <li>Check if Queue is Empty:</li> </ul> <pre><code>bool isEmpty = q.empty();\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/Queue/#other-common-operations","title":"Other Common Operations","text":"<ul> <li>Swap Contents:</li> </ul> <pre><code>std::queue&lt;int&gt; q1, q2;\nq1.swap(q2);  // Swaps the contents of q1 and q2\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/Queue/#examples","title":"Examples","text":"<pre><code>#include &lt;iostream&gt;\n#include &lt;queue&gt;\n\nint main() {\n    std::queue&lt;int&gt; q;\n\n    // Adding elements to the queue\n    q.push(1);\n    q.push(2);\n    q.push(3);\n\n    // Accessing elements\n    std::cout &lt;&lt; \"Front: \" &lt;&lt; q.front() &lt;&lt; std::endl; // Outputs 1\n    std::cout &lt;&lt; \"Back: \" &lt;&lt; q.back() &lt;&lt; std::endl;  // Outputs 3\n\n    // Removing elements\n    q.pop();  // Now the front is 2\n\n    // Checking size and if empty\n    std::cout &lt;&lt; \"Size: \" &lt;&lt; q.size() &lt;&lt; std::endl; // Outputs 2\n    std::cout &lt;&lt; \"Is empty: \" &lt;&lt; (q.empty() ? \"Yes\" : \"No\") &lt;&lt; std::endl;  // Outputs No\n\n    return 0;\n}\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/String/","title":"String","text":"<pre><code>#include&lt;string&gt;\n\nstring str = \"Hello, world!\";  //\nstring sub = str.substr(0, 5);  // sub = \"Hello\"\n\n# \u7c7b\u578b\u8f6c\u6362\nint num = 43;\nstring sNum = to_string(num);\n\n# \u67e5\u627e\u5143\u7d20\nString.find()-&gt;size_t\n// \u627e\u5230\u7684\u7b2c\u4e00\u4e2a\u5b57\u7b26\u6216\u8005\u5b57\u4e32\u7684index\u6216\u8005\u6ca1\u627e\u5230\u5c31\u662fstring::npos\nstd::string myString = \"Hello, world!\";\nsize_t position = myString.find(\"world\");// \u65e0\u7b26\u53f7\u6574\u6570\u7c7b\u578b\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/map/","title":"Map","text":""},{"location":"Language/C%2B%2B/Data_model/map/#initialization","title":"Initialization","text":"<ul> <li>Empty Map:</li> </ul> <pre><code>std::map&lt;int, std::string&gt; myMap;\n\n    std::unordered_map&lt;int, std::string&gt; myMap = {\n    {1, \"one\"},\n    {2, \"two\"},\n    {3, \"three\"}\n\n};\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/map/#insert-elements","title":"Insert Elements","text":"<ul> <li>Insert Single Element:</li> </ul> <pre><code>myMap[1] = \"one\";  // Insert key-value pair\n// or\nmyMap.insert(std::make_pair(1, \"one\"));  // Insert key-value pair\n\nmyMap.emplace(1, \"one\"); // create key-value pair directly\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/map/#access-elements","title":"Access Elements","text":"<ul> <li>Access Value by Key:</li> </ul> <pre><code>std::string value = myMap[1];  // Access value by key, returns std::string\n</code></pre> <ul> <li>Find Element:</li> </ul> <pre><code>auto it = myMap.find(1);  // Returns iterator to the element if key exists, otherwise returns myMap.end()\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/map/#removing-elements","title":"Removing Elements","text":"<ul> <li>Remove Element by Key:</li> </ul> <pre><code>myMap.erase(1);  // Removes element with key 1, returns number of elements removed (size_t)\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/map/#query-attributes","title":"Query Attributes","text":"<ul> <li>Get Map Size:</li> </ul> <pre><code>size_t size = myMap.size();  // Returns the number of elements in the map (size_t)\n</code></pre> <ul> <li>Check if Map is Empty:</li> </ul> <pre><code>bool isEmpty = myMap.empty();  // Returns true if the map is empty, otherwise false (bool)\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/map/#iterating-through-map","title":"Iterating through Map","text":"<ul> <li>Using Iterator:</li> </ul> <pre><code>for (auto it = myMap.begin(); it != myMap.end(); ++it) {\n    // it-&gt;first: key\n    // it-&gt;second: value\n}\n</code></pre> <ul> <li>Using Range-based For Loop:</li> </ul> <pre><code>for (const auto&amp; [key, value] : myMap) {\n    // key: key\n    // value: value\n}\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/map/#other-common-operations","title":"Other Common Operations","text":"<ul> <li>Clear Map:</li> </ul> <pre><code>myMap.clear();  // Removes all elements from the map\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/set/","title":"Set","text":"<ul> <li>std::set is an associative container that contains a sorted set of unique objects. It is usually implemented as a red-black tree.</li> </ul>"},{"location":"Language/C%2B%2B/Data_model/set/#initialization","title":"Initialization","text":"<ul> <li>Empty Set:</li> </ul> <pre><code>std::set&lt;int&gt; mySet;\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/set/#insert-elements","title":"Insert Elements","text":"<ul> <li>Insert Single Element (Average case O(log n)):</li> </ul> <pre><code>mySet.insert(10); // Insert an element\n</code></pre> <ul> <li>Emplace Element (Average case O(log n)):</li> </ul> <pre><code>mySet.emplace(10); // Construct and insert element\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/set/#access-elements","title":"Access Elements","text":"<ul> <li>Find Element (Average case O(log n)):</li> </ul> <pre><code>auto it = mySet.find(10); // Returns iterator to the element if found, otherwise returns mySet.end()\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/set/#removing-elements","title":"Removing Elements","text":"<ul> <li>Remove Element by Value (Average case O(log n)):</li> </ul> <pre><code>mySet.erase(10); // Erases element with value 10\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/set/#query-attributes","title":"Query Attributes","text":"<ul> <li>Get Set Size (O(1)):</li> </ul> <pre><code>size_t size = mySet.size(); // Returns the number of elements\n</code></pre> <ul> <li>Check if Set is Empty (O(1)):</li> </ul> <pre><code>bool isEmpty = mySet.empty(); // Returns true if set is empty, otherwise false\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/set/#iterating-through-set","title":"Iterating through Set","text":"<ul> <li>Using Iterator (O(n)):</li> </ul> <pre><code>for (auto it = mySet.begin(); it != mySet.end(); ++it) {\n    // Access the element as *it\n}\n</code></pre> <ul> <li>Using Range-based For Loop (O(n)):</li> </ul> <pre><code>for (const auto&amp; elem : mySet) {\n    // Access the element directly as elem\n}\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/set/#other-common-operations","title":"Other Common Operations","text":"<ul> <li>Clear Set (O(n)):</li> </ul> <pre><code>mySet.clear(); // Removes all elements from the set\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/stack/","title":"Stack","text":"<pre><code>#include &lt;stack&gt;\nint main() {\n    std::stack&lt;int&gt; myStack;\n\n    // \u6dfb\u52a0\u5143\u7d20\u5230\u6808\u4e2d\n    myStack.push(1);\n    // \u8f93\u51fa\u6808\u9876\u5143\u7d20\n myStack.top();\n\n    // \u79fb\u9664\u6808\u9876\u5143\u7d20\n    myStack.pop();\n\n    // \u68c0\u67e5\u6808\u662f\u5426\u4e3a\u7a7a\n    myStack.empty()\n\n}\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/unordered_map/","title":"unordered_map","text":""},{"location":"Language/C%2B%2B/Data_model/unordered_map/#initialization","title":"Initialization","text":"<ul> <li>Empty Unordered Map:</li> </ul> <pre><code>std::unordered_map&lt;int, std::string&gt; myMap;  // Empty unordered_map\n</code></pre> <ul> <li>brace initialization:</li> </ul> <pre><code>std::unordered_map&lt;int, std::string&gt; myMap = {\n    {1, \"one\"},\n    {2, \"two\"},\n    {3, \"three\"}\n};\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/unordered_map/#insert-elements","title":"Insert Elements","text":"<ul> <li>Insert Single Element:</li> </ul> <pre><code>myMap[1] = \"one\";  // Insert key-value pair\n// or\nmyMap.insert(std::make_pair(1, \"one\"));  // Insert key-value pair\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/unordered_map/#access-elements","title":"Access Elements","text":"<ul> <li>Access Value by Key:</li> </ul> <pre><code>std::string value = myMap[1];  // Access value by key, returns std::string\n</code></pre> <ul> <li>Find Element:</li> </ul> <pre><code>auto it = myMap.find(1);\n// Returns iterator to the element if key exists, otherwise returns myMap.end()\nit-&gt;first  # key\nit-&gt;second # value\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/unordered_map/#removing-elements","title":"Removing Elements","text":"<ul> <li>Remove Element by Key:</li> </ul> <pre><code>myMap.erase(1);  // Removes element with key 1, returns number of elements removed (size_t)\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/unordered_map/#query-attributes","title":"Query Attributes","text":"<ul> <li>Get Map Size:</li> </ul> <pre><code>size_t size = myMap.size();  // Returns the number of elements in the map (size_t)\n</code></pre> <ul> <li>Check if Map is Empty:</li> </ul> <pre><code>bool isEmpty = myMap.empty();  // Returns true if the map is empty, otherwise false (bool)\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/unordered_map/#iterating-through-unordered-map","title":"Iterating through Unordered Map","text":"<ul> <li>Using Iterator:</li> </ul> <pre><code>for (auto it = myMap.begin(); it != myMap.end(); ++it) {\n    // it-&gt;first: key\n    // it-&gt;second: value\n}\n</code></pre> <ul> <li>Using Range-based For Loop:</li> </ul> <pre><code>for (const auto&amp; [key, value] : myMap) {\n    // key: key\n    // value: value\n}\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/unordered_map/#other-common-operations","title":"Other Common Operations","text":"<ul> <li>Clear Map:</li> </ul> <pre><code>myMap.clear();  // Removes all elements from the map\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/unordered_set/","title":"unordered_set","text":"<ul> <li>std::unordered_set is an associative container that contains a set of unique objects. Search, insertion, and removal operations have average constant-time complexity.</li> </ul>"},{"location":"Language/C%2B%2B/Data_model/unordered_set/#initialization","title":"Initialization","text":"<ul> <li>Empty Unordered Set:</li> </ul> <pre><code>std::unordered_set&lt;int&gt; myUnorderedSet;\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/unordered_set/#insert-elements","title":"Insert Elements","text":"<ul> <li>Insert Single Element (Average case O(1)):</li> </ul> <pre><code>myUnorderedSet.insert(10); // Insert an element\n</code></pre> <ul> <li>Emplace Element (Average case O(1)):</li> </ul> <pre><code>myUnorderedSet.emplace(10); // Construct and insert element\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/unordered_set/#access-elements","title":"Access Elements","text":"<ul> <li>Find Element (Average case O(1)):</li> </ul> <pre><code>auto it = myUnorderedSet.find(10);\n// Returns iterator to the element if found, otherwise returns myUnorderedSet.end()\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/unordered_set/#removing-elements","title":"Removing Elements","text":"<ul> <li>Remove Element by Value (Average case O(1)):</li> </ul> <pre><code>myUnorderedSet.erase(10); // Erases element with value 10\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/unordered_set/#query-attributes","title":"Query Attributes","text":"<ul> <li>Get Unordered Set Size (O(1)):</li> </ul> <pre><code>size_t size = myUnorderedSet.size(); // Returns the number of elements\n</code></pre> <ul> <li>Check if Unordered Set is Empty (O(1)):</li> </ul> <pre><code>bool isEmpty = myUnorderedSet.empty(); // Returns true if unordered set is empty, otherwise false\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/unordered_set/#iterating-through-unordered-set","title":"Iterating through Unordered Set","text":"<ul> <li>Using Iterator (O(n)):</li> </ul> <pre><code>for (auto it = myUnorderedSet.begin(); it != myUnorderedSet.end(); ++it) {\n    // Access the element as *it\n}\n</code></pre> <ul> <li>Using Range-based For Loop (O(n)):</li> </ul> <pre><code>for (const auto&amp; elem : myUnorderedSet) {\n    // Access the element directly as elem\n}\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/unordered_set/#other-common-operations","title":"Other Common Operations","text":"<ul> <li>Clear Unordered Set (O(n)):</li> </ul> <pre><code>myUnorderedSet.clear(); // Removes all elements from the unordered set\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/vector/","title":"Initialization","text":"<ul> <li>Empty Vector:</li> </ul> <pre><code>std::vector&lt;int&gt; vec;\n</code></pre> <ul> <li>Pre-allocate Size:</li> </ul> <pre><code>std::vector&lt;int&gt; vec(10);  // Contains 10 elements, all initialized to 0\n</code></pre> <ul> <li>Pre-allocate Size with Value:</li> </ul> <pre><code>std::vector&lt;int&gt; vec(10, 1);  // Contains 10 elements, all initialized to 1\n</code></pre> <ul> <li>Initialize from Array:</li> </ul> <pre><code>int arr[] = {1, 2, 3};\nstd::vector&lt;int&gt; vec(arr, arr + sizeof(arr) / sizeof(int));\n</code></pre> <ul> <li>Initialize from Another Vector:</li> </ul> <pre><code>std::vector&lt;int&gt; vec2(vec);\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/vector/#adding-elements","title":"Adding Elements","text":"<ul> <li>Append to End:</li> </ul> <pre><code>vec.push_back(4);\n</code></pre> <ul> <li>Insert at Beginning:</li> </ul> <pre><code>vec.insert(vec.begin(), 4);\n</code></pre> <ul> <li>Insert at Specific Position:</li> </ul> <pre><code>vec.insert(vec.begin() + 1, 4);\n</code></pre> <ul> <li>append vector</li> </ul> <pre><code>vec.insert(vec.end(),vec_2.begin(),vec_2.end())\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/vector/#access-elements","title":"Access Elements","text":"<ul> <li>Access Last Element:</li> </ul> <pre><code>int last = vec.back();\n</code></pre> <ul> <li>Access First Element:</li> </ul> <pre><code>int first = vec.front();\n</code></pre> <ul> <li>Access i-th Element (0-based):</li> </ul> <pre><code>int elem = vec[i];\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/vector/#removing-elements","title":"Removing Elements","text":"<ul> <li>Remove Last Element:</li> </ul> <pre><code>vec.pop_back();    -&gt;void\n</code></pre> <ul> <li>Remove i-th Element:</li> </ul> <pre><code>vec.erase(vec.begin() + i);\n</code></pre> <ul> <li>Remove a Range of Elements:</li> </ul> <pre><code>vec.erase(vec.begin() + i, vec.begin() + j);  // Removes elements from i to j-1\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/vector/#query-attributes","title":"Query Attributes","text":"<ul> <li>Get Vector Size:</li> </ul> <pre><code>size_t size = vec.size();\n</code></pre> <ul> <li>Check if Vector is Empty:</li> </ul> <pre><code>bool isEmpty = vec.empty();\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/vector/#other-common-operations","title":"Other Common Operations","text":"<ul> <li>Clear Vector:</li> </ul> <pre><code>vec.clear();\n</code></pre> <ul> <li>Resize Vector:</li> </ul> <pre><code>vec.resize(20);  // New elements are initialized to 0\n</code></pre> <ul> <li>Find Element Position:</li> </ul> <pre><code>std::find(vec.begin(), vec.end(), value) != vec.end()\n</code></pre> <ul> <li>Sort Vector:</li> </ul> <pre><code>std::sort(vec.begin(), vec.end());\n</code></pre>"},{"location":"Language/C%2B%2B/Data_model/vector/#examples","title":"Examples","text":"<pre><code>#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;algorithm&gt;\n\nint main() {\n    std::vector&lt;int&gt; vec = {3, 1, 4, 1, 5, 9, 2, 6, 5};\n\n    // \u4f7f\u7528 std::min_element \u548c lambda \u51fd\u6570\u627e\u5230\u6700\u5c0f\u6b63\u6574\u6570\n    auto it = std::min_element(vec.begin(), vec.end(), [](int a, int b) {\n        if (a &lt;= 0) return false;\n        if (b &lt;= 0) return true;\n        return a &lt; b;\n    });\n\n    if (it != vec.end() &amp;&amp; *it &gt; 0) {\n        std::cout &lt;&lt; \"\u6700\u5c0f\u6b63\u6574\u6570\u662f\uff1a\" &lt;&lt; *it &lt;&lt; std::endl;\n    } else {\n        std::cout &lt;&lt; \"\u6ca1\u6709\u627e\u5230\u6b63\u6574\u6570\" &lt;&lt; std::endl;\n    }\n\n    return 0;\n}\n</code></pre> <pre><code>#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;algorithm&gt;\n\nint main() {\n    std::vector&lt;int&gt; vec = {1, 2, 2, 3, 4, 4, 5};\n\n    // \u5148\u6392\u5e8f\n    std::sort(vec.begin(), vec.end());\n\n    // \u4f7f\u7528 std::unique \u53bb\u91cd\n    auto last = std::unique(vec.begin(), vec.end());\n\n    // \u5220\u9664\u591a\u4f59\u5143\u7d20\n    vec.erase(last, vec.end());\n\n    // \u8f93\u51fa\u53bb\u91cd\u540e\u7684\u5411\u91cf\n    for (const auto&amp; elem : vec) {\n        std::cout &lt;&lt; elem &lt;&lt; \" \";\n    }\n    std::cout &lt;&lt; std::endl;\n\n    return 0;\n}\n</code></pre>"},{"location":"Language/C%2B%2B/STL/Pointer/","title":"Pointer","text":""},{"location":"Language/C%2B%2B/STL/Pointer/#c","title":"C++ \u667a\u80fd\u6307\u9488\u7b80\u4ecb","text":"<p>\u667a\u80fd\u6307\u9488\u662f\u73b0\u4ee3 C++ \u4e2d\u7528\u4e8e\u81ea\u52a8\u7ba1\u7406\u5185\u5b58\u7684\u5bf9\u8c61\uff0c\u901a\u8fc7\u5c01\u88c5\u539f\u59cb\u6307\u9488\uff0c\u5b83\u4eec\u5e2e\u52a9\u7a0b\u5e8f\u5458\u907f\u514d\u5185\u5b58\u6cc4\u6f0f\u5e76\u63d0\u4f9b\u5f02\u5e38\u5b89\u5168\u3002C++11 \u6807\u51c6\u5f15\u5165\u4e86\u51e0\u79cd\u667a\u80fd\u6307\u9488\u7c7b\u578b\uff0c\u4f7f\u5f97\u8d44\u6e90\u7ba1\u7406\u66f4\u52a0\u7b80\u5355\u548c\u5b89\u5168\u3002</p>"},{"location":"Language/C%2B%2B/STL/Pointer/#_1","title":"\u667a\u80fd\u6307\u9488\u7c7b\u578b","text":"<ol> <li> <p><code>std::unique_ptr</code></p> </li> <li> <p>\u7279\u6027\uff1a\u63d0\u4f9b\u5bf9\u5185\u5b58\u7684\u72ec\u5360\u6240\u6709\u6743\uff0c\u5373\u540c\u4e00\u65f6\u95f4\u5185\u53ea\u6709\u4e00\u4e2a <code>std::unique_ptr</code> \u53ef\u4ee5\u62e5\u6709\u8d44\u6e90\u3002</p> </li> <li>\u5178\u578b\u7528\u9014\uff1a\u7528\u4e8e\u9700\u8981\u660e\u786e\u5355\u4e00\u6240\u6709\u6743\u7684\u60c5\u51b5\uff0c\u4f8b\u5982\u5728\u51fd\u6570\u5185\u90e8\u521b\u5efa\u4e34\u65f6\u5bf9\u8c61\u3002</li> <li>\u4e0d\u53ef\u590d\u5236\uff1a\u53ea\u80fd\u79fb\u52a8\uff0c\u786e\u4fdd\u6240\u6709\u6743\u7684\u552f\u4e00\u6027\u3002</li> <li> <p>\u5f15\u5165\u7248\u672c\uff1aC++11\u3002</p> </li> <li> <p><code>std::shared_ptr</code></p> </li> <li> <p>\u7279\u6027\uff1a\u652f\u6301\u591a\u91cd\u6240\u6709\u6743\u6982\u5ff5\uff0c\u5373\u591a\u4e2a <code>std::shared_ptr</code> \u5bf9\u8c61\u53ef\u4ee5\u5171\u4eab\u540c\u4e00\u4e2a\u8d44\u6e90\u3002</p> </li> <li>\u5178\u578b\u7528\u9014\uff1a\u7528\u4e8e\u5bf9\u8c61\u751f\u547d\u5468\u671f\u9700\u8981\u7531\u591a\u4e2a\u6240\u6709\u8005\u5171\u540c\u7ba1\u7406\u7684\u60c5\u51b5\uff0c\u4f8b\u5982\u5728\u591a\u4e2a\u6570\u636e\u7ed3\u6784\u4e4b\u95f4\u5171\u4eab\u6570\u636e\u3002</li> <li> <p>\u5f15\u5165\u7248\u672c\uff1aC++11\u3002</p> </li> <li> <p><code>std::weak_ptr</code></p> </li> <li>\u7279\u6027\uff1a\u63d0\u4f9b\u4e00\u79cd\u975e\u62e5\u6709\u6027\u7684\u667a\u80fd\u6307\u9488\uff0c\u7528\u6765\u89c2\u5bdf <code>std::shared_ptr</code>\uff0c\u4f46\u4e0d\u5f71\u54cd\u5176\u5f15\u7528\u8ba1\u6570\u3002</li> <li>\u5178\u578b\u7528\u9014\uff1a\u7528\u4e8e\u89e3\u51b3 <code>std::shared_ptr</code> \u53ef\u80fd\u5bfc\u81f4\u7684\u5faa\u73af\u5f15\u7528\u95ee\u9898\u3002</li> <li>\u5f15\u5165\u7248\u672c\uff1aC++11\u3002</li> </ol> <pre><code>#include &lt;iostream&gt;\n#include &lt;memory&gt;\n\nclass Child;\nclass Parent;\n\nclass Parent : public std::enable_shared_from_this&lt;Parent&gt; {\npublic:\n    std::shared_ptr&lt;Child&gt; child;\n    ~Parent() { std::cout &lt;&lt; \"Parent destroyed\\n\"; }\n\n    void setChild(std::shared_ptr&lt;Child&gt; c) {\n        child = c;\n        child-&gt;setParent(shared_from_this());  // \u4f7f\u7528 shared_from_this() \u83b7\u53d6\u5f53\u524d\u5bf9\u8c61\u7684 shared_ptr\n    }\n};\n\nclass Child {\npublic:\n    std::weak_ptr&lt;Parent&gt; parent;  // \u4f7f\u7528 weak_ptr \u907f\u514d\u5faa\u73af\u5f15\u7528\n    ~Child() { std::cout &lt;&lt; \"Child destroyed\\n\"; }\n\n    void setParent(std::shared_ptr&lt;Parent&gt; p) {\n        parent = p;  // \u8bbe\u7f6e parent \u4e3a\u4f20\u5165\u7684 shared_ptr\n    }\n};\n\nint main() {\n    std::shared_ptr&lt;Parent&gt; p = std::make_shared&lt;Parent&gt;();\n    std::shared_ptr&lt;Child&gt; c = std::make_shared&lt;Child&gt;();\n    p-&gt;setChild(c);\n\n    std::cout &lt;&lt; \"Parent use count: \" &lt;&lt; p.use_count() &lt;&lt; std::endl;  // \u8f93\u51fa\u4f1a\u662f 2\uff0c\u56e0\u4e3a child \u4e5f\u6301\u6709 parent \u7684 shared_ptr\n    std::cout &lt;&lt; \"Child use count: \" &lt;&lt; c.use_count() &lt;&lt; std::endl;   // \u8f93\u51fa 1\n}\n</code></pre>"},{"location":"Language/C%2B%2B/STL/Pointer/#_2","title":"\u521b\u5efa\u548c\u4f7f\u7528\u667a\u80fd\u6307\u9488","text":"<ul> <li><code>std::make_shared</code></li> <li>\u63cf\u8ff0\uff1a\u7528\u4e8e\u6784\u9020 <code>std::shared_ptr</code> \u7684\u4f18\u9009\u65b9\u6cd5\uff0c\u56e0\u4e3a\u5b83\u53ef\u4ee5\u5728\u5355\u4e00\u64cd\u4f5c\u4e2d\u5206\u914d\u5185\u5b58\uff0c\u63d0\u9ad8\u6548\u7387\u5e76\u51cf\u5c11\u5f02\u5e38\u98ce\u9669\u3002</li> <li>\u5f15\u5165\u7248\u672c\uff1aC++11\u3002</li> <li><code>std::make_unique</code></li> <li>\u63cf\u8ff0\uff1a\u7528\u4e8e\u6784\u9020 <code>std::unique_ptr</code> \u7684\u65b9\u6cd5\u3002\u7c7b\u4f3c\u4e8e <code>std::make_shared</code>\uff0c\u4f46\u4e13\u4e3a <code>std::unique_ptr</code> \u8bbe\u8ba1\u3002</li> <li>\u5f15\u5165\u7248\u672c\uff1aC++14\u3002</li> <li><code>shared_from_this</code></li> <li>\u63cf\u8ff0\uff1a\u5f53\u7c7b\u7ee7\u627f\u81ea <code>std::enable_shared_from_this</code> \u65f6\uff0c\u4f60\u53ef\u4ee5\u5b89\u5168\u5730\u5728\u7c7b\u5185\u90e8\u83b7\u53d6\u4e00\u4e2a\u6307\u5411\u81ea\u5df1\u7684 <code>std::shared_ptr</code>\u3002</li> <li>\u5f15\u5165\u7248\u672c\uff1aC++11\u3002</li> </ul> <p><code>shared_from_this()</code> \u53ea\u80fd\u5728 <code>std::shared_ptr</code> \u5df2\u7ecf\u62e5\u6709\u8be5\u5bf9\u8c61\u65f6\u624d\u80fd\u5b89\u5168\u8c03\u7528\u3002\u5982\u679c\u5728\u4efb\u4f55 <code>std::shared_ptr</code> \u62e5\u6709\u5bf9\u8c61\u4e4b\u524d\u8c03\u7528\u5b83\uff0c\u7a0b\u5e8f\u5c06\u629b\u51fa <code>std::bad_weak_ptr</code> \u5f02\u5e38\u3002</p>"},{"location":"Language/C%2B%2B/STL/Pointer/#_3","title":"\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>#include &lt;memory&gt;\n#include &lt;iostream&gt;\n\nclass MyClass : public std::enable_shared_from_this&lt;MyClass&gt; {\npublic:\n    void show() {\n        auto sharedPtr = shared_from_this();\n        std::cout &lt;&lt; \"MyClass instance has \" &lt;&lt; sharedPtr.use_count() &lt;&lt; \" references\\n\";\n    }\n};\n\nint main() {\n    auto myObject = std::make_shared&lt;MyClass&gt;();\n    myObject-&gt;show();  // \u663e\u793a\u5f15\u7528\u8ba1\u6570\n}\n</code></pre>"},{"location":"Language/C%2B%2B/STL/Queue/","title":"Queue","text":"<p>A queue is a container adaptor that provides a FIFO (First-In-First-Out) data structure. The standard library's <code>std::queue</code> is defined in the header <code>&lt;queue&gt;</code>.</p>"},{"location":"Language/C%2B%2B/STL/Queue/#initialization","title":"Initialization","text":"<ul> <li>Empty Queue:</li> </ul> <pre><code>std::queue&lt;int&gt; q;\n</code></pre>"},{"location":"Language/C%2B%2B/STL/Queue/#adding-elements","title":"Adding Elements","text":"<ul> <li>Append to End:</li> </ul> <pre><code>q.push(4);\n</code></pre>"},{"location":"Language/C%2B%2B/STL/Queue/#access-elements","title":"Access Elements","text":"<ul> <li>Access Next Element:</li> </ul> <pre><code>int next = q.front();\n</code></pre> <ul> <li>Access Last Element:</li> </ul> <pre><code>int last = q.back();\n</code></pre>"},{"location":"Language/C%2B%2B/STL/Queue/#removing-elements","title":"Removing Elements","text":"<ul> <li>Remove Next Element:</li> </ul> <pre><code>q.pop();  // Removes the element at the front of the queue\n</code></pre>"},{"location":"Language/C%2B%2B/STL/Queue/#query-attributes","title":"Query Attributes","text":"<ul> <li>Get Queue Size:</li> </ul> <pre><code>size_t size = q.size();\n</code></pre> <ul> <li>Check if Queue is Empty:</li> </ul> <pre><code>bool isEmpty = q.empty();\n</code></pre>"},{"location":"Language/C%2B%2B/STL/Queue/#other-common-operations","title":"Other Common Operations","text":"<ul> <li>Swap Contents:</li> </ul> <pre><code>std::queue&lt;int&gt; q1, q2;\nq1.swap(q2);  // Swaps the contents of q1 and q2\n</code></pre>"},{"location":"Language/C%2B%2B/STL/Queue/#examples","title":"Examples","text":"<pre><code>#include &lt;iostream&gt;\n#include &lt;queue&gt;\n\nint main() {\n    std::queue&lt;int&gt; q;\n\n    // Adding elements to the queue\n    q.push(1);\n    q.push(2);\n    q.push(3);\n\n    // Accessing elements\n    std::cout &lt;&lt; \"Front: \" &lt;&lt; q.front() &lt;&lt; std::endl; // Outputs 1\n    std::cout &lt;&lt; \"Back: \" &lt;&lt; q.back() &lt;&lt; std::endl;  // Outputs 3\n\n    // Removing elements\n    q.pop();  // Now the front is 2\n\n    // Checking size and if empty\n    std::cout &lt;&lt; \"Size: \" &lt;&lt; q.size() &lt;&lt; std::endl; // Outputs 2\n    std::cout &lt;&lt; \"Is empty: \" &lt;&lt; (q.empty() ? \"Yes\" : \"No\") &lt;&lt; std::endl;  // Outputs No\n\n    return 0;\n}\n</code></pre>"},{"location":"Language/C%2B%2B/STL/String/","title":"String","text":"<pre><code>#include&lt;string&gt;\n\nstring str = \"Hello, world!\";  //\nstring sub = str.substr(0, 5);  // sub = \"Hello\"\n\n# \u7c7b\u578b\u8f6c\u6362\nint num = 43;\nstring sNum = to_string(num);\n\n# \u67e5\u627e\u5143\u7d20\nString.find()-&gt;size_t\n// \u627e\u5230\u7684\u7b2c\u4e00\u4e2a\u5b57\u7b26\u6216\u8005\u5b57\u4e32\u7684index\u6216\u8005\u6ca1\u627e\u5230\u5c31\u662fstring::npos\nstd::string myString = \"Hello, world!\";\nsize_t position = myString.find(\"world\");// \u65e0\u7b26\u53f7\u6574\u6570\u7c7b\u578b\n</code></pre>"},{"location":"Language/C%2B%2B/STL/algorithm/","title":"Algorithm","text":""},{"location":"Language/C%2B%2B/STL/algorithm/#reverse","title":"Reverse","text":"<pre><code>#include &lt;algorithm&gt; // for std::reverse\n#include &lt;string&gt;\n\nint main() {\n    std::string str = \"Hello\";\n    std::reverse(str.begin(), str.end());\n    std::cout &lt;&lt; str &lt;&lt; std::endl;  // \u8f93\u51fa \"olleH\"\n</code></pre>"},{"location":"Language/C%2B%2B/STL/algorithm/#char-digit-to-int","title":"Char Digit to Int","text":"<pre><code>#include &lt;string&gt;\nint main() {\n    std::string digit_char = \"42\";\n    int num_1 = std::stoi(digit_char);\n    long num_2 = std::stol(digit_char);\n    long long num_3 = std::stoll(digit_char);\n\n    // single char\n char digitChar = '4';\n int digit = digitChar - '0';\n    return 0;\n}\n</code></pre>"},{"location":"Language/C%2B%2B/STL/map/","title":"Map","text":""},{"location":"Language/C%2B%2B/STL/map/#initialization","title":"Initialization","text":"<ul> <li>Empty Map:</li> </ul> <pre><code>std::map&lt;int, std::string&gt; myMap;\n\n    std::unordered_map&lt;int, std::string&gt; myMap = {\n    {1, \"one\"},\n    {2, \"two\"},\n    {3, \"three\"}\n\n};\n</code></pre>"},{"location":"Language/C%2B%2B/STL/map/#insert-elements","title":"Insert Elements","text":"<ul> <li>Insert Single Element:</li> </ul> <pre><code>myMap[1] = \"one\";  // Insert key-value pair\n// or\nmyMap.insert(std::make_pair(1, \"one\"));  // Insert key-value pair\n\nmyMap.emplace(1, \"one\"); // create key-value pair directly\n</code></pre>"},{"location":"Language/C%2B%2B/STL/map/#access-elements","title":"Access Elements","text":"<ul> <li>Access Value by Key:</li> </ul> <pre><code>std::string value = myMap[1];  // Access value by key, returns std::string\n</code></pre> <ul> <li>Find Element:</li> </ul> <pre><code>auto it = myMap.find(1);  // Returns iterator to the element if key exists, otherwise returns myMap.end()\n</code></pre>"},{"location":"Language/C%2B%2B/STL/map/#removing-elements","title":"Removing Elements","text":"<ul> <li>Remove Element by Key:</li> </ul> <pre><code>myMap.erase(1);  // Removes element with key 1, returns number of elements removed (size_t)\n</code></pre>"},{"location":"Language/C%2B%2B/STL/map/#query-attributes","title":"Query Attributes","text":"<ul> <li>Get Map Size:</li> </ul> <pre><code>size_t size = myMap.size();  // Returns the number of elements in the map (size_t)\n</code></pre> <ul> <li>Check if Map is Empty:</li> </ul> <pre><code>bool isEmpty = myMap.empty();  // Returns true if the map is empty, otherwise false (bool)\n</code></pre>"},{"location":"Language/C%2B%2B/STL/map/#iterating-through-map","title":"Iterating through Map","text":"<ul> <li>Using Iterator:</li> </ul> <pre><code>for (auto it = myMap.begin(); it != myMap.end(); ++it) {\n    // it-&gt;first: key\n    // it-&gt;second: value\n}\n</code></pre> <ul> <li>Using Range-based For Loop:</li> </ul> <pre><code>for (const auto&amp; [key, value] : myMap) {\n    // key: key\n    // value: value\n}\n</code></pre>"},{"location":"Language/C%2B%2B/STL/map/#other-common-operations","title":"Other Common Operations","text":"<ul> <li>Clear Map:</li> </ul> <pre><code>myMap.clear();  // Removes all elements from the map\n</code></pre>"},{"location":"Language/C%2B%2B/STL/set/","title":"Set","text":"<ul> <li>std::set is an associative container that contains a sorted set of unique objects. It is usually implemented as a red-black tree.</li> </ul>"},{"location":"Language/C%2B%2B/STL/set/#initialization","title":"Initialization","text":"<ul> <li>Empty Set:</li> </ul> <pre><code>std::set&lt;int&gt; mySet;\n</code></pre>"},{"location":"Language/C%2B%2B/STL/set/#insert-elements","title":"Insert Elements","text":"<ul> <li>Insert Single Element (Average case O(log n)):</li> </ul> <pre><code>mySet.insert(10); // Insert an element\n</code></pre> <ul> <li>Emplace Element (Average case O(log n)):</li> </ul> <pre><code>mySet.emplace(10); // Construct and insert element\n</code></pre>"},{"location":"Language/C%2B%2B/STL/set/#access-elements","title":"Access Elements","text":"<ul> <li>Find Element (Average case O(log n)):</li> </ul> <pre><code>auto it = mySet.find(10); // Returns iterator to the element if found, otherwise returns mySet.end()\n</code></pre>"},{"location":"Language/C%2B%2B/STL/set/#removing-elements","title":"Removing Elements","text":"<ul> <li>Remove Element by Value (Average case O(log n)):</li> </ul> <pre><code>mySet.erase(10); // Erases element with value 10\n</code></pre>"},{"location":"Language/C%2B%2B/STL/set/#query-attributes","title":"Query Attributes","text":"<ul> <li>Get Set Size (O(1)):</li> </ul> <pre><code>size_t size = mySet.size(); // Returns the number of elements\n</code></pre> <ul> <li>Check if Set is Empty (O(1)):</li> </ul> <pre><code>bool isEmpty = mySet.empty(); // Returns true if set is empty, otherwise false\n</code></pre>"},{"location":"Language/C%2B%2B/STL/set/#iterating-through-set","title":"Iterating through Set","text":"<ul> <li>Using Iterator (O(n)):</li> </ul> <pre><code>for (auto it = mySet.begin(); it != mySet.end(); ++it) {\n    // Access the element as *it\n}\n</code></pre> <ul> <li>Using Range-based For Loop (O(n)):</li> </ul> <pre><code>for (const auto&amp; elem : mySet) {\n    // Access the element directly as elem\n}\n</code></pre>"},{"location":"Language/C%2B%2B/STL/set/#other-common-operations","title":"Other Common Operations","text":"<ul> <li>Clear Set (O(n)):</li> </ul> <pre><code>mySet.clear(); // Removes all elements from the set\n</code></pre>"},{"location":"Language/C%2B%2B/STL/stack/","title":"Stack","text":"<pre><code>#include &lt;stack&gt;\nint main() {\n    std::stack&lt;int&gt; myStack;\n\n    // \u6dfb\u52a0\u5143\u7d20\u5230\u6808\u4e2d\n    myStack.push(1);\n    // \u8f93\u51fa\u6808\u9876\u5143\u7d20\n myStack.top();\n\n    // \u79fb\u9664\u6808\u9876\u5143\u7d20\n    myStack.pop();\n\n    // \u68c0\u67e5\u6808\u662f\u5426\u4e3a\u7a7a\n    myStack.empty()\n\n}\n</code></pre>"},{"location":"Language/C%2B%2B/STL/unordered_map/","title":"unordered_map","text":""},{"location":"Language/C%2B%2B/STL/unordered_map/#initialization","title":"Initialization","text":"<ul> <li>Empty Unordered Map:</li> </ul> <pre><code>std::unordered_map&lt;int, std::string&gt; myMap;  // Empty unordered_map\n</code></pre> <ul> <li>brace initialization:</li> </ul> <pre><code>std::unordered_map&lt;int, std::string&gt; myMap = {\n    {1, \"one\"},\n    {2, \"two\"},\n    {3, \"three\"}\n};\n</code></pre>"},{"location":"Language/C%2B%2B/STL/unordered_map/#insert-elements","title":"Insert Elements","text":"<ul> <li>Insert Single Element:</li> </ul> <pre><code>myMap[1] = \"one\";  // Insert key-value pair\n// or\nmyMap.insert(std::make_pair(1, \"one\"));  // Insert key-value pair\n</code></pre>"},{"location":"Language/C%2B%2B/STL/unordered_map/#access-elements","title":"Access Elements","text":"<ul> <li>Access Value by Key:</li> </ul> <pre><code>std::string value = myMap[1];  // Access value by key, returns std::string\n</code></pre> <ul> <li>Find Element:</li> </ul> <pre><code>auto it = myMap.find(1);\n// Returns iterator to the element if key exists, otherwise returns myMap.end()\nit-&gt;first  # key\nit-&gt;second # value\n</code></pre>"},{"location":"Language/C%2B%2B/STL/unordered_map/#removing-elements","title":"Removing Elements","text":"<ul> <li>Remove Element by Key:</li> </ul> <pre><code>myMap.erase(1);  // Removes element with key 1, returns number of elements removed (size_t)\n</code></pre>"},{"location":"Language/C%2B%2B/STL/unordered_map/#query-attributes","title":"Query Attributes","text":"<ul> <li>Get Map Size:</li> </ul> <pre><code>size_t size = myMap.size();  // Returns the number of elements in the map (size_t)\n</code></pre> <ul> <li>Check if Map is Empty:</li> </ul> <pre><code>bool isEmpty = myMap.empty();  // Returns true if the map is empty, otherwise false (bool)\n</code></pre>"},{"location":"Language/C%2B%2B/STL/unordered_map/#iterating-through-unordered-map","title":"Iterating through Unordered Map","text":"<ul> <li>Using Iterator:</li> </ul> <pre><code>for (auto it = myMap.begin(); it != myMap.end(); ++it) {\n    // it-&gt;first: key\n    // it-&gt;second: value\n}\n</code></pre> <ul> <li>Using Range-based For Loop:</li> </ul> <pre><code>for (const auto&amp; [key, value] : myMap) {\n    // key: key\n    // value: value\n}\n</code></pre>"},{"location":"Language/C%2B%2B/STL/unordered_map/#other-common-operations","title":"Other Common Operations","text":"<ul> <li>Clear Map:</li> </ul> <pre><code>myMap.clear();  // Removes all elements from the map\n</code></pre>"},{"location":"Language/C%2B%2B/STL/unordered_set/","title":"unordered_set","text":"<ul> <li>std::unordered_set is an associative container that contains a set of unique objects. Search, insertion, and removal operations have average constant-time complexity.</li> </ul>"},{"location":"Language/C%2B%2B/STL/unordered_set/#initialization","title":"Initialization","text":"<ul> <li>Empty Unordered Set:</li> </ul> <pre><code>std::unordered_set&lt;int&gt; myUnorderedSet;\n</code></pre>"},{"location":"Language/C%2B%2B/STL/unordered_set/#insert-elements","title":"Insert Elements","text":"<ul> <li>Insert Single Element (Average case O(1)):</li> </ul> <pre><code>myUnorderedSet.insert(10); // Insert an element\n</code></pre> <ul> <li>Emplace Element (Average case O(1)):</li> </ul> <pre><code>myUnorderedSet.emplace(10); // Construct and insert element\n</code></pre>"},{"location":"Language/C%2B%2B/STL/unordered_set/#access-elements","title":"Access Elements","text":"<ul> <li>Find Element (Average case O(1)):</li> </ul> <pre><code>auto it = myUnorderedSet.find(10);\n// Returns iterator to the element if found, otherwise returns myUnorderedSet.end()\n</code></pre>"},{"location":"Language/C%2B%2B/STL/unordered_set/#removing-elements","title":"Removing Elements","text":"<ul> <li>Remove Element by Value (Average case O(1)):</li> </ul> <pre><code>myUnorderedSet.erase(10); // Erases element with value 10\n</code></pre>"},{"location":"Language/C%2B%2B/STL/unordered_set/#query-attributes","title":"Query Attributes","text":"<ul> <li>Get Unordered Set Size (O(1)):</li> </ul> <pre><code>size_t size = myUnorderedSet.size(); // Returns the number of elements\n</code></pre> <ul> <li>Check if Unordered Set is Empty (O(1)):</li> </ul> <pre><code>bool isEmpty = myUnorderedSet.empty(); // Returns true if unordered set is empty, otherwise false\n</code></pre>"},{"location":"Language/C%2B%2B/STL/unordered_set/#iterating-through-unordered-set","title":"Iterating through Unordered Set","text":"<ul> <li>Using Iterator (O(n)):</li> </ul> <pre><code>for (auto it = myUnorderedSet.begin(); it != myUnorderedSet.end(); ++it) {\n    // Access the element as *it\n}\n</code></pre> <ul> <li>Using Range-based For Loop (O(n)):</li> </ul> <pre><code>for (const auto&amp; elem : myUnorderedSet) {\n    // Access the element directly as elem\n}\n</code></pre>"},{"location":"Language/C%2B%2B/STL/unordered_set/#other-common-operations","title":"Other Common Operations","text":"<ul> <li>Clear Unordered Set (O(n)):</li> </ul> <pre><code>myUnorderedSet.clear(); // Removes all elements from the unordered set\n</code></pre>"},{"location":"Language/C%2B%2B/STL/vector/","title":"Initialization","text":"<ul> <li>Empty Vector:</li> </ul> <pre><code>std::vector&lt;int&gt; vec;\n</code></pre> <ul> <li>Pre-allocate Size:</li> </ul> <pre><code>std::vector&lt;int&gt; vec(10);  // Contains 10 elements, all initialized to 0\n</code></pre> <ul> <li>Pre-allocate Size with Value:</li> </ul> <pre><code>std::vector&lt;int&gt; vec(10, 1);  // Contains 10 elements, all initialized to 1\n</code></pre> <ul> <li>Initialize from Array:</li> </ul> <pre><code>int arr[] = {1, 2, 3};\nstd::vector&lt;int&gt; vec(arr, arr + sizeof(arr) / sizeof(int));\n</code></pre> <ul> <li>Initialize from Another Vector:</li> </ul> <pre><code>std::vector&lt;int&gt; vec2(vec);\n</code></pre>"},{"location":"Language/C%2B%2B/STL/vector/#adding-elements","title":"Adding Elements","text":"<ul> <li>Append to End:</li> </ul> <pre><code>vec.push_back(4);\n</code></pre> <ul> <li>Insert at Beginning:</li> </ul> <pre><code>vec.insert(vec.begin(), 4);\n</code></pre> <ul> <li>Insert at Specific Position:</li> </ul> <pre><code>vec.insert(vec.begin() + 1, 4);\n</code></pre> <ul> <li>append vector</li> </ul> <pre><code>vec.insert(vec.end(),vec_2.begin(),vec_2.end())\n</code></pre>"},{"location":"Language/C%2B%2B/STL/vector/#access-elements","title":"Access Elements","text":"<ul> <li>Access Last Element:</li> </ul> <pre><code>int last = vec.back();\n</code></pre> <ul> <li>Access First Element:</li> </ul> <pre><code>int first = vec.front();\n</code></pre> <ul> <li>Access i-th Element (0-based):</li> </ul> <pre><code>int elem = vec[i];\n</code></pre>"},{"location":"Language/C%2B%2B/STL/vector/#removing-elements","title":"Removing Elements","text":"<ul> <li>Remove Last Element:</li> </ul> <pre><code>vec.pop_back();    -&gt;void\n</code></pre> <ul> <li>Remove i-th Element:</li> </ul> <pre><code>vec.erase(vec.begin() + i);\n</code></pre> <ul> <li>Remove a Range of Elements:</li> </ul> <pre><code>vec.erase(vec.begin() + i, vec.begin() + j);  // Removes elements from i to j-1\n</code></pre>"},{"location":"Language/C%2B%2B/STL/vector/#query-attributes","title":"Query Attributes","text":"<ul> <li>Get Vector Size:</li> </ul> <pre><code>size_t size = vec.size();\n</code></pre> <ul> <li>Check if Vector is Empty:</li> </ul> <pre><code>bool isEmpty = vec.empty();\n</code></pre>"},{"location":"Language/C%2B%2B/STL/vector/#other-common-operations","title":"Other Common Operations","text":"<ul> <li>Clear Vector:</li> </ul> <pre><code>vec.clear();\n</code></pre> <ul> <li>Resize Vector:</li> </ul> <pre><code>vec.resize(20);  // New elements are initialized to 0\n</code></pre> <ul> <li>Find Element Position:</li> </ul> <pre><code>std::find(vec.begin(), vec.end(), value) != vec.end()\n</code></pre> <ul> <li>Sort Vector:</li> </ul> <pre><code>std::sort(vec.begin(), vec.end());\n</code></pre>"},{"location":"Language/C%2B%2B/STL/vector/#examples","title":"Examples","text":"<pre><code>#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;algorithm&gt;\n\nint main() {\n    std::vector&lt;int&gt; vec = {3, 1, 4, 1, 5, 9, 2, 6, 5};\n\n    // \u4f7f\u7528 std::min_element \u548c lambda \u51fd\u6570\u627e\u5230\u6700\u5c0f\u6b63\u6574\u6570\n    auto it = std::min_element(vec.begin(), vec.end(), [](int a, int b) {\n        if (a &lt;= 0) return false;\n        if (b &lt;= 0) return true;\n        return a &lt; b;\n    });\n\n    if (it != vec.end() &amp;&amp; *it &gt; 0) {\n        std::cout &lt;&lt; \"\u6700\u5c0f\u6b63\u6574\u6570\u662f\uff1a\" &lt;&lt; *it &lt;&lt; std::endl;\n    } else {\n        std::cout &lt;&lt; \"\u6ca1\u6709\u627e\u5230\u6b63\u6574\u6570\" &lt;&lt; std::endl;\n    }\n\n    return 0;\n}\n</code></pre> <pre><code>#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;algorithm&gt;\n\nint main() {\n    std::vector&lt;int&gt; vec = {1, 2, 2, 3, 4, 4, 5};\n\n    // \u5148\u6392\u5e8f\n    std::sort(vec.begin(), vec.end());\n\n    // \u4f7f\u7528 std::unique \u53bb\u91cd\n    auto last = std::unique(vec.begin(), vec.end());\n\n    // \u5220\u9664\u591a\u4f59\u5143\u7d20\n    vec.erase(last, vec.end());\n\n    // \u8f93\u51fa\u53bb\u91cd\u540e\u7684\u5411\u91cf\n    for (const auto&amp; elem : vec) {\n        std::cout &lt;&lt; elem &lt;&lt; \" \";\n    }\n    std::cout &lt;&lt; std::endl;\n\n    return 0;\n}\n</code></pre>"},{"location":"Language/python/Mypy/","title":"Mypy","text":""},{"location":"Language/python/Mypy/#numpy","title":"numpy","text":"<p>type stubs will be included in <code>numpy</code> since <code>1.20</code></p> <pre><code>[tool.mypy]\nplugins = [\"numpy.typing.mypy_plugin\"]\n</code></pre>"},{"location":"Language/python/Mypy/#scipy","title":"scipy","text":"<pre><code>uv add scipy-stubs\n</code></pre>"},{"location":"Language/python/Pytest/","title":"Pytest","text":""},{"location":"Language/python/Pytest/#pytest-mastery-guide","title":"Pytest Mastery Guide","text":""},{"location":"Language/python/Pytest/#pytest-exception-testing-cheat-sheet","title":"Pytest Exception Testing Cheat Sheet","text":""},{"location":"Language/python/Pytest/#10-mocking-mistakes-new-developers-make-and-how-to-avoid-them","title":"10 Mocking Mistakes New\u00a0Developers Make and How to\u00a0Avoid Them","text":""},{"location":"Language/python/Pytest/#how-to-test-external-apis-without-mocks","title":"How To Test External APIs without Mocks","text":""},{"location":"Language/python/Pytest/#pytest-mocking-handbook-quick-reference","title":"Pytest Mocking Handbook - Quick Reference","text":""},{"location":"Language/python/Std/Pathlib/","title":"Pathlib","text":"<ul> <li>___ \u73b0\u4ee3\u7684 \u9762\u5411\u5bf9\u8c61 \u7684\u7cfb\u7edf \u8def\u5f84 \u7ba1\u7406\u5e93 ___</li> </ul> <p>classmethod\u00a0Path.cwd() \u8fd4\u56de\u4e00\u4e2a\u65b0\u7684\u8868\u793a\u5f53\u524d\u76ee\u5f55\u7684\u8def\u5f84\u5bf9\u8c61\uff08\u548c\u00a0<code>os.getcwd()</code>\u00a0\u8fd4\u56de\u7684\u76f8\u540c\uff0c\u4e5f\u662f\u811a\u672c\u88ab\u8c03\u7528\u7684\u5730\u65b9 Path(__file__) \u662f\u811a\u672c\u6587\u4ef6\u6240\u5728\u7684\u8def\u5f84\uff0c\u548c\u88ab\u8c03\u7528\u8005\u65e0\u5173</p>"},{"location":"Language/python/Std/Pathlib/#class-attributes","title":"Class Attributes","text":"<ol> <li>PurePath.parents    \u63d0\u4f9b\u8bbf\u95ee\u6b64\u8def\u5f84\u7684\u903b\u8f91\u7956\u5148\u7684\u4e0d\u53ef\u53d8\u5e8f\u5217:</li> </ol> <pre><code>&gt;&gt;&gt; p = Path('some path dir....')\n&gt;&gt;&gt; p.parent == p.parents[0]\nTrue\n&gt;&gt;&gt; p.Parent.parent = p.parents[1]\nTrue\n......\u4ee5\u6b64\u7c7b\u63a8\n\"\"\" python 3.10 \u652f\u6301 \u8d1f\u6570 \u7d22\u5f15\u548c \u5207\u7247\"\"\"\n</code></pre> <ol> <li>PurePath.name</li> <li>\u6700\u540e\u4e00\u4e2a\u7ec4\u4ef6\uff0c\u5373 dir \u6700\u6df1\u5904\u7684 file_name\uff08with suffix\uff09or folder_name</li> </ol> <pre><code>&gt;&gt;&gt; PurePosixPath('my/library/setup.py').name\n'setup.py'\n</code></pre> <ol> <li>PurePath.suffix\u00b6</li> <li>\u5373 PurePath.name \u7684\u540e\u7f00 str</li> </ol> <pre><code>&gt;&gt;&gt; PurePosixPath('my/library/setup.py').suffix\n'.py'\n&gt;&gt;&gt; PurePosixPath('my/library.tar.gz').suffix\n'.gz'\n&gt;&gt;&gt; PurePosixPath('my/library').suffix\n''\n</code></pre>"},{"location":"Language/python/Venv/Poetry/","title":"Poetry","text":"<p>Poetry is a dependency management and packaging tool in Python. It allows you to declare the libraries your project depends on and manage distribution packages.</p>"},{"location":"Language/python/Venv/Poetry/#installation-of-poetry","title":"Installation of Poetry","text":"<p>Reference Documentation: Poetry Official Documentation</p> <pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre>"},{"location":"Language/python/Venv/Poetry/#setting-environment-variables","title":"Setting Environment Variables","text":"<pre><code>echo 'export PATH=\"$HOME/.local/bin:$PATH\"' &gt;&gt; ~/.zshrc\nsource ~/.zshrc\n</code></pre>"},{"location":"Language/python/Venv/Poetry/#poetry-commands","title":"Poetry Commands","text":""},{"location":"Language/python/Venv/Poetry/#project-initialization","title":"Project Initialization","text":"<p>Use the current directory's <code>pyproject.toml</code> environment:</p> <pre><code>poetry env use python\n</code></pre> <p>Create a new project (with <code>src</code> directory):</p> <pre><code>poetry new --src your_project\n</code></pre> <p>Initialize <code>pyproject.toml</code> file:</p> <pre><code>poetry init\n</code></pre> <p>Set Poetry configuration (e.g., to create virtual environments):</p> <pre><code>poetry config virtualenvs.create true --local\n</code></pre> <p>List Poetry configuration:</p> <pre><code>poetry config --list\n</code></pre> <p>Display environment information:</p> <pre><code>poetry env info\n</code></pre> <p>Create a project named <code>poetry-demo</code> with a <code>src</code> folder:</p> <pre><code>poetry new --src poetry-demo\n</code></pre> <p>Check Poetry version:</p> <pre><code>poetry --version\n</code></pre> <p>Add a Python package (e.g., add <code>request</code>):</p> <pre><code>poetry add request\n</code></pre> <p>Get the path to the virtual environment (for use in IDEs like PyCharm):</p> <pre><code>poetry env info --path\n</code></pre> <p>Export <code>requirements.txt</code> file:</p> <pre><code>poetry export -f requirements.txt --output requirements.txt\n</code></pre> <p>List virtual environments:</p> <pre><code>poetry env list\n</code></pre> <p>Remove a specific virtual environment:</p> <pre><code>poetry env remove &lt;venv_name&gt;\n</code></pre>"},{"location":"Language/python/Venv/Poetry/#dependency-management","title":"Dependency Management","text":""},{"location":"Language/python/Venv/Poetry/#export-requirementstxt","title":"Export <code>requirements.txt</code>","text":"<pre><code>poetry export -f requirements.txt --output requirements.txt\n</code></pre>"},{"location":"Language/python/Venv/Poetry/#install-dependencies-from-requirementstxt","title":"Install Dependencies from <code>requirements.txt</code>","text":"<pre><code>poetry add $(cat requirements.txt)\n</code></pre>"},{"location":"Language/python/Venv/Poetry/#publishing-packages","title":"Publishing Packages","text":""},{"location":"Language/python/Venv/Poetry/#initialize-pypi","title":"Initialize PyPI","text":"<pre><code>poetry config pypi-token.pypi &lt;token&gt;\n</code></pre>"},{"location":"Language/python/Venv/Poetry/#build-and-publish","title":"Build and Publish","text":"<pre><code>Remove-Item -Path 'dist/*' -Recurse -Force\n\npoetry publish --build\n</code></pre>"},{"location":"Language/python/Venv/UV/","title":"Installation","text":"<ul> <li>download</li> </ul> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <ul> <li>apply</li> </ul> <pre><code>source $HOME/.cargo/env\n</code></pre> <ul> <li>update</li> </ul> <pre><code>uv self update\n</code></pre> <ul> <li>auto auto completion</li> </ul> <pre><code># Determine your shell (e.g., with `echo $SHELL`), then run one of:\necho 'eval \"$(uv generate-shell-completion bash)\"' &gt;&gt; ~/.bashrc\necho 'eval \"$(uv generate-shell-completion zsh)\"' &gt;&gt; ~/.zshrc\necho 'uv generate-shell-completion fish | source' &gt;&gt; ~/.config/fish/config.fish\necho 'eval (uv generate-shell-completion elvish | slurp)' &gt;&gt; ~/.elvish/rc.elv\n</code></pre> <pre><code># Determine your shell (e.g., with `echo $SHELL`), then run one of:\necho 'eval \"$(uvx --generate-shell-completion bash)\"' &gt;&gt; ~/.bashrc\necho 'eval \"$(uvx --generate-shell-completion zsh)\"' &gt;&gt; ~/.zshrc\necho 'uvx --generate-shell-completion fish | source' &gt;&gt; ~/.config/fish/config.fish\necho 'eval (uvx --generate-shell-completion elvish | slurp)' &gt;&gt; ~/.elvish/rc.elv\n</code></pre> <ul> <li>Uninstallation</li> </ul> <pre><code>uv cache clean\nrm -r \"$(uv python dir)\"\nrm -r \"$(uv tool dir)\"\nrm ~/.cargo/bin/uv ~/.cargo/bin/uvx\n</code></pre> <ul> <li>enable Sudo</li> </ul> <pre><code>sudo ln -sf $(which uv) /usr/local/bin/\n</code></pre>"},{"location":"Language/python/Venv/UV/#python","title":"python","text":"<ul> <li><code>uv</code> will\u00a0automatically fetch Python versions\u00a0as needed \u2014 you don't need to install Python to get started.</li> <li>also it can manage your python envs</li> <li>Once Python is installed, it will be used by\u00a0<code>uv</code>\u00a0commands automatically.</li> <li>activate venv</li> </ul> <pre><code>uv sync\nsource .venv/bin/activate\n</code></pre> <p>activate in <code>zsh</code> automatically</p> <pre><code>if ! grep -q \"auto_activate_venv\" ~/.zshrc; then\n    echo '\nfunction auto_activate_venv() {\n    # finding .venv\n    local current_dir=\"$PWD\"\n    while [[ \"$current_dir\" != \"/\" ]]; do\n        if [[ -d \"$current_dir/.venv\" ]]; then\n            source \"$current_dir/.venv/bin/activate\"\n            # echo \"\ud83d\udc0d Virtual environment activated: $current_dir/.venv\"\n            return\n        fi\n        current_dir=\"$(dirname \"$current_dir\")\"\n    done\n}\n\n# Initial check when opening terminal\nauto_activate_venv' &gt;&gt; ~/.zshrc\n    echo \"Added auto_activate_venv to .zshrc\"\nelse\n    echo \"auto_activate_venv already exists in .zshrc\"\nfi\n</code></pre> <ul> <li>Finding a Python executable</li> </ul> <pre><code>uv python find\n</code></pre>"},{"location":"Language/python/Venv/UV/#tool","title":"Tool","text":"<p>package installed as tool would be manage in a special isolated environment</p> <p><code>uvx</code> == <code>uv tool run</code></p> <pre><code>ruff check\n</code></pre>"},{"location":"Language/python/Venv/UV/#dependencies","title":"Dependencies","text":""},{"location":"Language/python/Venv/UV/#scripts","title":"Scripts","text":"<p>we can add dependencies that are automatically managed by <code>uv</code> on calling:</p> <pre><code>uv add --script &lt;your_script.py&gt; \"&lt;package_a&gt;\" \"&lt;package_b&gt;\"\n</code></pre> <p>then we would find some the declaration of dependencies in your <code>your_script.py</code></p>"},{"location":"Language/python/Venv/UV/#project","title":"Project","text":"<p>we use <code>pyproject.toml</code> to manage python projects' dependencies.</p>"},{"location":"Language/python/Venv/UV/#add","title":"Add","text":"<p>Package name and version would be added into <code>pyproject.toml</code> while calling:</p> <pre><code>uv add requests\n</code></pre> <p>Specify a version constraint</p> <pre><code>uv add 'requests==2.31.0'\n</code></pre> <p>Add a <code>git</code> dependency</p> <pre><code>uv add requests --git https://github.com/psf/requests\n</code></pre> <p>There ate three sort of groups in <code>pyproject.toml</code></p> <ol> <li>we add package to <code>dependencies</code> of <code>[project]</code> normally</li> <li><code>dependency-groups</code> of <code>[tool.uv]</code> contains developments needed</li> <li><code>optional_group</code> of <code>[project.optional-dependencies]</code> contains special group needed</li> </ol> <pre><code># same to uv add --group dev pytest\nuv add pytest --dev\n</code></pre> <pre><code>uv add --group docs mkdocs\n</code></pre>"},{"location":"Language/python/Venv/UV/#sync","title":"Sync","text":"<p>sync all dependencies from <code>pyproject.toml</code></p> <pre><code>uv sync --all-extras --all-groups\n</code></pre> <p><code>sync</code> -&gt;<code>dependencies</code>, <code>--all-extras</code> -&gt; <code>--optional</code>, <code>--all-groups</code> -&gt; <code>dependency-groups</code></p>"},{"location":"Language/python/Venv/UV/#update","title":"Update","text":"<p>update all</p> <pre><code>uv lock --upgrade\n</code></pre> <p>update specified</p> <pre><code>uv lock --upgrade-package requests\n</code></pre> <p>remove</p> <pre><code>uv remove requests\n</code></pre>"},{"location":"Language/python/Venv/conda/","title":"Conda","text":""},{"location":"Language/python/Venv/conda/#install-conda","title":"Install Conda","text":"<ol> <li>install by wget</li> </ol> <pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nchmod +x Miniconda3-latest-Linux-x86_64.sh\n./Miniconda3-latest-Linux-x86_64.sh\n# note it may show that conda installed in /root/miniconda3\n</code></pre> <p>remember set no to auto activate in your shell if u use poetry,etc to manage py-env</p> <ol> <li>add <code>/root/miniconda3/bin</code> to <code>PATH</code> in <code>/etc/environment</code></li> </ol> <pre><code>PATH=\"/root/miniconda3/bin:\"\nsource /etc/environment\n# or\nexport PATH=\"/root/miniconda3/bin:$PATH\"\n</code></pre> <ol> <li>test conda</li> </ol> <pre><code>conda info\n</code></pre> <ol> <li>optional, enable auto activate conda</li> </ol> <pre><code>conda config --set auto_activate_base true\n</code></pre> <p>repeat to warn <code>conda init</code> ,try <code>conda init &lt;shell&gt;</code></p>"},{"location":"Language/python/Venv/conda/#channels","title":"Channels","text":"<ul> <li>3rd</li> </ul> <pre><code>conda config --add channels conda-forge\nconda config --set channel_priority strict\n</code></pre> <pre><code>conda config --show channels\n</code></pre> <ul> <li>search</li> </ul> <pre><code>conda search cudnn --channel conda-forge\n</code></pre>"},{"location":"Language/python/Venv/conda/#install-deps","title":"Install Deps","text":"<ul> <li>install from environment.yml</li> </ul> <pre><code>conda env create -f environment.yml\n</code></pre> <ul> <li>output current env deps to environment.yml</li> </ul> <pre><code>conda env export --from-history &gt; environment.yml\n</code></pre> <ul> <li>update from environment.yml <code>--prune</code> for auto update deps</li> </ul> <pre><code>conda env update -f environment.yml\n</code></pre> <ul> <li>install by auto update deps</li> </ul> <pre><code>conda install &lt;package&gt; --update-all\n</code></pre>"},{"location":"Language/python/Venv/conda/#info-check","title":"Info Check","text":"<ul> <li>show conda info</li> </ul> <pre><code>conda info\n</code></pre> <ul> <li>show all conda envs</li> </ul> <pre><code>conda env list\n</code></pre> <ul> <li>current env packages</li> </ul> <pre><code>conda list\n</code></pre> <ul> <li>specified env packages</li> </ul> <pre><code>conda list -n &lt;env&gt;\n</code></pre>"},{"location":"Learning/Computer%20Vison/3D/","title":"3D","text":""},{"location":"Learning/Computer%20Vison/3D/#camera","title":"Camera","text":"<p>References: - cameras \u00b7 PyTorch3D - Pinhole Camera - Kornia</p>"},{"location":"Learning/Computer%20Vison/3D/#camera-coordinate-systems","title":"Camera Coordinate Systems","text":"<p>Camera Coordinate System: A reference frame that changes with camera position and orientation. In the camera coordinate system, the origin is at the camera position, with the Z-axis typically aligned with the camera's line of sight, and X and Y axes aligned with the camera's horizontal and vertical axes respectively.</p> <p>World Coordinate System: A fixed, global reference frame used to describe object positions in a scene. In this coordinate system, each object's position is defined relative to a fixed point (world origin).</p>"},{"location":"Learning/Computer%20Vison/3D/#intrinsic-matrix","title":"Intrinsic Matrix","text":"<p>Camera Intrinsic Matrix with Example in Python | by Neeraj Krishna | Towards Data Science</p> <ul> <li><code>fx</code> and <code>fy</code>: Focal lengths in pixels along the image plane's x and y axes. These reflect the lens magnification of the scene. Ideally, for square pixels, <code>fx</code> and <code>fy</code> should be identical, but they may differ slightly due to lens distortion and manufacturing tolerances.</li> <li><code>cx</code> and <code>cy</code>: Principal point coordinates, representing the image coordinate system origin's position on the image plane. Typically assumed to be at the image center but may be offset due to imprecise lens manufacturing and assembly.</li> </ul> <p>The camera intrinsic matrix <code>K</code> is typically represented as:</p> \\[ K=\\begin{bmatrix}f_x&amp;0&amp;c_x\\\\0&amp;f_y&amp;c_y\\\\0&amp;0&amp;1\\end{bmatrix} \\]"},{"location":"Learning/Computer%20Vison/3D/#extrinsic-matrix","title":"Extrinsic Matrix","text":"<p>Camera Extrinsic Matrix with Example in Python | by Neeraj Krishna | Towards Data Science</p> <p>Describes the camera's position and orientation in global space (world coordinate system):</p> \\[ E_t=\\begin{bmatrix}R&amp;\\mathbf{t}\\end{bmatrix} \\] <ul> <li>Rotation: The <code>3x3</code> rotation matrix component enables object rotation around the origin. Rotations can be single-axis (around X, Y, or Z) or any combination thereof.</li> <li>Translation: The <code>Tx</code>, <code>Ty</code>, <code>Tz</code> elements enable object movement along each direction in 3D space.</li> </ul> <p>In practice, a <code>4x4</code> homogeneous transformation matrix is often used to handle both rotation and translation, simplifying calculations through a single matrix multiplication.</p>"},{"location":"Learning/Computer%20Vison/3D/#homogeneous-coordinate-transformation-matrix","title":"Homogeneous Coordinate Transformation Matrix","text":"\\[ \\mathbf{M}_{\\mathrm{c2w}}=\\begin{bmatrix}R&amp;T\\\\0&amp;1\\end{bmatrix} \\] <ul> <li>\\(R\\) (Rotation Matrix): Describes how the camera coordinate system's basis vectors (forward, up, and right directions) are rotated relative to the world coordinate system.</li> <li>\\(T\\) (Translation Vector): Represents the camera coordinate system's origin (camera's optical center) position in the world coordinate system.</li> </ul> <p>This matrix transforms points from camera coordinates to world coordinates through rotation by \\(R\\) followed by translation by \\(T\\).</p> <p>For transforming \\(P_w\\) to \\(P_c\\), we use \\(\\mathbf{M}_{\\mathrm{w2c}}\\), which can be obtained through inverse transformation: \\(\\mathbf{M}_{\\mathrm{w2c}}=\\mathbf{M}_{\\mathrm{c2w}}^{-1}\\)</p> <p>Trajectory files (<code>traj.txt</code>) typically contain <code>4x4</code> transformation matrices per line:</p> <pre><code>R11 R12 R13 Tx\nR21 R22 R23 Ty\nR31 R32 R33 Tz\n 0   0   0  1\n</code></pre>"},{"location":"Learning/Computer%20Vison/3D/#coordinate-transformation-during-imaging","title":"Coordinate Transformation During Imaging","text":"<p>The projection of 3D world coordinates to 2D image plane involves:</p> <ol> <li>World to Camera Coordinates: Using homogeneous extrinsic matrix \\(\\mathbf{M}_\\mathrm{w2c}=\\begin{bmatrix}R&amp;T\\\\0&amp;1\\end{bmatrix}\\), transform \\(P_w=(X_w,Y_w,Z_w,1)^T\\) to \\(\\mathbf{P}_c=(X_c,Y_c,Z_c)^T\\):</li> </ol> \\[ \\mathbf{P}_c=\\mathbf{M}_{\\mathrm{w2c}}\\cdot\\mathbf{P}_w \\] <ol> <li>Camera to Image Plane: Using intrinsic matrix \\(K\\), project \\(\\mathbf{P}_c=(X_c,Y_c,Z_c)^T\\) to image plane pixel \\(P_i=(u,v)\\):</li> </ol> \\[ \\mathbf{P}_i=\\mathbf{K}\\cdot\\begin{bmatrix}X_c\\\\Y_c\\\\Z_c\\end{bmatrix}/Z_c \\]"},{"location":"Learning/Computer%20Vison/Image/","title":"Image","text":""},{"location":"Learning/Computer%20Vison/Image/#image-coordinate-system","title":"Image Coordinate System","text":"<p> \u56fe\u50cf\u5750\u6807\u7cfb\u4e2d\uff1a</p> <ul> <li>\u539f\u70b9\uff080, 0\uff09\u4f4d\u4e8e\u56fe\u50cf\u7684\u5de6\u4e0a\u89d2\u3002</li> <li>__X \u8f74 __ \u7684\u6b63\u65b9\u5411\u662f\u5411\u53f3\uff0c\u8868\u793a\u56fe\u50cf\u7684\u5bbd\u5ea6\uff08w) \u65b9\u5411\u3002</li> <li>__Y \u8f74 __ \u7684\u6b63\u65b9\u5411\u662f\u5411\u4e0b\uff0c\u8868\u793a\u56fe\u50cf\u7684\u9ad8\u5ea6\uff08h\uff09\u65b9\u5411\u3002   reason   \u8fd9\u79cd\u5750\u6807\u7cfb\u7edf\u9009\u62e9\u4e3b\u8981\u662f\u57fa\u4e8e\u56fe\u50cf\u6570\u636e\u5728\u8ba1\u7b97\u673a\u5185\u5b58\u4e2d\u7684\u5b58\u50a8\u65b9\u5f0f\u3002\u5728\u5927\u591a\u6570\u56fe\u50cf\u5904\u7406\u5e93\u548c\u56fe\u5f62\u754c\u9762\u7cfb\u7edf\u4e2d\uff0c\u56fe\u50cf\u6570\u636e\u662f\u6309\u884c\u5b58\u50a8\u7684\uff0c\u6bcf\u884c\u4ece\u5de6\u5230\u53f3\uff0c\u884c\u4ece\u4e0a\u5230\u4e0b\u6392\u5217\u3002\u56e0\u6b64\uff0c\u56fe\u50cf\u7684\u7b2c\u4e00\u4e2a\u50cf\u7d20\uff08\u4f4d\u4e8e\u5de6\u4e0a\u89d2\uff09\u5bf9\u5e94\u4e8e\u5750\u6807\uff080, 0\uff09\u3002</li> </ul>"},{"location":"Learning/Computer%20Vison/Image/#color","title":"Color","text":"<ol> <li>channel first and RGB(red, green, blue)-&gt;pytorch,etc</li> </ol> <pre><code>image.shape = (RGB,Height,Weight)\n</code></pre> <ol> <li>channel not first and BGR(blue,green,red) for opencv,etc</li> </ol> <pre><code>image.shape = (Height,Weight,BGR)\n</code></pre> <ul> <li>normalize</li> </ul> <pre><code>image/255.\n[0,255] -&gt; [0,1.0]\n</code></pre>"},{"location":"Learning/Computer%20Vison/Image/#depth","title":"Depth","text":"<ol> <li>raw depth data from <code>.png</code> with shape(h,w)</li> </ol> <pre><code>depth_image = cv2.imread('depth_image.png', cv2.IMREAD_GRAYSCALE)\n</code></pre> <ol> <li>expand_dims for <code>pytorch</code></li> </ol> <pre><code>depth_tensor.unsqueeze(0)\nshape-&gt;(1,h,w)\n</code></pre>"},{"location":"Machine%20Learning/Data/","title":"Data","text":"<p>For machine learning tasks, high-quality, uniformly distributed data is essential, as it indicates the need to identify the location of the target subspace within the high-dimensional input data encoding space</p>"},{"location":"Machine%20Learning/Data/#preparation","title":"Preparation","text":"<p>Preparing our dataset by ingesting and splitting it.<sup>1</sup></p>"},{"location":"Machine%20Learning/Data/#ingestion","title":"Ingestion","text":""},{"location":"Machine%20Learning/Data/#io","title":"IO","text":"<ol> <li><code>pandas</code> -&gt; <code>.csv</code></li> <li><code>opencv</code>,<code>PIL</code>, <code>kornia.image</code> -&gt; <code>.jpg</code>,<code>.png</code></li> </ol>"},{"location":"Machine%20Learning/Data/#data-loader","title":"Data Loader","text":"<p>inherit <code>VisionDataset</code> and pass it to <code>DataLoader</code><sup>2</sup></p> <pre><code>from torchvision.datasets import VisionDataset\nclass TestDataset(VisionDataset):\n    pass\ntrain_dataset = TestDataset(...)\nfrom torch.utils.data import DataLoader\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=4,\n    pin_memory=True,\n)\n</code></pre> <p>finally,dataset is able to get into <code>trainer.fit()</code><sup>3</sup></p> <pre><code>import lightning as L\nfrom your_module import MyLightningModule\nmodel = MyLightningModule()\ntrainer = L.Trainer()\ntrainer.fit(model, train_dataloader, val_dataloader)\n</code></pre>"},{"location":"Machine%20Learning/Data/#splitting","title":"Splitting","text":"<p>define dataset type using lighting DataModule</p> <p>Cite</p> <p>we need to split our training dataset into <code>train</code> and <code>val</code> data splits.</p> <ol> <li>Use the <code>train</code> split to train the model.</li> </ol> <p>Here the model will have access to both inputs (features) and outputs (labels) to optimize its internal weights.</p> <ol> <li>After each iteration (epoch) through the training split, we will use the <code>val</code> split to determine the model's performance.</li> </ol> <p>Here the model will not use the labels to optimize its weights but instead, we will use the validation performance to optimize training hyperparameters such as the learning rate, etc.</p> <ol> <li>Finally, we will use a separate holdout <code>test</code> dataset to determine the model's performance after training.</li> </ol> <p>This is our best measure of how the model may behave on new, unseen data that is from a similar distribution to our training dataset.<sup>1</sup></p>"},{"location":"Machine%20Learning/Data/#exploratory-data-analysis-eda","title":"Exploratory Data Analysis (EDA)","text":"<p>Note</p> <p>Goal is to convince yourself that the data you have is sufficient for the task.<sup>4</sup></p>"},{"location":"Machine%20Learning/Data/#collect-data","title":"Collect Data","text":""},{"location":"Machine%20Learning/Data/#preprocessing","title":"Preprocessing","text":"<p>torchvision transform <sup>5</sup></p>"},{"location":"Machine%20Learning/Data/#vectorization","title":"Vectorization","text":"<ol> <li> <p>https://madewithml.com/courses/mlops/preparation/\u00a0\u21a9\u21a9</p> </li> <li> <p>https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#loading-a-dataset\u00a0\u21a9</p> </li> <li> <p>https://lightning.ai/docs/pytorch/stable/common/trainer.html#basic-use\u00a0\u21a9</p> </li> <li> <p>https://madewithml.com/courses/mlops/exploratory-data-analysis/\u00a0\u21a9</p> </li> <li> <p>https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_e2e.html#sphx-glr-auto-examples-transforms-plot-transforms-e2e-py\u00a0\u21a9</p> </li> </ol>"},{"location":"Machine%20Learning/basics/","title":"Basics","text":""},{"location":"Machine%20Learning/basics/#define-tasks","title":"Define Tasks","text":""},{"location":"Machine%20Learning/basics/#define","title":"Define","text":""},{"location":"Machine%20Learning/basics/#metrics","title":"Metrics","text":""},{"location":"OS/","title":"OS","text":"<p>Here for all kinds of operating system documents.\ud83d\udcbb</p>"},{"location":"OS/Bugs/","title":"Bugs","text":""},{"location":"OS/Bugs/#bugs_1","title":"Bugs","text":""},{"location":"OS/Bugs/#time-wrong-in-dul-system","title":"Time Wrong in Dul System","text":"<p>To change the RTC setting to UTC in Windows, you need to edit the registry:</p> <ol> <li>Press <code>Win + R</code>, type <code>regedit</code> and press Enter.</li> <li>Navigate to <code>HKEY_LOCAL_MACHINE\\System\\CurrentControlSet\\Control\\TimeZoneInformation</code>.</li> <li>Right-click the blank area and select <code>New</code> -&gt; <code>DWORD (32-bit) Value</code>.</li> <li>Name the new value <code>RealTimeIsUniversal</code>.</li> <li>Double-click the newly created <code>RealTimeIsUniversal</code>, enter <code>1</code> in the Value Data box, and click OK.</li> <li>Restart Windows. F. Restart Windows.</li> </ol>"},{"location":"OS/GPU/","title":"GPU","text":""},{"location":"OS/GPU/#ubuntu","title":"Ubuntu","text":""},{"location":"OS/GPU/#install-gpu-driver","title":"Install GPU Driver","text":""},{"location":"OS/GPU/#install-cudatoolkit-and-cudnn","title":"Install <code>cudatoolkit</code> and <code>cudnn</code>","text":"<pre><code>name: your env\nchannels:\n  - conda-forge\n  - nvidia\n  - defaults\ndependencies:\n  - python=3.10\n  - cudatoolkit=11.8\n  - cudnn=8.9.2.26\n</code></pre>"},{"location":"OS/GPU/#gpu-monitor","title":"GPU Monitor","text":"<pre><code>conda install -c conda-forge nvitop\n</code></pre>"},{"location":"OS/Ubuntu/","title":"Ubuntu","text":""},{"location":"OS/Ubuntu/Appearance/","title":"Appearance","text":""},{"location":"OS/Ubuntu/Appearance/#fonts","title":"Fonts","text":"<p>Install bold Nerd Fonts and Tweak your system fonts, sharp and slim fonts would easy your eyes.</p> <p><code>Comic Sans MS</code> in obsidian looks like handwriting fonts.</p>"},{"location":"OS/Ubuntu/Appearance/#theme","title":"Theme","text":"<ol> <li>theme_manager.sh</li> <li>WhiteSur-gtk-theme</li> </ol>"},{"location":"OS/Ubuntu/Appearance/#basic-extension","title":"Basic Extension","text":"<ul> <li>user-themes\u00a0to enable gnome-shell theme (and not just the application theme)</li> <li>dash-to-dock</li> <li>blur-my-shell</li> </ul>"},{"location":"OS/Ubuntu/Appearance/#install","title":"Install","text":"<pre><code>git clone git@github.com:vinceliuice/WhiteSur-gtk-theme.git\ncd WhiteSure-gtk-theme\n</code></pre> <pre><code>./install.sh --libadwaita \\\n-c Dark \\\n-t blue \\\n--gnome-shell \\\n--round\n</code></pre> <p>Note</p> <p>enable solid one in <code>Tweaks</code></p> <p>Flatpak</p> <pre><code>sudo flatpak override --filesystem=xdg-config/gtk-3.0 &amp;&amp; sudo flatpak override --filesystem=xdg-config/gtk-4.0\n</code></pre>"},{"location":"OS/Ubuntu/Appearance/#tweak","title":"Tweak","text":"<p>Firefox , GDE, Flatpak</p> <pre><code>./tweaks.sh -F \\\n-f \\\n--dash-to-dock \\\n--color Dark \\\n--theme blue\n</code></pre> <p>Lock Screen</p> <pre><code>sudo ./tweaks.sh -g -N\n</code></pre>"},{"location":"OS/Ubuntu/Appearance/#icons","title":"Icons","text":"<p>GitHub - vinceliuice/WhiteSur-icon-theme: MacOS Big Sure style icon theme for linux desktops</p> <pre><code>git clone git@github.com:vinceliuice/WhiteSur-icon-theme.git\ncd WhiteSure-icon-theme\n./install.sh -a -b\n</code></pre>"},{"location":"OS/Ubuntu/Appearance/#cursors","title":"Cursors","text":"<pre><code>git clone git@github.com:vinceliuice/McMojave-cursors.git\ncd McMojave-cursors\nsudo ./install.sh\n</code></pre>"},{"location":"OS/Ubuntu/Appearance/#wallpaper","title":"Wallpaper","text":"<p> OneDrive share link</p>"},{"location":"OS/Ubuntu/Gnome/","title":"Gnome","text":""},{"location":"OS/Ubuntu/Gnome/#manager","title":"Manager","text":"<p>Install extensions manager</p> <pre><code>sudo apt install gnome-shell-extension-manager\n</code></pre> <p>Or Install the GNOME Shell Integration browser extension</p> <ol> <li>FireFox</li> <li>Chrome</li> </ol>"},{"location":"OS/Ubuntu/Gnome/#tweaks","title":"Tweaks","text":"<p>tweak your fronts,etc</p> <pre><code>sudo apt install gnome-tweaks\n</code></pre>"},{"location":"OS/Ubuntu/Gnome/#phone","title":"Phone","text":"<pre><code>sudo apt install gnome-shell-extension-gsconnect\n</code></pre>"},{"location":"OS/Ubuntu/Gnome/#accelerate","title":"Accelerate","text":"<ul> <li>Impatience</li> </ul>"},{"location":"OS/Ubuntu/Package/","title":"Package","text":"<p>Repositories official tutorial</p>"},{"location":"OS/Ubuntu/Package/#apt-advanced-package-tool","title":"APT (Advanced Package Tool)","text":""},{"location":"OS/Ubuntu/Package/#basic-usage","title":"Basic Usage","text":"<ul> <li>Update package list: <code>sudo apt update</code></li> <li>Upgrade all packages: <code>sudo apt upgrade</code></li> <li>Install software: <code>sudo apt install &lt;package-name&gt;</code></li> <li>Uninstall software: <code>sudo apt remove &lt;package-name&gt;</code></li> <li>Search for software: <code>apt search &lt;keyword&gt;</code></li> <li>Display package information: <code>apt show &lt;package-name&gt;</code></li> </ul> <pre><code>sudo rm /etc/apt/sources.list.d/&lt;package&gt;\n</code></pre>"},{"location":"OS/Ubuntu/Package/#principles","title":"Principles","text":"<p>APT is an advanced package management tool that uses dpkg as its backend. APT can:</p> <ul> <li>Resolve dependencies</li> <li>Fetch packages from configured sources</li> <li>Perform complex package management operations</li> </ul>"},{"location":"OS/Ubuntu/Package/#ppa-configuration","title":"PPA Configuration","text":"<p>PPA (Personal Package Archive) allows developers and users to create their own software repositories.</p> <ul> <li>Add PPA: <code>sudo add-apt-repository ppa:&lt;repository-name&gt;</code></li> <li>Remove PPA: <code>sudo add-apt-repository --remove ppa:&lt;repository-name&gt;</code></li> </ul> <p>PPA source files are located in the <code>/etc/apt/sources.list.d/</code> directory.</p>"},{"location":"OS/Ubuntu/Package/#add-personal-repo","title":"Add Personal Repo","text":"<p>https://wiki.debian.org/DebianRepository/Format?action=show&amp;redirect=RepositoryFormat Here's how to configure GPG signing in CI:</p> <p>https://wiki.debian.org/DebianRepository/Setup</p> <p>https://wiki.debian.org/DebianRepository/Setup</p> <p>Generate GPG key (locally):</p> <pre><code>gpg --full-generate-key\ngpg --list-secret-keys --keyid-format LONG\ngpg --armor --export-secret-key YOUR_KEY_ID &gt; private.key\ngpg --armor --export YOUR_KEY_ID &gt; public.key\n</code></pre> <p>Add keys as GitHub Secrets: - Go to repo Settings -&gt; Secrets and variables -&gt; Actions -&gt; Repository secrets - Add two secrets     - <code>GPG_PRIVATE_KEY</code>: private key content     - <code>GPG_PASSPHRASE</code>: key passphrase</p> <p>GitHub - AtticusZeller/deb-index</p>"},{"location":"OS/Ubuntu/Package/#flatpak","title":"Flatpak","text":"<p>Flatpak is a system for building and distributing desktop applications.</p>"},{"location":"OS/Ubuntu/Package/#install-flatpak","title":"Install Flatpak","text":"<pre><code>sudo apt install flatpak\n</code></pre>"},{"location":"OS/Ubuntu/Package/#install-the-software-flatpak-plugin","title":"Install the Software Flatpak Plugin","text":"<p>The Flatpak plugin for the Software app makes it possible to install apps without needing the command line. To install, run:</p> <pre><code>sudo apt install gnome-software-plugin-flatpak\n</code></pre> <p>Note</p> <p>the Software app is distributed as a Snap since Ubuntu 20.04 and does not support graphical installation of Flatpak apps. Installing the Flatpak plugin will also install a deb version of Software and result in two Software apps being installed at the same time.</p>"},{"location":"OS/Ubuntu/Package/#add-the-flathub-repository","title":"Add the Flathub Repository","text":"<p>Flathub is the best place to get Flatpak apps. To enable it, run:</p> <pre><code>flatpak remote-add --if-not-exists flathub https://dl.flathub.org/repo/flathub.flatpakrepo\n</code></pre>"},{"location":"OS/Ubuntu/Package/#restart","title":"Restart","text":"<p>To complete setup, restart your system. Now all you have to do is install some apps!</p>"},{"location":"OS/Ubuntu/Package/#unsnap","title":"Unsnap","text":"<p>GitHub - popey/unsnap: Quickly migrate from using snap packages to flatpaks</p> <pre><code>git clone git@github.com:popey/unsnap.git\ncd unsnap\n./unsnap auto\n</code></pre>"},{"location":"OS/Ubuntu/Package/#local-management","title":"Local Management","text":"<p>GitHub - flattool/warehouse: A versatile toolbox for viewing flatpak info, managing user data, and batch managing installed flatpaks</p> <pre><code>flatpak install flathub io.github.flattool.Warehouse\n</code></pre>"},{"location":"OS/Ubuntu/Package/#override","title":"Override","text":"<p>global settings if no following with <code>flatpak list</code> got Application ID like<code>com.example.App</code></p> <pre><code>sudo flatpak override --filesystem=home\n</code></pre> <p>BUG</p> <ol> <li>use <code>flatpak-spawn --host --env=TERM=xterm-256color zsh</code> as shell path in <code>pycharm</code> Can't use zsh in terminal \u00b7 Issue #23 \u00b7 flathub/com.jetbrains.IntelliJ-IDEA-Ultimate \u00b7 GitHub</li> </ol>"},{"location":"OS/Ubuntu/Package/#homebrew","title":"Homebrew","text":"<pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre>"},{"location":"OS/Ubuntu/System/","title":"System","text":""},{"location":"OS/Ubuntu/System/#system_1","title":"System","text":""},{"location":"OS/Ubuntu/System/#install","title":"Install","text":"<ol> <li>download <code>ios file</code> Download Ubuntu Desktop | Download | Ubuntu</li> <li>download <code>Flash OS images app</code>balenaEtcher - Flash OS images to SD cards &amp; USB drives</li> <li>let device start by the flashed drivers</li> </ol>"},{"location":"OS/Ubuntu/System/#ignore-lid-switch","title":"Ignore Lid Switch","text":"<pre><code>sudo nano /etc/systemd/logind.conf\n# make sure\nHandleLidSwitch=ignore\nHandleLidSwitchExternalPower=ignore\nHandleLidSwitchDocked=ignore\n# then run\nsudo systemctl restart systemd-logind\n</code></pre>"},{"location":"OS/Ubuntu/System/#usermod","title":"usermod","text":"<p>add into <code>sudo</code> group</p> <pre><code>sudo usermod -aG sudo yourusername\n# check it\ngroups yourusername\n</code></pre>"},{"location":"OS/Ubuntu/System/#sudoers","title":"Sudoers","text":"<p>avoid be asked input passwd call <code>sudo</code> every time</p> <pre><code>sudo visudo\n# fine the line begin with %sudo and replace it\n%sudo ALL=NOPASSWD: ALL\n</code></pre>"},{"location":"OS/Ubuntu/System/#ssh-server","title":"SSH Server","text":""},{"location":"OS/Ubuntu/System/#install-and-enable","title":"Install and Enable","text":"<pre><code>sudo apt update\nsudo apt install openssh-server\nsudo systemctl start ssh\nsudo systemctl enable ssh\nsudo systemctl status ssh\n</code></pre>"},{"location":"OS/Ubuntu/System/#add-ssh-key","title":"Add Ssh Key","text":"<p>copy your key</p> <pre><code>cat ~/.ssh/id_ed25519.pub\n</code></pre> <p>write into ssh server</p> <pre><code>echo \"your key content\" &gt;&gt; ~/.ssh/authorized_keys\n</code></pre>"},{"location":"OS/Ubuntu/System/#add-ssh-configoptional","title":"Add Ssh Config(optional)","text":"<pre><code>code ~/.ssh/config\n</code></pre> <p>write the following into ssh config</p> <pre><code>Host ubuntu-laptop\n    Hostname 192.168.0.107\n    Port 22\n    User root\n</code></pre> <p>we assume <code>192.168.0.107</code> is your ssh server IP which could be found after running <code>ifconfig</code></p>"},{"location":"OS/Ubuntu/System/#connect","title":"Connect","text":"<pre><code># == ssh root@192.168.0.107\nssh ubuntu-laptop\n</code></pre>"},{"location":"OS/Ubuntu/System/#current-user","title":"Current User","text":"<pre><code> whoami\n</code></pre>"},{"location":"OS/Ubuntu/System/#display-server","title":"Display Server","text":"<p>enable wayland</p> <pre><code>code /etc/gdm3/custom.conf\n# WaylandEnable=true\n</code></pre>"},{"location":"OS/Ubuntu/System/#service","title":"Service","text":""},{"location":"OS/Ubuntu/System/#service_1","title":"Service","text":"<p>service.type</p> <p>It is recommended to use Type=exec for long-running services, as it ensures that process setup errors (e.g. errors such as a missing service executable, or missing user) are properly tracked.[1]</p> <pre><code>journalctl --since \"2025-01-14 08:09:00\" --until \"2025-01-14 08:09:30\"\n</code></pre>"},{"location":"OS/Ubuntu/System/#basic-tools-for-desktop","title":"Basic Tools for Desktop","text":""},{"location":"OS/Ubuntu/System/#browse","title":"Browse","text":""},{"location":"OS/Ubuntu/System/#firefox","title":"FireFox","text":"<pre><code>sudo snap remove firefox\n</code></pre> <pre><code>sudo install -d -m 0755 /etc/apt/keyrings\nwget -q https://packages.mozilla.org/apt/repo-signing-key.gpg -O- | sudo tee /etc/apt/keyrings/packages.mozilla.org.asc &gt; /dev/null\necho \"deb [signed-by=/etc/apt/keyrings/packages.mozilla.org.asc] https://packages.mozilla.org/apt mozilla main\" | sudo tee -a /etc/apt/sources.list.d/mozilla.list &gt; /dev/null\necho '\nPackage: *\nPin: origin packages.mozilla.org\nPin-Priority: 1000\n\nPackage: firefox*\nPin: release o=Ubuntu\nPin-Priority: -1' | sudo tee /etc/apt/preferences.d/mozilla\nsudo apt update &amp;&amp; sudo apt remove FireFox\nsudo apt install firefox\n</code></pre>"},{"location":"OS/Ubuntu/System/#settings","title":"Settings","text":"<ol> <li>enable Ctrl+Tab cycles through tabs in recently used order</li> <li>Fonts settings</li> </ol> <p>PWA plugin</p>"},{"location":"OS/Ubuntu/System/#use-eye-protection-mode","title":"Use Eye Protection Mode","text":"<p>setting-&gt; display-&gt; night light</p>"},{"location":"OS/Ubuntu/System/#keyboard","title":"Keyboard","text":"<pre><code>sudo apt-get update\nsudo apt-get install ibus ibus-pinyin ibus-libpinyin\n</code></pre> <p>Warning</p> <p>remember to reboot and add input resources in <code>setting-&gt;keyboard</code></p>"},{"location":"OS/Ubuntu/System/#system-backup","title":"System Backup","text":"<pre><code>sudo apt-get update\nsudo apt-get install timeshift\n</code></pre>"},{"location":"OS/Ubuntu/System/#clipboard","title":"Clipboard","text":"<p>GitHub - Tudmotu/gnome-shell-extension-clipboard-indicator: The most popular clipboard manager for GNOME, with over 1M downloads</p> <p>Clipboard Indicator - GNOME Shell Extensions</p> <p>Custom Shortcuts <code>setting-&gt;Keyboard-&gt;Keyboard Shortcuts-&gt;Custom Shortcuts</code> set as <code>win+alt+v</code></p>"},{"location":"OS/Ubuntu/System/#ocr-to-clipboard","title":"OCR to Clipboard","text":"<p>GitHub - tesseract-ocr/tesseract: Tesseract Open Source OCR Engine (main repository)</p> <pre><code>sudo apt-get update\nsudo apt-get install tesseract-ocr xclip gnome-screenshot\n\ncd DevSpace\ngit clone git@github.com:Atticuszz/scripts.git\nsudo chmod +x ./scripts/ocr_clipboard.sh\n</code></pre> <p>scripts/ocr_clipboard.sh at main \u00b7 Atticuszz/scripts \u00b7 GitHub <code>setting-&gt;keyboard-&gt;shortcuts</code></p> <ul> <li>command :<code>~/DevSpace/ocr_clipboard.sh</code></li> <li>shortcuts : <code>ctrl+win+Q</code></li> </ul>"},{"location":"OS/Ubuntu/System/#gpu-monitor","title":"Gpu Monitor","text":"<pre><code>uv tool install nvitop\n</code></pre>"},{"location":"OS/Ubuntu/System/#cpu-ram-swap-monitor","title":"CPU ,RAM, SWAP Monitor","text":"<pre><code>sudo apt install htop\n</code></pre>"},{"location":"OS/Ubuntu/System/#clean-hard-driver","title":"Clean Hard Driver","text":"<pre><code>sudo apt install ncdu\n</code></pre>"},{"location":"OS/Ubuntu/System/#fan-mode","title":"Fan Mode","text":"<p>use <code>Legion</code> in windows to set mode</p>"},{"location":"OS/Ubuntu/System/#screenshot","title":"Screenshot","text":"<p>Snipaste Downloads</p> <pre><code>sudo apt-get update\nsudo apt-get install fuse\nchmod +x /home/atticuszz/Downloads/Snipaste-2.8.9-Beta-x86_64.AppImage\n/home/atticuszz/Downloads/Snipaste-2.8.9-Beta-x86_64.AppImage --appimage-extract\n# create desktop app then\nmkdir -p ~/.config/autostart/\ncp ~/Desktop/Snipaste.desktop ~/.config/autostart/\n</code></pre>"},{"location":"OS/Ubuntu/System/#install-targz","title":"Install .tar.gz","text":"<pre><code>tar -xzf filepath\n</code></pre>"},{"location":"OS/Ubuntu/System/#apt-source-change","title":"Apt Source Change","text":"<pre><code>sudo nano /etc/apt/sources.list\nsudo apt update\nsudo apt upgrade\n</code></pre>"},{"location":"OS/Ubuntu/System/#dual-system-extend-disk-for-ubuntu","title":"Dual System Extend Disk for Ubuntu","text":"<p>tutorial</p>"},{"location":"OS/Ubuntu/System/#increase-swap-space-via-swap-file","title":"Increase Swap Space via Swap File","text":"<p>extend to 16gb for example</p> <pre><code>sudo swapoff -a\nsudo fallocate -l 16G /swapfile\n# if failed fallocate try\nsudo dd if=/dev/zero of=/swapfile bs=1G count=16\n\n# then\nsudo chmod 600 /swapfile\nsudo mkswap /swapfile\nsudo swapon /swapfile\n\n# Make it permanent\necho '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab\n# test it\nswapon --show\n-&gt;\n\u276f swapon --show\n\nNAME      TYPE SIZE USED PRIO\n/swapfile file  16G   0B   -2\n</code></pre>"},{"location":"OS/Ubuntu/System/#zip-file","title":"Zip File","text":"<p>Note:</p> <ul> <li>The <code>-r</code> option stands for \"recursive\" and tells <code>zip</code> to include all files and <code>subfolders</code> in the specified folder.</li> <li>If you want to exclude certain files or folders, you can use the <code>-x</code> option followed by the file or folder name. For example:</li> </ul> <pre><code>zip -r myfolder.zip myfolder -x myfolder/excluded_file.txt\n</code></pre>"},{"location":"OS/Ubuntu/System/#onedrive","title":"OneDrive","text":"<p>onedrive/docs/ubuntu-package-install.md at master \u00b7 abraunegg/onedrive \u00b7 GitHub</p> <p>install</p> <pre><code>sudo apt remove onedrive\nsudo rm /etc/systemd/user/default.target.wants/onedrive.service\nwget -qO - https://download.opensuse.org/repositories/home:/npreining:/debian-ubuntu-onedrive/xUbuntu_24.04/Release.key | gpg --dearmor | sudo tee /usr/share/keyrings/obs-onedrive.gpg &gt; /dev/null\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/obs-onedrive.gpg] https://download.opensuse.org/repositories/home:/npreining:/debian-ubuntu-onedrive/xUbuntu_24.10/ ./\" | sudo tee /etc/apt/sources.list.d/onedrive.list\nsudo apt-get update\nsudo apt install --no-install-recommends --no-install-suggests onedrive\n</code></pre>"},{"location":"OS/Ubuntu/System/#login","title":"Login","text":"<pre><code>onedrive\n</code></pre>"},{"location":"OS/Ubuntu/System/#config","title":"Config","text":"<pre><code>code ~/.config/onedrive\n</code></pre> <p>and fill it with following:</p> <pre><code># sync home\n\nsync_dir = \"~\"\n\n# Skip dot files\n\nskip_dotfiles = \"true\"\n\n# Skip github sync dir\n\nskip_dir = \"DevSpace\"\nskip_dir = \"miniconda3\"\nskip_dir = \"NVIDIA Nsight Compute\"\nskip_dir = \"snap\"\nskip_dir = \"cache\"\nskip_dir = \"temp\"\n\n# upload_only = \"true\"\n\n# https://github.com/abraunegg/onedrive/blob/master/docs/application-config-options.md#monitor_interval\nmonitor_interval = \"600\"\n\nthreads = \"16\"\n</code></pre>"},{"location":"OS/Ubuntu/System/#usage","title":"Usage","text":"<p>once</p> <pre><code>onedrive --sync --verbose\n</code></pre> <p>monitor</p> <pre><code>onedrive --monitor\n</code></pre> <p>monitor as startup</p> <pre><code>...\n</code></pre>"},{"location":"OS/Ubuntu/System/#compatibility-with-curl","title":"Compatibility with curl","text":"<pre><code>brew install curl\necho 'export PATH=\"/home/linuxbrew/.linuxbrew/opt/curl/bin:$PATH\"' &gt;&gt; ~/.zshrc\necho 'export LD_LIBRARY_PATH=\"/home/linuxbrew/.linuxbrew/opt/curl/lib:$LD_LIBRARY_PATH\"' &gt;&gt; ~/.zshrc\n</code></pre>"},{"location":"OS/Ubuntu/System/#zotero","title":"Zotero","text":"<p>GitHub - retorquere/zotero-deb: Packaged versions of Zotero and Juris-M for Debian-based systems</p> <pre><code>wget -qO- https://raw.githubusercontent.com/retorquere/zotero-deb/master/install.sh | sudo bash\nsudo apt update\nsudo apt install zotero\n</code></pre>"},{"location":"OS/Ubuntu/Terminal/","title":"Terminal","text":""},{"location":"OS/Ubuntu/Terminal/#zsh-and-oh-my-zsh","title":"Zsh and Oh My ZSH","text":"<p>Oh My Zsh is a framework for Zsh, the Z shell.[1]</p>"},{"location":"OS/Ubuntu/Terminal/#install-ohmyzsh","title":"Install ohmyzsh","text":"<pre><code>sudo apt install zsh\n# install oh my zsh\nsh -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\n</code></pre>"},{"location":"OS/Ubuntu/Terminal/#install-fonts","title":"Install Fonts","text":"<p>Info</p> <p>Nerd Fonts is a project that patches developer targeted fonts with a high number of glyphs (icons). Specifically to add a high number of extra glyphs from popular 'iconic fonts' such as Font Awesome, Devicons, Octicons, and others.[2]</p> <p>Download Nerd Fonts and update Font cache:</p> <pre><code>mkdir -p ~/.local/share/fonts\ncd ~/.local/share/fonts\nwget https://github.com/ryanoasis/nerd-fonts/releases/latest/download/JetBrainsMono.tar.xz\ntar -xf JetBrainsMono.tar.xz\nrm JetBrainsMono.tar.xz\nfc-cache -fv\n</code></pre> <p>select <code>JetBrainsMono &lt;Nerd fonts&gt;</code> in <code>preferences--&gt;profiles-&gt;Text-&gt;Custom font</code></p> <p>restart terminal and test icons</p> <pre><code>echo -e \"\\ue62b \\uf296 \\ue62b\"\n</code></pre> <p></p>"},{"location":"OS/Ubuntu/Terminal/#install-theme","title":"Install Theme","text":"<p>[!WARNING] <code>Nerd-fonts</code> is prepared for displaying the <code>powerlevel10k</code> icon</p> <pre><code># install as oh-my-zsh plugin\ngit clone --depth=1 https://github.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k\n</code></pre> <p>Open <code>~/.zshrc</code>, find the line that sets <code>ZSH_THEME</code>, and change its value to <code>\"powerlevel10k/powerlevel10k\"</code> as following:</p> <pre><code># correct the old ZSH_THEME or can not find p10k command\nZSH_THEME=\"powerlevel10k/powerlevel10k\"\n</code></pre> <p>then restart terminal and config themes</p> <pre><code>p10k configure\n</code></pre> <p>\ud83e\udd73 </p>"},{"location":"OS/Ubuntu/Terminal/#install-plugins","title":"Install Plugins","text":"<p>[!NOTE] You can update plugins manually use <code>omz update</code>, or the system will periodically prompt you for updates.</p> <p>install extra plugin from GitHub</p> <pre><code>git clone https://github.com/zsh-users/zsh-autosuggestions.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions\ngit clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting\ngit clone https://github.com/Pilaton/OhMyZsh-full-autoupdate.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/ohmyzsh-full-autoupdate\ngit clone https://github.com/fdellwing/zsh-bat.git $ZSH_CUSTOM/plugins/zsh-bat\n</code></pre> <p>activate the built-in or extra plugins in <code>~/.zshrc</code></p> <pre><code>plugins=(\n    command-not-found\n    extract\n    docker\n    history-substring-search\n    web-search\n    z\n    zsh-autosuggestions\n    zsh-syntax-highlighting\n    ohmyzsh-full-autoupdate\n)\n</code></pre> <p>Note</p> <p>plugin config must before the <code>source $ZSH/oh-my-zsh.sh</code></p>"},{"location":"OS/Ubuntu/Terminal/#command-not-found","title":"Command-not-found","text":"<p>this plugin uses the command-not-found package for Zsh to provide suggested packages to be installed if a command cannot be found.</p>"},{"location":"OS/Ubuntu/Terminal/#extract","title":"Extract","text":"<p>This plugin defines a function called <code>extract</code> that extracts the archive file you pass it, and it supports a wide variety of archive filetypes.</p>"},{"location":"OS/Ubuntu/Terminal/#history-substring-search","title":"History-substring-search","text":"<p>The Plugin enables searching through commands history by typing a partial match of previous command.</p> <p>Use \u2b06\ufe0f or \u2b07\ufe0f to match commands from history </p>"},{"location":"OS/Ubuntu/Terminal/#web-search","title":"Web-search","text":"<p>Open your browser with terminal search content </p>"},{"location":"OS/Ubuntu/Terminal/#z","title":"Z","text":"<p>Zsh-z is a command-line tool that allows you to jump quickly to directories that you have visited frequently or recently.[3]</p> <p> 1. <code>cd</code> needs the specific directory, keywords of your directory which visited before is enough for <code>z</code> 2. <code>tab</code> with <code>z</code> show the scope that the keywords would match. </p>"},{"location":"OS/Ubuntu/Terminal/#zsh-autosuggestions","title":"Zsh-autosuggestions","text":"<p>It suggests commands based on your history of previous commands and completions </p>"},{"location":"OS/Ubuntu/Terminal/#zsh-syntax-highlighting","title":"Zsh-syntax-highlighting","text":"<p>Alert the wrong syntax with red color </p>"},{"location":"OS/Ubuntu/Terminal/#zsh-bat","title":"Zsh-bat","text":"<p>This plugin will replace <code>cat</code> with <code>batcat</code></p> <pre><code>sudo apt install bat\n</code></pre> <p></p>"},{"location":"OS/Ubuntu/Terminal/#vs-code-terminal-config","title":"VS Code Terminal Config","text":"<p>To config VS Code terminal, add the following properties to your user\u00a0<code>settings.json</code> to enable <code>Zsh</code>and <code>monospace</code> fonts.</p> <p>[!NOTE] A restart is required for font changes to take effect.</p> <pre><code>{\n  \"terminal.integrated.fontFamily\": \"JetBrainsMono Nerd Font Mono\"\n  \"terminal.integrated.fontSize\": 14,\n  \"terminal.integrated.shellIntegration.enabled\": true,\n  \"terminal.integrated.defaultProfile.windows\": \"PowerShell\",\n  \"terminal.integrated.defaultProfile.linux\": \"zsh\"\n}\n</code></pre>"},{"location":"OS/Ubuntu/Terminal/#shortcut","title":"Shortcut","text":"<p>Use <code>Tab</code> to autocomplete file, directory, commands.</p> <p>Erase the whole line via <code>Ctrl+U</code>.</p> <p><code>Home</code> or <code>End</code> (<code>Ctrl+A</code>, <code>Ctrl+E</code>) can jump to the front or the end of the command.</p> <p><code>zsh-autosuggestions</code> with \u27a1\ufe0f gets the command from history, but sometimes the partial commands, words need to be modified to run,<code>Ctrl+U</code> remove the word before the cursor.</p> <p><code>Ctrl+D</code> terminates the current SSH connection.</p> <p><code>history-substring-search</code> allows us to use <code>keyword</code> with \u2b06\ufe0f, \u2b07\ufe0f to filter commands from history.</p> <p><code>Ctrl+C</code> to kill the running program.<code>Ctrl+Z</code> to suspend it.</p> <p>Use <code>Ctrl+Shift+C</code> and <code>Ctrl+Shift+V</code> to copy and paste.</p>"},{"location":"OS/Ubuntu/Terminal/#reference","title":"Reference","text":"<ol> <li>The Only 5 Zsh Plugins You Need</li> <li>ZSH + Oh My ZSH! on Windows with WSL</li> <li>12 Linux Terminal Shortcuts Every Power Linux User Must Know</li> <li>Plugins-oh my zsh</li> </ol>"},{"location":"OS/Windows/Powershell/","title":"Powershell","text":""},{"location":"OS/Windows/Powershell/#oh-my-posh","title":"Oh My Posh","text":"<pre><code>winget install JanDeDobbeleer.OhMyPosh -s winget\n$env:Path += \";C:\\Users\\user\\AppData\\Local\\Programs\\oh-my-posh\\bin\"\noh-my-posh font install\ncode  $PROFILE\n</code></pre> <pre><code>oh-my-posh init pwsh --config \"$env:POSH_THEMES_PATH\\peru.omp.json\" | Invoke-Expression\nImport-Module PSReadLine\nSet-PSReadLineOption -PredictionSource History\nSet-PSReadLineOption -PredictionViewStyle ListView\nImport-Module Terminal-Icons\nSet-PSReadLineKeyHandler -Chord Tab -Function MenuComplete\n</code></pre> <p>set <code>powershell</code> as default instead of <code>windows powershell</code> set front in config-&gt; default -&gt; appearance</p> <p>GitHub - marticliment/UniGetUI: UniGetUI: The Graphical Interface for your package managers. Could be terribly described as a package manager manager to manage your package managers</p> <pre><code>winget install --exact --id MartiCliment.UniGetUI --source winget\n</code></pre>"},{"location":"OS/Windows/Powershell/#tips","title":"Tips","text":""},{"location":"OS/Windows/Powershell/#kill-the-process-by-pid","title":"kill The Process by PID","text":"<pre><code>taskkill /PID &lt;PID&gt; /F\n</code></pre>"},{"location":"OS/Windows/Powershell/#find-out-the-process-occupied-the-port","title":"Find out the Process Occupied the port","text":"<p>the last line of output is PID</p> <pre><code>netstat -ano | findstr :&lt;port&gt;\n</code></pre>"},{"location":"OS/Windows/Powershell/#find-out-the-process-occupied-the-file","title":"Find out the Process Occupied the file","text":"<p>Sysinternals Suite - Microsoft Apps</p> <pre><code>handle.exe &lt;file path&gt;\n</code></pre>"},{"location":"OS/Windows/Powershell/#get-command-exe-path","title":"Get Command .exe Path","text":"<pre><code>where.exe poetry\n</code></pre> <p><code>-&gt;C:\\Users\\18317\\AppData\\Roaming\\Python\\Scripts\\poetry.exe</code></p>"},{"location":"OS/Windows/Powershell/#add-path","title":"Add Path","text":"<p>[!INFO] Especially for invoking commands through program itself instead of <code>pwsh</code></p> <pre><code># admin\n# use Tab to autocomlete to make sure path exit\n$newPath = \"C:\\Program Files\\xpdf-tools-win-4.05\\bin64\"\n$currentPath = [Environment]::GetEnvironmentVariable(\"Path\", [System.EnvironmentVariableTarget]::Machine)\nif ($currentPath -notlike \"*$newPath*\") {\n    $updatedPath = $currentPath + \";\" + $newPath\n    [Environment]::SetEnvironmentVariable(\"Path\", $updatedPath, [System.EnvironmentVariableTarget]::Machine)\n    Write-Output \"Path added successfully.\"\n} else {\n    Write-Output \"Path already exists in the system PATH.\"\n}\n# check path\n[Environment]::GetEnvironmentVariable(\"Path\", [System.EnvironmentVariableTarget]::Machine) -split \";\"\n</code></pre>"},{"location":"OS/Windows/Powershell/#using-bash-to-handle-shell","title":"Using <code>bash</code> to Handle Shell","text":"<pre><code>Add-Content -Path $PROFILE -Value \"`n`$env:PATH += `\";C:\\Program Files\\Git\\bin`\"\"\n</code></pre>"},{"location":"Reference/Finding%20and%20managing%20research%20papers%20a%20survey%20of%20tools%20and%20products/","title":"Finding and Managing Research Papers a Survey of Tools and Products","text":"<p>At the end of this post you can find a tl;dr with my suggestions for the most useful tools to improve your workflow with scientific papers.</p> <p>Major update (2020): We have released a tool for visually finding and exploring academic papers. See our launching blog post for Connected Papers!</p> <p>As researchers, especially in (overly) prolific fields like Deep Learning, we often find ourselves overwhelmed by the huge amount of papers to read and keep track of in our work. I think one big reason for this is insufficient use of existing tools and services that aim to make our life easier. Another reason is the lack of a really good product which meets all our needs under one interface, but that is a topic for another post.</p> <p>Lately I\u2019ve been getting into a new subfield of ML and got extremely frustrated with the process of prioritizing, reading and managing the relevant papers\u2026 I ended up looking for tools to help me deal with this overload and want to share with you the products and services that I\u2019ve found. The goal is to improve the workflow and quality of life of anyone who works with scientific papers.</p> <p>I will focus mainly on consumption of papers (as opposed to writing) and cover:</p> <ol> <li>Reference Managers (AKA paper library)</li> <li>Social platforms to share knowledge</li> <li>Automatic paper analysis to gain additional metadata (keywords, relevant datasets, important citations\u2026)</li> </ol>","tags":["scholar"]},{"location":"Reference/Finding%20and%20managing%20research%20papers%20a%20survey%20of%20tools%20and%20products/#reference-managers-aka-paper-library","title":"Reference Managers (AKA Paper library)","text":"<p>These are platforms where you can create and organize lists of all your past and future reading, add personal notes and share with a small group. The libraries are synced to the cloud which means your papers should be available anywhere. Think goodreads, but for papers. Choose one of the following:</p> <ol> <li>Mendeley: It\u2019s not the best looking product, but it has a freemium business model and supports multiple platforms including web, PC, Mac, and mobile. In addition to general paper notes you can annotate and highlight the PDFs directly. You pay for additional cloud storage (necessary after a few hundred papers).</li> <li>Paperpile: paid subscription (no free version), but looks and feels modern. Very easy to import your library there from other services. The library is synced to your own Google Drive, which is a plus. At the moment only works on a chrome browser.</li> <li>Zotero: A freemium and open source implementation where you pay for additional cloud storage. Similar to Mendeley but less versatile.</li> </ol> <p>There are more options, but these are the ones I have tried and they are all fine. If I had to choose one it would be Mendeley for its platform versatility and being freemium.</p> <p></p> <p>Mendeley\u2019s Interface</p>","tags":["scholar"]},{"location":"Reference/Finding%20and%20managing%20research%20papers%20a%20survey%20of%20tools%20and%20products/#arxiv-enhancers","title":"ArXiv Enhancers","text":"<p>ArXiv has been around since 1991, and generally changed very little over the last decade, while the publication volume has increased dramatically [1]. It is natural that today we have different requirements and needs from our primary repository of papers. We want algorithms that perform paper analysis, we want to find code which implements the papers, we want a social layer via which we can share information, and perhaps we don\u2019t want to squint at a double-columned pdf.</p> <p>Searching the internet for existing solutions, I found many such tools:</p>","tags":["scholar"]},{"location":"Reference/Finding%20and%20managing%20research%20papers%20a%20survey%20of%20tools%20and%20products/#social-layers","title":"Social Layers","text":"<ol> <li>Shortscience: A platform for sharing paper summaries; Currently over 1000 summaries and growing. Works for any paper with a DOI (so, more than arXiv).</li> <li>OpenReview: A transparent paper review process which is open to public reviews as well, currently available only for selected conferences such as NIPS and ICLR. In addition to the official reviews, recently many papers there are seeing active conversations with responses from the original authors.</li> <li>Scirate: Adds a like (ehh, \u201cscite\u201d) button over a clone of arXiv. Adds a comment section. Mostly inactive.</li> </ol>","tags":["scholar"]},{"location":"Reference/Finding%20and%20managing%20research%20papers%20a%20survey%20of%20tools%20and%20products/#find-code-implementation-of-papers","title":"Find Code Implementation of Papers","text":"<ol> <li>Papers With Code: Automatically connects papers to github repositories that implement them and sorts by github stars. There can be multiple, unmerged entries for each paper.</li> <li>Github pwc: A minimalistic approach which automatically(?) connects papers to only one code implementation, displayed as a simple table.</li> <li>GitXiv: Collaboratively curated feed of projects. Each project is conveniently presented as arXiv + Github + Links + Discussion. Unfortunately this project is no longer maintained.</li> </ol> <p>Some links from Github pwc</p>","tags":["scholar"]},{"location":"Reference/Finding%20and%20managing%20research%20papers%20a%20survey%20of%20tools%20and%20products/#other","title":"Other","text":"<ol> <li>arXiv-sanity: Gives a makeover to arXiv with exposed abstracts, paper previews and very basic social and library features. A valiant attempt at tying many of the ideas above together, built in spare-time by Andrej Karpathy. The ideas are all there, but in my opinion the implementation isn\u2019t good enough to become a go-to tool for researchers, and the project has not been very active in the past year.</li> <li>arXiv-vanity : Renders academic papers from arXiv as responsive web pages so you don\u2019t have to squint at a PDF.</li> </ol>","tags":["scholar"]},{"location":"Reference/Finding%20and%20managing%20research%20papers%20a%20survey%20of%20tools%20and%20products/#paper-search-and-analysis","title":"Paper search and Analysis","text":"<ol> <li>Google scholar: Today\u2019s go-to place to search for papers, view paper statistics as well as citations and references, set up alerts for new papers by following an author or a paper, and keep a basic library with automatic recommendations.</li> <li>IBM Science Summarizer: Summaries are generated by analyzing the content of papers, as well as their structure, sections, paragraphs, and key terms. It doesn\u2019t always work well, but it\u2019s continually improving and is great for skimming papers quickly.</li> <li>Semantic scholar: Semantic analysis of papers with external material aggregation. Features include: expose citations and references and measure their impact, show paper figures, automatically generate keywords (topics), analyze authors, find additional resources on the internet (e.g. related youtube videos) and suggest recommended papers. A great new effort supported by AI2. Lately they\u2019ve done a small integration with Paperswithcode mentioned above and with arXiv itself (!).</li> </ol> <p>Semantic Scholar: author profile page</p>","tags":["scholar"]},{"location":"Reference/Finding%20and%20managing%20research%20papers%20a%20survey%20of%20tools%20and%20products/#tools-for-authors","title":"Tools for Authors","text":"<ol> <li>Overleaf: Collaborative, online LaTeX editor. Think Google docs for writing papers. Very well implemented.</li> <li>Authorea: A 21st century approach to collaboratively writing papers online, aiming to mostly drop LaTeX in favor of a modern WYSIWYG editor. Supports inline code and data for reproducibility, inline public comments, and other features that make perfect sense.</li> <li>Code ocean: A cloud-based computational reproducibility platform. My understanding is that you upload your research as a Jupyter environment code, run it online and reproduce the same graphs /output that the authors get. Here\u2019s an example (press Run at the top right).</li> </ol>","tags":["scholar"]},{"location":"Reference/Finding%20and%20managing%20research%20papers%20a%20survey%20of%20tools%20and%20products/#tldr-my-recommendations","title":"Tl;dr \u2014 My recommendations","text":"<ul> <li>Manage your reading library: Mendeley</li> <li>Read and write paper reviews: shortscience and openreview</li> <li>Match papers to github repositories: paperswithcode and pwc</li> <li>Paper and author analysis: Semantic scholar</li> <li>Write papers: Overleaf</li> </ul> <p>I hope this post has introduced you to at least one service that will improve your workflow. Please, if you know any helpful tools which haven\u2019t been mentioned in this post, share them below for everyone\u2019s benefit.</p> <p>[1] By October 2016 the submission rate had grown to more than 10,000 per month. https://en.wikipedia.org/wiki/ArXiv,</p> <p></p> <p>arXiv submission by Topic, from their statistics page</p>","tags":["scholar"]},{"location":"Reference/blog/Steph%20Ango/What%20can%20we%20remove/","title":"What Can We Remove","text":"<p>Our bias is to always add more. More rules, more process, more code, more features, more stuff. Interdependencies proliferate, and gradually strangle us. Systems want to grow and grow, but without pruning, they collapse. Slowly, then spectacularly.</p> <p>When a piece of trash drifts across the beach, it is our duty to pick it up so the next person can enjoy a pristine shoreline. When a thousand pieces litter the beach, it is too late. We can only lament the landscape. That\u2019s just how beaches are now.</p> <p>A good system is designed to be periodically cleared of cruft. It has a built-in counterbalance. Without this pressure, our bias drives us to add band-aid after band-aid, until the only choice is to destroy the whole system and start from scratch.</p> <p>Why is it so much easier to add than to remove? Maybe because we attach our identity to what is visible. But there is a difference between the ornamentation that defines our style and the vestigial burdens we carry.</p> <p>Remember those who did the invisible work of removing. Their legacy was not to build a sand castle, but to care for the beautiful beach on which we play.</p>","tags":["blog"]},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/","title":"A Docker Tutorial for Beginners","text":""},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/#introduction","title":"Introduction","text":""},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/#what-is-docker","title":"What is Docker?","text":"<p>Wikipedia defines Docker as</p> <p>an open-source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux.</p> <p>Wow! That's a mouthful. In simpler words, Docker is a tool that allows developers, sys-admins etc. to easily deploy their applications in a sandbox (called containers) to run on the host operating system i.e. Linux. The key benefit of Docker is that it allows users to package an application with all of its dependencies into a standardized unit for software development. Unlike virtual machines, containers do not have high overhead and hence enable more efficient usage of the underlying system and resources.</p>"},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/#what-are-containers","title":"What Are Containers?","text":"<p>The industry standard today is to use Virtual Machines (VMs) to run software applications. VMs run applications inside a guest Operating System, which runs on virtual hardware powered by the server\u2019s host OS.</p> <p>VMs are great at providing full process isolation for applications: there are very few ways a problem in the host operating system can affect the software running in the guest operating system, and vice-versa. But this isolation comes at great cost \u2014 the computational overhead spent virtualizing hardware for a guest OS to use is substantial.</p> <p>Containers take a different approach: by leveraging the low-level mechanics of the host operating system, containers provide most of the isolation of virtual machines at a fraction of the computing power.</p>"},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/#why-use-containers","title":"Why Use Containers?","text":"<p>Containers offer a logical packaging mechanism in which applications can be abstracted from the environment in which they actually run. This decoupling allows container-based applications to be deployed easily and consistently, regardless of whether the target environment is a private data center, the public cloud, or even a developer\u2019s personal laptop. This gives developers the ability to create predictable environments that are isolated from the rest of the applications and can be run anywhere.</p> <p>From an operations standpoint, apart from portability containers also give more granular control over resources giving your infrastructure improved efficiency which can result in better utilization of your compute resources.</p> <p></p> <p>Google Trends for Docker</p> <p>Due to these benefits, containers (&amp; Docker) have seen widespread adoption. Companies like Google, Facebook, Netflix and Salesforce leverage containers to make large engineering teams more productive and to improve utilization of compute resources. In fact, Google credited containers for eliminating the need for an entire data center.</p>"},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/#what-will-this-tutorial-teach-me","title":"What Will This Tutorial Teach Me?","text":"<p>This tutorial aims to be the one-stop shop for getting your hands dirty with Docker. Apart from demystifying the Docker landscape, it'll give you hands-on experience with building and deploying your own webapps on the Cloud. We'll be using Amazon Web Services to deploy a static website, and two dynamic webapps on EC2 using Elastic Beanstalk and Elastic Container Service. Even if you have no prior experience with deployments, this tutorial should be all you need to get started.</p>"},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/#getting-started","title":"Getting Started","text":"<p>This document contains a series of several sections, each of which explains a particular aspect of Docker. In each section, we will be typing commands (or writing code). All the code used in the tutorial is available in the Github repo.</p> <p>Note: This tutorial uses version 18.05.0-ce of Docker. If you find any part of the tutorial incompatible with a future version, please raise an issue. Thanks!</p>"},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/#prerequisites","title":"Prerequisites","text":"<p>There are no specific skills needed for this tutorial beyond a basic comfort with the command line and using a text editor. This tutorial uses <code>git clone</code> to clone the repository locally. If you don't have Git installed on your system, either install it or remember to manually download the zip files from Github. Prior experience in developing web applications will be helpful but is not required. As we proceed further along the tutorial, we'll make use of a few cloud services. If you're interested in following along, please create an account on each of these websites:</p> <ul> <li>Amazon Web Services</li> <li>Docker Hub</li> </ul>"},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/#setting-up-your-computer","title":"Setting up Your Computer","text":"<p>Getting all the tooling setup on your computer can be a daunting task, but thankfully as Docker has become stable, getting Docker up and running on your favorite OS has become very easy.</p> <p>Until a few releases ago, running Docker on OSX and Windows was quite a hassle. Lately however, Docker has invested significantly into improving the on-boarding experience for its users on these OSes, thus running Docker now is a cakewalk. The getting started guide on Docker has detailed instructions for setting up Docker on Mac, Linux and Windows.</p> <p>Once you are done installing Docker, test your Docker installation by running the following:</p> <pre><code>$ docker run hello-world\n\nHello from Docker.\nThis message shows that your installation appears to be working correctly.\n...\n</code></pre>"},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/#hello-world","title":"Hello World","text":""},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/#playing-with-busybox","title":"Playing with Busybox","text":"<p>Now that we have everything setup, it's time to get our hands dirty. In this section, we are going to run a Busybox container on our system and get a taste of the <code>docker run</code> command.</p> <p>To get started, let's run the following in our terminal:</p> <pre><code>$ docker pull busybox\n</code></pre> <p>Note: Depending on how you've installed docker on your system, you might see a <code>permission denied</code> error after running the above command. If you're on a Mac, make sure the Docker engine is running. If you're on Linux, then prefix your <code>docker</code> commands with <code>sudo</code>. Alternatively, you can create a docker group to get rid of this issue.</p> <p>The <code>pull</code> command fetches the busybox image from the Docker registry and saves it to our system. You can use the <code>docker images</code> command to see a list of all images on your system.</p> <pre><code>$ docker images\nREPOSITORY              TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nbusybox                 latest              c51f86c28340        4 weeks ago         1.109 MB\n</code></pre>"},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/#docker-run","title":"Docker Run","text":"<p>Great! Let's now run a Docker container based on this image. To do that we are going to use the almighty <code>docker run</code> command.</p> <pre><code>$ docker run busybox\n$\n</code></pre> <p>Wait, nothing happened! Is that a bug? Well, no. Behind the scenes, a lot of stuff happened. When you call <code>run</code>, the Docker client finds the image (busybox in this case), loads up the container and then runs a command in that container. When we run <code>docker run busybox</code>, we didn't provide a command, so the container booted up, ran an empty command and then exited. Well, yeah - kind of a bummer. Let's try something more exciting.</p> <pre><code>$ docker run busybox echo \"hello from busybox\"\nhello from busybox\n</code></pre> <p>Nice - finally we see some output. In this case, the Docker client dutifully ran the <code>echo</code> command in our busybox container and then exited it. If you've noticed, all of that happened pretty quickly. Imagine booting up a virtual machine, running a command and then killing it. Now you know why they say containers are fast! Ok, now it's time to see the <code>docker ps</code> command. The <code>docker ps</code> command shows you all containers that are currently running.</p> <pre><code>$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n</code></pre> <p>Since no containers are running, we see a blank line. Let's try a more useful variant: <code>docker ps -a</code></p> <pre><code>$ docker ps -a\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                      PORTS               NAMES\n305297d7a235        busybox             \"uptime\"            11 minutes ago      Exited (0) 11 minutes ago                       distracted_goldstine\nff0a5c3750b9        busybox             \"sh\"                12 minutes ago      Exited (0) 12 minutes ago                       elated_ramanujan\n14e5bd11d164        hello-world         \"/hello\"            2 minutes ago       Exited (0) 2 minutes ago                        thirsty_euclid\n</code></pre> <p>So what we see above is a list of all containers that we ran. Do notice that the <code>STATUS</code> column shows that these containers exited a few minutes ago.</p> <p>You're probably wondering if there is a way to run more than just one command in a container. Let's try that now:</p> <pre><code>$ docker run -it busybox sh\n/ # ls\nbin   dev   etc   home  proc  root  sys   tmp   usr   var\n/ # uptime\n 05:45:21 up  5:58,  0 users,  load average: 0.00, 0.01, 0.04\n</code></pre> <p>Running the <code>run</code> command with the <code>-it</code> flags attaches us to an interactive tty in the container. Now we can run as many commands in the container as we want. Take some time to run your favorite commands.</p> <p>Danger Zone: If you're feeling particularly adventurous you can try <code>rm -rf bin</code> in the container. Make sure you run this command in the container and not in your laptop/desktop. Doing this will make any other commands like <code>ls</code>, <code>uptime</code> not work. Once everything stops working, you can exit the container (type <code>exit</code> and press Enter) and then start it up again with the <code>docker run -it busybox sh</code> command. Since Docker creates a new container every time, everything should start working again.</p> <p>That concludes a whirlwind tour of the mighty <code>docker run</code> command, which would most likely be the command you'll use most often. It makes sense to spend some time getting comfortable with it. To find out more about <code>run</code>, use <code>docker run --help</code> to see a list of all flags it supports. As we proceed further, we'll see a few more variants of <code>docker run</code>.</p> <p>Before we move ahead though, let's quickly talk about deleting containers. We saw above that we can still see remnants of the container even after we've exited by running <code>docker ps -a</code>. Throughout this tutorial, you'll run <code>docker run</code> multiple times and leaving stray containers will eat up disk space. Hence, as a rule of thumb, I clean up containers once I'm done with them. To do that, you can run the <code>docker rm</code> command. Just copy the container IDs from above and paste them alongside the command.</p> <pre><code>$ docker rm 305297d7a235 ff0a5c3750b9\n305297d7a235\nff0a5c3750b9\n</code></pre> <p>On deletion, you should see the IDs echoed back to you. If you have a bunch of containers to delete in one go, copy-pasting IDs can be tedious. In that case, you can simply run -</p> <pre><code>$ docker rm $(docker ps -a -q -f status=exited)\n</code></pre> <p>This command deletes all containers that have a status of <code>exited</code>. In case you're wondering, the <code>-q</code> flag, only returns the numeric IDs and <code>-f</code> filters output based on conditions provided. One last thing that'll be useful is the <code>--rm</code> flag that can be passed to <code>docker run</code> which automatically deletes the container once it's exited from. For one off docker runs, <code>--rm</code> flag is very useful.</p> <p>In later versions of Docker, the <code>docker container prune</code> command can be used to achieve the same effect.</p> <pre><code>$ docker container prune\nWARNING! This will remove all stopped containers.\nAre you sure you want to continue? [y/N] y\nDeleted Containers:\n4a7f7eebae0f63178aff7eb0aa39f0627a203ab2df258c1a00b456cf20063\nf98f9c2aa1eaf727e4ec9c0283bcaa4762fbdba7f26191f26c97f64090360\n\nTotal reclaimed space: 212 B\n</code></pre> <p>Lastly, you can also delete images that you no longer need by running <code>docker rmi</code>.</p>"},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/#terminology","title":"Terminology","text":"<p>In the last section, we used a lot of Docker-specific jargon which might be confusing to some. So before we go further, let me clarify some terminology that is used frequently in the Docker ecosystem.</p> <ul> <li>Images - The blueprints of our application which form the basis of containers. In the demo above, we used the <code>docker pull</code> command to download the busybox image.</li> <li>Containers - Created from Docker images and run the actual application. We create a container using <code>docker run</code> which we did using the busybox image that we downloaded. A list of running containers can be seen using the <code>docker ps</code> command.</li> <li>Docker Daemon - The background service running on the host that manages building, running and distributing Docker containers. The daemon is the process that runs in the operating system which clients talk to.</li> <li>Docker Client - The command line tool that allows the user to interact with the daemon. More generally, there can be other forms of clients too - such as docker-desktop which provide a GUI to the users.</li> <li>Docker Hub - A registry of Docker images. You can think of the registry as a directory of all available Docker images. If required, one can host their own Docker registries and can use them for pulling images.</li> </ul>"},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/#webapps-with-docker","title":"Webapps with Docker","text":"<p>Great! So we have now looked at <code>docker run</code>, played with a Docker container and also got a hang of some terminology. Armed with all this knowledge, we are now ready to get to the real-stuff, i.e. deploying web applications with Docker!</p>"},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/#static-sites","title":"Static Sites","text":"<p>Let's start by taking baby-steps. The first thing we're going to look at is how we can run a dead-simple static website. We're going to pull a Docker image from Docker Hub, run the container and see how easy it is to run a webserver.</p> <p>Let's begin. The image that we are going to use is a single-page website that I've already created for the purpose of this demo and hosted on the registry - <code>prakhar1989/static-site</code>. We can download and run the image directly in one go using <code>docker run</code>. As noted above, the <code>--rm</code> flag automatically removes the container when it exits and the <code>-it</code> flag specifies an interactive terminal which makes it easier to kill the container with Ctrl+C (on windows).</p> <pre><code>$ docker run --rm -it prakhar1989/static-site\n</code></pre> <p>Since the image doesn't exist locally, the client will first fetch the image from the registry and then run the image. If all goes well, you should see a <code>Nginx is running\u2026</code> message in your terminal. Okay now that the server is running, how to see the website? What port is it running on? And more importantly, how do we access the container directly from our host machine? Hit Ctrl+C to stop the container.</p> <p>Well, in this case, the client is not exposing any ports so we need to re-run the <code>docker run</code> command to publish ports. While we're at it, we should also find a way so that our terminal is not attached to the running container. This way, you can happily close your terminal and keep the container running. This is called detached mode.</p> <pre><code>$ docker run -d -P --name static-site prakhar1989/static-site\ne61d12292d69556eabe2a44c16cbd54486b2527e2ce4f95438e504afb7b02810\n</code></pre> <p>In the above command, <code>-d</code> will detach our terminal, <code>-P</code> will publish all exposed ports to random ports and finally <code>--name</code> corresponds to a name we want to give. Now we can see the ports by running the <code>docker port [CONTAINER]</code> command</p> <pre><code>$ docker port static-site\n80/tcp -&gt; 0.0.0.0:32769\n443/tcp -&gt; 0.0.0.0:32768\n</code></pre> <p>You can open http://localhost:32769 in your browser.</p> <p>Note: If you're using docker-toolbox, then you might need to use <code>docker-machine ip default</code> to get the IP.</p> <p>You can also specify a custom port to which the client will forward connections to the container.</p> <pre><code>$ docker run -p 8888:80 prakhar1989/static-site\nNginx is running...\n</code></pre> <p></p> <p>To stop a detached container, run <code>docker stop</code> by giving the container ID. In this case, we can use the name <code>static-site</code> we used to start the container.</p> <pre><code>$ docker stop static-site\nstatic-site\n</code></pre> <p>I'm sure you agree that was super simple. To deploy this on a real server you would just need to install Docker, and run the above Docker command. Now that you've seen how to run a webserver inside a Docker image, you must be wondering - how do I create my own Docker image? This is the question we'll be exploring in the next section.</p>"},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/#docker-images","title":"Docker Images","text":"<p>We've looked at images before, but in this section we'll dive deeper into what Docker images are and build our own image! Lastly, we'll also use that image to run our application locally and finally deploy on AWS to share it with our friends! Excited? Great! Let's get started.</p> <p>Docker images are the basis of containers. In the previous example, we pulled the Busybox image from the registry and asked the Docker client to run a container based on that image. To see the list of images that are available locally, use the <code>docker images</code> command.</p> <pre><code>$ docker images\nREPOSITORY                      TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nprakhar1989/catnip              latest              c7ffb5626a50        2 hours ago         697.9 MB\nprakhar1989/static-site         latest              b270625a1631        21 hours ago        133.9 MB\npython                          3-onbuild           cf4002b2c383        5 days ago          688.8 MB\nmartin/docker-cleanup-volumes   latest              b42990daaca2        7 weeks ago         22.14 MB\nubuntu                          latest              e9ae3c220b23        7 weeks ago         187.9 MB\nbusybox                         latest              c51f86c28340        9 weeks ago         1.109 MB\nhello-world                     latest              0a6ba66e537a        11 weeks ago        960 B\n</code></pre> <p>The above gives a list of images that I've pulled from the registry, along with ones that I've created myself (we'll shortly see how). The <code>TAG</code> refers to a particular snapshot of the image and the <code>IMAGE ID</code> is the corresponding unique identifier for that image.</p> <p>For simplicity, you can think of an image akin to a git repository - images can be committed with changes and have multiple versions. If you don't provide a specific version number, the client defaults to <code>latest</code>. For example, you can pull a specific version of <code>ubuntu</code> image</p> <pre><code>$ docker pull ubuntu:18.04\n</code></pre> <p>To get a new Docker image you can either get it from a registry (such as the Docker Hub) or create your own. There are tens of thousands of images available on Docker Hub. You can also search for images directly from the command line using <code>docker search</code>.</p> <p>An important distinction to be aware of when it comes to images is the difference between base and child images.</p> <ul> <li>Base images are images that have no parent image, usually images with an OS like ubuntu, busybox or debian.</li> <li>Child images are images that build on base images and add additional functionality.</li> </ul> <p>Then there are official and user images, which can be both base and child images.</p> <ul> <li>Official images are images that are officially maintained and supported by the folks at Docker. These are typically one word long. In the list of images above, the <code>python</code>, <code>ubuntu</code>, <code>busybox</code> and <code>hello-world</code> images are official images.</li> <li>User images are images created and shared by users like you and me. They build on base images and add additional functionality. Typically, these are formatted as <code>user/image-name</code>.</li> </ul>"},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/#our-first-image","title":"Our First Image","text":"<p>Now that we have a better understanding of images, it's time to create our own. Our goal in this section will be to create an image that sandboxes a simple Flask application. For the purposes of this workshop, I've already created a fun little Flask app that displays a random cat <code>.gif</code> every time it is loaded - because you know, who doesn't like cats? If you haven't already, please go ahead and clone the repository locally like so -</p> <pre><code>$ git clone https://github.com/prakhar1989/docker-curriculum.git\n$ cd docker-curriculum/flask-app\n</code></pre> <p>This should be cloned on the machine where you are running the docker commands and not inside a docker container.</p> <p>The next step now is to create an image with this web app. As mentioned above, all user images are based on a base image. Since our application is written in Python, the base image we're going to use will be Python 3.</p>"},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/#dockerfile","title":"Dockerfile","text":"<p>A Dockerfile is a simple text file that contains a list of commands that the Docker client calls while creating an image. It's a simple way to automate the image creation process. The best part is that the commands you write in a Dockerfile are almost identical to their equivalent Linux commands. This means you don't really have to learn new syntax to create your own dockerfiles.</p> <p>The application directory does contain a Dockerfile but since we're doing this for the first time, we'll create one from scratch. To start, create a new blank file in our favorite text-editor and save it in the same folder as the flask app by the name of <code>Dockerfile</code>.</p> <p>We start with specifying our base image. Use the <code>FROM</code> keyword to do that -</p> <pre><code>FROM python:3.8\n</code></pre> <p>The next step usually is to write the commands of copying the files and installing the dependencies. First, we set a working directory and then copy all the files for our app.</p> <pre><code># set a directory for the app\nWORKDIR /usr/src/app\n\n# copy all the files to the container\nCOPY . .\n</code></pre> <p>Now, that we have the files, we can install the dependencies.</p> <pre><code># install dependencies\nRUN pip install --no-cache-dir -r requirements.txt\n</code></pre> <p>The next thing we need to specify is the port number that needs to be exposed. Since our flask app is running on port <code>5000</code>, that's what we'll indicate.</p> <pre><code>EXPOSE 5000\n</code></pre> <p>The last step is to write the command for running the application, which is simply - <code>python ./app.py</code>. We use the CMD command to do that -</p> <pre><code>CMD [\"python\", \"./app.py\"]\n</code></pre> <p>The primary purpose of <code>CMD</code> is to tell the container which command it should run when it is started. With that, our <code>Dockerfile</code> is now ready. This is how it looks -</p> <pre><code>FROM python:3.8\n\n# set a directory for the app\nWORKDIR /usr/src/app\n\n# copy all the files to the container\nCOPY . .\n\n# install dependencies\nRUN pip install --no-cache-dir -r requirements.txt\n\n# define the port number the container should expose\nEXPOSE 5000\n\n# run the command\nCMD [\"python\", \"./app.py\"]\n</code></pre> <p>Now that we have our <code>Dockerfile</code>, we can build our image. The <code>docker build</code> command does the heavy-lifting of creating a Docker image from a <code>Dockerfile</code>.</p> <p>The section below shows you the output of running the same. Before you run the command yourself (don't forget the period), make sure to replace my username with yours. This username should be the same one you created when you registered on Docker hub. If you haven't done that yet, please go ahead and create an account. The <code>docker build</code> command is quite simple - it takes an optional tag name with <code>-t</code> and a location of the directory containing the <code>Dockerfile</code>.</p> <pre><code>$ docker build -t yourusername/catnip .\nSending build context to Docker daemon 8.704 kB\nStep 1 : FROM python:3.8\n# Executing 3 build triggers...\nStep 1 : COPY requirements.txt /usr/src/app/\n ---&gt; Using cache\nStep 1 : RUN pip install --no-cache-dir -r requirements.txt\n ---&gt; Using cache\nStep 1 : COPY . /usr/src/app\n ---&gt; 1d61f639ef9e\nRemoving intermediate container 4de6ddf5528c\nStep 2 : EXPOSE 5000\n ---&gt; Running in 12cfcf6d67ee\n ---&gt; f423c2f179d1\nRemoving intermediate container 12cfcf6d67ee\nStep 3 : CMD python ./app.py\n ---&gt; Running in f01401a5ace9\n ---&gt; 13e87ed1fbc2\nRemoving intermediate container f01401a5ace9\nSuccessfully built 13e87ed1fbc2\n</code></pre> <p>If you don't have the <code>python:3.8</code> image, the client will first pull the image and then create your image. Hence, your output from running the command will look different from mine. If everything went well, your image should be ready! Run <code>docker images</code> and see if your image shows.</p> <p>The last step in this section is to run the image and see if it actually works (replacing my username with yours).</p> <pre><code>$ docker run -p 8888:5000 yourusername/catnip\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n</code></pre> <p>The command we just ran used port 5000 for the server inside the container and exposed this externally on port 8888. Head over to the URL with port 8888, where your app should be live.</p> <p></p> <p>Congratulations! You have successfully created your first docker image.</p>"},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/#docker-on-aws","title":"Docker on AWS","text":"<p>What good is an application that can't be shared with friends, right? So in this section we are going to see how we can deploy our awesome application to the cloud so that we can share it with our friends! We're going to use AWS Elastic Beanstalk to get our application up and running in a few clicks. We'll also see how easy it is to make our application scalable and manageable with Beanstalk!</p>"},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/#docker-push","title":"Docker Push","text":"<p>The first thing that we need to do before we deploy our app to AWS is to publish our image on a registry which can be accessed by AWS. There are many different Docker registries you can use (you can even host your own). For now, let's use Docker Hub to publish the image.</p> <p>If this is the first time you are pushing an image, the client will ask you to login. Provide the same credentials that you used for logging into Docker Hub.</p> <pre><code>$ docker login\nLogin in with your Docker ID to push and pull images from Docker Hub. If you do not have a Docker ID, head over to https://hub.docker.com to create one.\nUsername: yourusername\nPassword:\nWARNING! Your password will be stored unencrypted in /Users/yourusername/.docker/config.json\nConfigure a credential helper to remove this warning. See\nhttps://docs.docker.com/engine/reference/commandline/login/credential-store\n\nLogin Succeeded\n</code></pre> <p>To publish, just type the below command remembering to replace the name of the image tag above with yours. It is important to have the format of <code>yourusername/image_name</code> so that the client knows where to publish.</p> <pre><code>$ docker push yourusername/catnip\n</code></pre> <p>Once that is done, you can view your image on Docker Hub. For example, here's the web page for my image.</p> <p>Note: One thing that I'd like to clarify before we go ahead is that it is not imperative to host your image on a public registry (or any registry) in order to deploy to AWS. In case you're writing code for the next million-dollar unicorn startup you can totally skip this step. The reason why we're pushing our images publicly is that it makes deployment super simple by skipping a few intermediate configuration steps.</p> <p>Now that your image is online, anyone who has docker installed can play with your app by typing just a single command.</p> <pre><code>$ docker run -p 8888:5000 yourusername/catnip\n</code></pre> <p>If you've pulled your hair out in setting up local dev environments / sharing application configuration in the past, you very well know how awesome this sounds. That's why Docker is so cool!</p>"},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/#beanstalk","title":"Beanstalk","text":"<p>AWS Elastic Beanstalk (EB) is a PaaS (Platform as a Service) offered by AWS. If you've used Heroku, Google App Engine etc. you'll feel right at home. As a developer, you just tell EB how to run your app and it takes care of the rest - including scaling, monitoring and even updates. In April 2014, EB added support for running single-container Docker deployments which is what we'll use to deploy our app. Although EB has a very intuitive CLI, it does require some setup, and to keep things simple we'll use the web UI to launch our application.</p> <p>To follow along, you need a functioning AWS account. If you haven't already, please go ahead and do that now - you will need to enter your credit card information. But don't worry, it's free and anything we do in this tutorial will also be free! Let's get started.</p> <p>Here are the steps:</p> <ul> <li>Login to your AWS console.</li> <li>Click on Elastic Beanstalk. It will be in the compute section on the top left. Alternatively, you can access the Elastic Beanstalk console.</li> </ul> <p></p> <ul> <li>Click on \"Create New Application\" in the top right</li> <li>Give your app a memorable (but unique) name and provide an (optional) description</li> <li>In the New Environment screen, create a new environment and choose the Web Server Environment.</li> <li>Fill in the environment information by choosing a domain. This URL is what you'll share with your friends so make sure it's easy to remember.</li> <li>Under base configuration section. Choose Docker from the predefined platform.</li> </ul> <p></p> <ul> <li>Now we need to upload our application code. But since our application is packaged in a Docker container, we just need to tell EB about our container. Open the <code>Dockerrun.aws.json</code> file located in the <code>flask-app</code> folder and edit the <code>Name</code> of the image to your image's name. Don't worry, I'll explain the contents of the file shortly. When you are done, click on the radio button for \"Upload your Code\", choose this file, and click on \"Upload\".</li> <li>Now click on \"Create environment\". The final screen that you see will have a few spinners indicating that your environment is being set up. It typically takes around 5 minutes for the first-time setup.</li> </ul> <p>While we wait, let's quickly see what the <code>Dockerrun.aws.json</code> file contains. This file is basically an AWS specific file that tells EB details about our application and docker configuration.</p> <pre><code>{\n  \"AWSEBDockerrunVersion\": \"1\",\n  \"Image\": {\n    \"Name\": \"prakhar1989/catnip\",\n    \"Update\": \"true\"\n  },\n  \"Ports\": [\n    {\n      \"ContainerPort\": 5000,\n      \"HostPort\": 8000\n    }\n  ],\n  \"Logging\": \"/var/log/nginx\"\n}\n</code></pre> <p>The file should be pretty self-explanatory, but you can always reference the official documentation for more information. We provide the name of the image that EB should use along with a port that the container should open.</p> <p>Hopefully by now, our instance should be ready. Head over to the EB page and you should see a green tick indicating that your app is alive and kicking.</p> <p></p> <p>Go ahead and open the URL in your browser and you should see the application in all its glory. Feel free to email / IM / snapchat this link to your friends and family so that they can enjoy a few cat gifs, too.</p>"},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/#cleanup","title":"Cleanup","text":"<p>Once you done basking in the glory of your app, remember to terminate the environment so that you don't end up getting charged for extra resources.</p> <p></p> <p>Congratulations! You have deployed your first Docker application! That might seem like a lot of steps, but with the command-line tool for EB you can almost mimic the functionality of Heroku in a few keystrokes! Hopefully, you agree that Docker takes away a lot of the pains of building and deploying applications in the cloud. I would encourage you to read the AWS documentation on single-container Docker environments to get an idea of what features exist.</p> <p>In the next (and final) part of the tutorial, we'll up the ante a bit and deploy an application that mimics the real-world more closely; an app with a persistent back-end storage tier. Let's get straight to it!</p>"},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/#multi-container-environments","title":"Multi-container Environments","text":"<p>In the last section, we saw how easy and fun it is to run applications with Docker. We started with a simple static website and then tried a Flask app. Both of which we could run locally and in the cloud with just a few commands. One thing both these apps had in common was that they were running in a single container.</p> <p>Those of you who have experience running services in production know that usually apps nowadays are not that simple. There's almost always a database (or any other kind of persistent storage) involved. Systems such as Redis and Memcached have become de rigueur of most web application architectures. Hence, in this section we are going to spend some time learning how to Dockerize applications which rely on different services to run.</p> <p>In particular, we are going to see how we can run and manage multi-container docker environments. Why multi-container you might ask? Well, one of the key points of Docker is the way it provides isolation. The idea of bundling a process with its dependencies in a sandbox (called containers) is what makes this so powerful.</p> <p>Just like it's a good strategy to decouple your application tiers, it is wise to keep containers for each of the services separate. Each tier is likely to have different resource needs and those needs might grow at different rates. By separating the tiers into different containers, we can compose each tier using the most appropriate instance type based on different resource needs. This also plays in very well with the whole microservices movement which is one of the main reasons why Docker (or any other container technology) is at the forefront of modern microservices architectures.</p>"},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/#sf-food-trucks","title":"SF Food Trucks","text":"<p>The app that we're going to Dockerize is called SF Food Trucks. My goal in building this app was to have something that is useful (in that it resembles a real-world application), relies on at least one service, but is not too complex for the purpose of this tutorial. This is what I came up with.</p> <p></p> <p>The app's backend is written in Python (Flask) and for search it uses Elasticsearch. Like everything else in this tutorial, the entire source is available on Github. We'll use this as our candidate application for learning out how to build, run and deploy a multi-container environment.</p> <p>First up, let's clone the repository locally.</p> <pre><code>$ git clone https://github.com/prakhar1989/FoodTrucks\n$ cd FoodTrucks\n$ tree -L 2\n.\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 aws-compose.yml\n\u251c\u2500\u2500 docker-compose.yml\n\u251c\u2500\u2500 flask-app\n\u2502   \u251c\u2500\u2500 app.py\n\u2502   \u251c\u2500\u2500 package-lock.json\n\u2502   \u251c\u2500\u2500 package.json\n\u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u251c\u2500\u2500 static\n\u2502   \u251c\u2500\u2500 templates\n\u2502   \u2514\u2500\u2500 webpack.config.js\n\u251c\u2500\u2500 setup-aws-ecs.sh\n\u251c\u2500\u2500 setup-docker.sh\n\u251c\u2500\u2500 shot.png\n\u2514\u2500\u2500 utils\n    \u251c\u2500\u2500 generate_geojson.py\n    \u2514\u2500\u2500 trucks.geojson\n</code></pre> <p>The <code>flask-app</code> folder contains the Python application, while the <code>utils</code> folder has some utilities to load the data into Elasticsearch. The directory also contains some YAML files and a Dockerfile, all of which we'll see in greater detail as we progress through this tutorial. If you are curious, feel free to take a look at the files.</p> <p>Now that you're excited (hopefully), let's think of how we can Dockerize the app. We can see that the application consists of a Flask backend server and an Elasticsearch service. A natural way to split this app would be to have two containers - one running the Flask process and another running the Elasticsearch (ES) process. That way if our app becomes popular, we can scale it by adding more containers depending on where the bottleneck lies.</p> <p>Great, so we need two containers. That shouldn't be hard right? We've already built our own Flask container in the previous section. And for Elasticsearch, let's see if we can find something on the hub.</p> <pre><code>$ docker search elasticsearch\nNAME                              DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED\nelasticsearch                     Elasticsearch is a powerful open source se...   697       [OK]\nitzg/elasticsearch                Provides an easily configurable Elasticsea...   17                   [OK]\ntutum/elasticsearch               Elasticsearch image - listens in port 9200.     15                   [OK]\nbarnybug/elasticsearch            Latest Elasticsearch 1.7.2 and previous re...   15                   [OK]\ndigitalwonderland/elasticsearch   Latest Elasticsearch with Marvel &amp; Kibana       12                   [OK]\nmonsantoco/elasticsearch          ElasticSearch Docker image                      9                    [OK]\n</code></pre> <p>Quite unsurprisingly, there exists an officially supported image for Elasticsearch. To get ES running, we can simply use <code>docker run</code> and have a single-node ES container running locally within no time.</p> <p>Note: Elastic, the company behind Elasticsearch, maintains its own registry for Elastic products. It's recommended to use the images from that registry if you plan to use Elasticsearch.</p> <p>Let's first pull the image</p> <pre><code>$ docker pull docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n</code></pre> <p>and then run it in development mode by specifying ports and setting an environment variable that configures the Elasticsearch cluster to run as a single-node.</p> <pre><code>$ docker run -d --name es -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n277451c15ec183dd939e80298ea4bcf55050328a39b04124b387d668e3ed3943\n</code></pre> <p>Note: If your container runs into memory issues, you might need to tweak some JVM flags to limit its memory consumption.</p> <p>As seen above, we use <code>--name es</code> to give our container a name which makes it easy to use in subsequent commands. Once the container is started, we can see the logs by running <code>docker container logs</code> with the container name (or ID) to inspect the logs. You should see logs similar to below if Elasticsearch started successfully.</p> <p>Note: Elasticsearch takes a few seconds to start so you might need to wait before you see <code>initialized</code> in the logs.</p> <pre><code>$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n277451c15ec1        docker.elastic.co/elasticsearch/elasticsearch:6.3.2   \"/usr/local/bin/dock\u2026\"   2 minutes ago       Up 2 minutes        0.0.0.0:9200-&gt;9200/tcp, 0.0.0.0:9300-&gt;9300/tcp   es\n\n$ docker container logs es\n[2018-07-29T05:49:09,304][INFO ][o.e.n.Node               ] [] initializing ...\n[2018-07-29T05:49:09,385][INFO ][o.e.e.NodeEnvironment    ] [L1VMyzt] using [1] data paths, mounts [[/ (overlay)]], net usable_space [54.1gb], net total_space [62.7gb], types [overlay]\n[2018-07-29T05:49:09,385][INFO ][o.e.e.NodeEnvironment    ] [L1VMyzt] heap size [990.7mb], compressed ordinary object pointers [true]\n[2018-07-29T05:49:11,979][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-security]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-sql]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-upgrade]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-watcher]\n[2018-07-29T05:49:11,981][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded plugin [ingest-geoip]\n[2018-07-29T05:49:11,981][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded plugin [ingest-user-agent]\n[2018-07-29T05:49:17,659][INFO ][o.e.d.DiscoveryModule    ] [L1VMyzt] using discovery type [single-node]\n[2018-07-29T05:49:18,962][INFO ][o.e.n.Node               ] [L1VMyzt] initialized\n[2018-07-29T05:49:18,963][INFO ][o.e.n.Node               ] [L1VMyzt] starting ...\n[2018-07-29T05:49:19,218][INFO ][o.e.t.TransportService   ] [L1VMyzt] publish_address {172.17.0.2:9300}, bound_addresses {0.0.0.0:9300}\n[2018-07-29T05:49:19,302][INFO ][o.e.x.s.t.n.SecurityNetty4HttpServerTransport] [L1VMyzt] publish_address {172.17.0.2:9200}, bound_addresses {0.0.0.0:9200}\n[2018-07-29T05:49:19,303][INFO ][o.e.n.Node               ] [L1VMyzt] started\n[2018-07-29T05:49:19,439][WARN ][o.e.x.s.a.s.m.NativeRoleMappingStore] [L1VMyzt] Failed to clear cache for realms [[]]\n[2018-07-29T05:49:19,542][INFO ][o.e.g.GatewayService     ] [L1VMyzt] recovered [0] indices into cluster_state\n</code></pre> <p>Now, lets try to see if can send a request to the Elasticsearch container. We use the <code>9200</code> port to send a <code>cURL</code> request to the container.</p> <pre><code>$ curl 0.0.0.0:9200\n{\n  \"name\" : \"ijJDAOm\",\n  \"cluster_name\" : \"docker-cluster\",\n  \"cluster_uuid\" : \"a_nSV3XmTCqpzYYzb-LhNw\",\n  \"version\" : {\n    \"number\" : \"6.3.2\",\n    \"build_flavor\" : \"default\",\n    \"build_type\" : \"tar\",\n    \"build_hash\" : \"053779d\",\n    \"build_date\" : \"2018-07-20T05:20:23.451332Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"7.3.1\",\n    \"minimum_wire_compatibility_version\" : \"5.6.0\",\n    \"minimum_index_compatibility_version\" : \"5.0.0\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n</code></pre> <p>Sweet! It's looking good! While we are at it, let's get our Flask container running too. But before we get to that, we need a <code>Dockerfile</code>. In the last section, we used <code>python:3.8</code> image as our base image. This time, however, apart from installing Python dependencies via <code>pip</code>, we want our application to also generate our minified Javascript file for production. For this, we'll require Nodejs. Since we need a custom build step, we'll start from the <code>ubuntu</code> base image to build our <code>Dockerfile</code> from scratch.</p> <p>Note: if you find that an existing image doesn't cater to your needs, feel free to start from another base image and tweak it yourself. For most of the images on Docker Hub, you should be able to find the corresponding <code>Dockerfile</code> on Github. Reading through existing Dockerfiles is one of the best ways to learn how to roll your own.</p> <p>Our Dockerfile for the flask app looks like below -</p> <pre><code># start from base\nFROM ubuntu:18.04\n\nMAINTAINER Prakhar Srivastav &lt;prakhar@prakhar.me&gt;\n\n# install system-wide deps for python and node\nRUN apt-get -yqq update\nRUN apt-get -yqq install python3-pip python3-dev curl gnupg\nRUN curl -sL https://deb.nodesource.com/setup_10.x | bash\nRUN apt-get install -yq nodejs\n\n# copy our application code\nADD flask-app /opt/flask-app\nWORKDIR /opt/flask-app\n\n# fetch app specific deps\nRUN npm install\nRUN npm run build\nRUN pip3 install -r requirements.txt\n\n# expose port\nEXPOSE 5000\n\n# start app\nCMD [ \"python3\", \"./app.py\" ]\n</code></pre> <p>Quite a few new things here so let's quickly go over this file. We start off with the Ubuntu LTS base image and use the package manager <code>apt-get</code> to install the dependencies namely - Python and Node. The <code>yqq</code> flag is used to suppress output and assumes \"Yes\" to all prompts.</p> <p>We then use the <code>ADD</code> command to copy our application into a new volume in the container - <code>/opt/flask-app</code>. This is where our code will reside. We also set this as our working directory, so that the following commands will be run in the context of this location. Now that our system-wide dependencies are installed, we get around to installing app-specific ones. First off we tackle Node by installing the packages from npm and running the build command as defined in our <code>package.json</code> file. We finish the file off by installing the Python packages, exposing the port and defining the <code>CMD</code> to run as we did in the last section.</p> <p>Finally, we can go ahead, build the image and run the container (replace <code>yourusername</code> with your username below).</p> <pre><code>$ docker build -t yourusername/foodtrucks-web .\n</code></pre> <p>In the first run, this will take some time as the Docker client will download the ubuntu image, run all the commands and prepare your image. Re-running <code>docker build</code> after any subsequent changes you make to the application code will almost be instantaneous. Now let's try running our app.</p> <pre><code>$ docker run -P --rm yourusername/foodtrucks-web\nUnable to connect to ES. Retrying in 5 secs...\nUnable to connect to ES. Retrying in 5 secs...\nUnable to connect to ES. Retrying in 5 secs...\nOut of retries. Bailing out...\n</code></pre> <p>Oops! Our flask app was unable to run since it was unable to connect to Elasticsearch. How do we tell one container about the other container and get them to talk to each other? The answer lies in the next section.</p>"},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/#docker-network","title":"Docker Network","text":"<p>Before we talk about the features Docker provides especially to deal with such scenarios, let's see if we can figure out a way to get around the problem. Hopefully, this should give you an appreciation for the specific feature that we are going to study.</p> <p>Okay, so let's run <code>docker container ls</code> (which is same as <code>docker ps</code>) and see what we have.</p> <pre><code>$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n277451c15ec1        docker.elastic.co/elasticsearch/elasticsearch:6.3.2   \"/usr/local/bin/dock\u2026\"   17 minutes ago      Up 17 minutes       0.0.0.0:9200-&gt;9200/tcp, 0.0.0.0:9300-&gt;9300/tcp   es\n</code></pre> <p>So we have one ES container running on <code>0.0.0.0:9200</code> port which we can directly access. If we can tell our Flask app to connect to this URL, it should be able to connect and talk to ES, right? Let's dig into our Python code and see how the connection details are defined.</p> <pre><code>es = Elasticsearch(host='es')\n</code></pre> <p>To make this work, we need to tell the Flask container that the ES container is running on <code>0.0.0.0</code> host (the port by default is <code>9200</code>) and that should make it work, right? Unfortunately, that is not correct since the IP <code>0.0.0.0</code> is the IP to access ES container from the host machine i.e. from my Mac. Another container will not be able to access this on the same IP address. Okay if not that IP, then which IP address should the ES container be accessible by? I'm glad you asked this question.</p> <p>Now is a good time to start our exploration of networking in Docker. When docker is installed, it creates three networks automatically.</p> <pre><code>$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nc2c695315b3a        bridge              bridge              local\na875bec5d6fd        host                host                local\nead0e804a67b        none                null                local\n</code></pre> <p>The bridge network is the network in which containers are run by default. So that means that when I ran the ES container, it was running in this bridge network. To validate this, let's inspect the network.</p> <pre><code>$ docker network inspect bridge\n[\n    {\n        \"Name\": \"bridge\",\n        \"Id\": \"c2c695315b3aaf8fc30530bb3c6b8f6692cedd5cc7579663f0550dfdd21c9a26\",\n        \"Created\": \"2018-07-28T20:32:39.405687265Z\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"EnableIPv6\": false,\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Options\": null,\n            \"Config\": [\n                {\n                    \"Subnet\": \"172.17.0.0/16\",\n                    \"Gateway\": \"172.17.0.1\"\n                }\n            ]\n        },\n        \"Internal\": false,\n        \"Attachable\": false,\n        \"Ingress\": false,\n        \"ConfigFrom\": {\n            \"Network\": \"\"\n        },\n        \"ConfigOnly\": false,\n        \"Containers\": {\n            \"277451c15ec183dd939e80298ea4bcf55050328a39b04124b387d668e3ed3943\": {\n                \"Name\": \"es\",\n                \"EndpointID\": \"5c417a2fc6b13d8ec97b76bbd54aaf3ee2d48f328c3f7279ee335174fbb4d6bb\",\n                \"MacAddress\": \"02:42:ac:11:00:02\",\n                \"IPv4Address\": \"172.17.0.2/16\",\n                \"IPv6Address\": \"\"\n            }\n        },\n        \"Options\": {\n            \"com.docker.network.bridge.default_bridge\": \"true\",\n            \"com.docker.network.bridge.enable_icc\": \"true\",\n            \"com.docker.network.bridge.enable_ip_masquerade\": \"true\",\n            \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\",\n            \"com.docker.network.bridge.name\": \"docker0\",\n            \"com.docker.network.driver.mtu\": \"1500\"\n        },\n        \"Labels\": {}\n    }\n]\n</code></pre> <p>You can see that our container <code>277451c15ec1</code> is listed under the <code>Containers</code> section in the output. What we also see is the IP address this container has been allotted - <code>172.17.0.2</code>. Is this the IP address that we're looking for? Let's find out by running our flask container and trying to access this IP.</p> <pre><code>$ docker run -it --rm yourusername/foodtrucks-web bash\nroot@35180ccc206a:/opt/flask-app# curl 172.17.0.2:9200\n{\n  \"name\" : \"Jane Foster\",\n  \"cluster_name\" : \"elasticsearch\",\n  \"version\" : {\n    \"number\" : \"2.1.1\",\n    \"build_hash\" : \"40e2c53a6b6c2972b3d13846e450e66f4375bd71\",\n    \"build_timestamp\" : \"2015-12-15T13:05:55Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"5.3.1\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\nroot@35180ccc206a:/opt/flask-app# exit\n</code></pre> <p>This should be fairly straightforward to you by now. We start the container in the interactive mode with the <code>bash</code> process. The <code>--rm</code> is a convenient flag for running one off commands since the container gets cleaned up when its work is done. We try a <code>curl</code> but we need to install it first. Once we do that, we see that we can indeed talk to ES on <code>172.17.0.2:9200</code>. Awesome!</p> <p>Although we have figured out a way to make the containers talk to each other, there are still two problems with this approach -</p> <ol> <li>How do we tell the Flask container that <code>es</code> hostname stands for <code>172.17.0.2</code> or some other IP since the IP can change?</li> <li>Since the bridge network is shared by every container by default, this method is not secure. How do we isolate our network?</li> </ol> <p>The good news that Docker has a great answer to our questions. It allows us to define our own networks while keeping them isolated using the <code>docker network</code> command.</p> <p>Let's first go ahead and create our own network.</p> <pre><code>$ docker network create foodtrucks-net\n0815b2a3bb7a6608e850d05553cc0bda98187c4528d94621438f31d97a6fea3c\n\n$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nc2c695315b3a        bridge              bridge              local\n0815b2a3bb7a        foodtrucks-net      bridge              local\na875bec5d6fd        host                host                local\nead0e804a67b        none                null                local\n</code></pre> <p>The <code>network create</code> command creates a new bridge network, which is what we need at the moment.</p> <p>Note</p> <p>In terms of Docker, a bridge network uses a software bridge which allows containers connected to the same bridge network to communicate, while providing isolation from containers which are not connected to that bridge network.</p> <p>The Docker bridge driver automatically installs rules in the host machine so that containers on different bridge networks cannot communicate directly with each other. There are other kinds of networks that you can create, and you are encouraged to read about them in the official docs.</p> <p>Now that we have a network, we can launch our containers inside this network using the <code>--net</code> flag. Let's do that - but first, in order to launch a new container with the same name, we will stop and remove our ES container that is running in the bridge (default) network.</p> <pre><code>$ docker container stop es\nes\n\n$ docker container rm es\nes\n\n$ docker run -d --name es --net foodtrucks-net -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n13d6415f73c8d88bddb1f236f584b63dbaf2c3051f09863a3f1ba219edba3673\n\n$ docker network inspect foodtrucks-net\n[\n    {\n        \"Name\": \"foodtrucks-net\",\n        \"Id\": \"0815b2a3bb7a6608e850d05553cc0bda98187c4528d94621438f31d97a6fea3c\",\n        \"Created\": \"2018-07-30T00:01:29.1500984Z\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"EnableIPv6\": false,\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Options\": {},\n            \"Config\": [\n                {\n                    \"Subnet\": \"172.18.0.0/16\",\n                    \"Gateway\": \"172.18.0.1\"\n                }\n            ]\n        },\n        \"Internal\": false,\n        \"Attachable\": false,\n        \"Ingress\": false,\n        \"ConfigFrom\": {\n            \"Network\": \"\"\n        },\n        \"ConfigOnly\": false,\n        \"Containers\": {\n            \"13d6415f73c8d88bddb1f236f584b63dbaf2c3051f09863a3f1ba219edba3673\": {\n                \"Name\": \"es\",\n                \"EndpointID\": \"29ba2d33f9713e57eb6b38db41d656e4ee2c53e4a2f7cf636bdca0ec59cd3aa7\",\n                \"MacAddress\": \"02:42:ac:12:00:02\",\n                \"IPv4Address\": \"172.18.0.2/16\",\n                \"IPv6Address\": \"\"\n            }\n        },\n        \"Options\": {},\n        \"Labels\": {}\n    }\n]\n</code></pre> <p>As you can see, our <code>es</code> container is now running inside the <code>foodtrucks-net</code> bridge network. Now let's inspect what happens when we launch in our <code>foodtrucks-net</code> network.</p> <pre><code>$ docker run -it --rm --net foodtrucks-net yourusername/foodtrucks-web bash\nroot@9d2722cf282c:/opt/flask-app# curl es:9200\n{\n  \"name\" : \"wWALl9M\",\n  \"cluster_name\" : \"docker-cluster\",\n  \"cluster_uuid\" : \"BA36XuOiRPaghPNBLBHleQ\",\n  \"version\" : {\n    \"number\" : \"6.3.2\",\n    \"build_flavor\" : \"default\",\n    \"build_type\" : \"tar\",\n    \"build_hash\" : \"053779d\",\n    \"build_date\" : \"2018-07-20T05:20:23.451332Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"7.3.1\",\n    \"minimum_wire_compatibility_version\" : \"5.6.0\",\n    \"minimum_index_compatibility_version\" : \"5.0.0\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\nroot@53af252b771a:/opt/flask-app# ls\napp.py  node_modules  package.json  requirements.txt  static  templates  webpack.config.js\nroot@53af252b771a:/opt/flask-app# python3 app.py\nIndex not found...\nLoading data in elasticsearch ...\nTotal trucks loaded:  733\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nroot@53af252b771a:/opt/flask-app# exit\n</code></pre> <p>Wohoo! That works! On user-defined networks like foodtrucks-net, containers can not only communicate by IP address, but can also resolve a container name to an IP address. This capability is called automatic service discovery. Great! Let's launch our Flask container for real now -</p> <pre><code>$ docker run -d --net foodtrucks-net -p 5000:5000 --name foodtrucks-web yourusername/foodtrucks-web\n852fc74de2954bb72471b858dce64d764181dca0cf7693fed201d76da33df794\n\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED              STATUS              PORTS                                            NAMES\n852fc74de295        yourusername/foodtrucks-web                           \"python3 ./app.py\"       About a minute ago   Up About a minute   0.0.0.0:5000-&gt;5000/tcp                           foodtrucks-web\n13d6415f73c8        docker.elastic.co/elasticsearch/elasticsearch:6.3.2   \"/usr/local/bin/dock\u2026\"   17 minutes ago       Up 17 minutes       0.0.0.0:9200-&gt;9200/tcp, 0.0.0.0:9300-&gt;9300/tcp   es\n\n$ curl -I 0.0.0.0:5000\nHTTP/1.0 200 OK\nContent-Type: text/html; charset=utf-8\nContent-Length: 3697\nServer: Werkzeug/0.11.2 Python/2.7.6\nDate: Sun, 10 Jan 2016 23:58:53 GMT\n</code></pre> <p>Head over to http://0.0.0.0:5000 and see your glorious app live! Although that might have seemed like a lot of work, we actually just typed 4 commands to go from zero to running. I've collated the commands in a bash script.</p> <pre><code>#!/bin/bash\n\n# build the flask container\ndocker build -t yourusername/foodtrucks-web .\n\n# create the network\ndocker network create foodtrucks-net\n\n# start the ES container\ndocker run -d --name es --net foodtrucks-net -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n\n# start the flask app container\ndocker run -d --net foodtrucks-net -p 5000:5000 --name foodtrucks-web yourusername/foodtrucks-web\n</code></pre> <p>Now imagine you are distributing your app to a friend, or running on a server that has docker installed. You can get a whole app running with just one command!</p> <pre><code>$ git clone https://github.com/prakhar1989/FoodTrucks\n$ cd FoodTrucks\n$ ./setup-docker.sh\n</code></pre> <p>And that's it! If you ask me, I find this to be an extremely awesome, and a powerful way of sharing and running your applications!</p>"},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/#docker-compose","title":"Docker Compose","text":"<p>Till now we've spent all our time exploring the Docker client. In the Docker ecosystem, however, there are a bunch of other open-source tools which play very nicely with Docker. A few of them are -</p> <ol> <li>Docker Machine - Create Docker hosts on your computer, on cloud providers, and inside your own data center</li> <li>Docker Compose - A tool for defining and running multi-container Docker applications.</li> <li>Docker Swarm - A native clustering solution for Docker</li> <li>Kubernetes - Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.</li> </ol> <p>In this section, we are going to look at one of these tools, Docker Compose, and see how it can make dealing with multi-container apps easier.</p> <p>The background story of Docker Compose is quite interesting. Roughly around January 2014, a company called OrchardUp launched a tool called Fig. The idea behind Fig was to make isolated development environments work with Docker. The project was very well received on Hacker News - I oddly remember reading about it but didn't quite get the hang of it.</p> <p>The first comment on the forum actually does a good job of explaining what Fig is all about.</p> <p>So really at this point, that's what Docker is about: running processes. Now Docker offers a quite rich API to run the processes: shared volumes (directories) between containers (i.e. running images), forward port from the host to the container, display logs, and so on. But that's it: Docker as of now, remains at the process level.</p> <p>While it provides options to orchestrate multiple containers to create a single \"app\", it doesn't address the management of such group of containers as a single entity. And that's where tools such as Fig come in: talking about a group of containers as a single entity. Think \"run an app\" (i.e. \"run an orchestrated cluster of containers\") instead of \"run a container\".</p> <p>It turns out that a lot of people using docker agree with this sentiment. Slowly and steadily as Fig became popular, Docker Inc. took notice, acquired the company and re-branded Fig as Docker Compose.</p> <p>So what is Compose used for? Compose is a tool that is used for defining and running multi-container Docker apps in an easy way. It provides a configuration file called <code>docker-compose.yml</code> that can be used to bring up an application and the suite of services it depends on with just one command. Compose works in all environments: production, staging, development, testing, as well as CI workflows, although Compose is ideal for development and testing environments.</p> <p>Let's see if we can create a <code>docker-compose.yml</code> file for our SF-Foodtrucks app and evaluate whether Docker Compose lives up to its promise.</p> <p>The first step, however, is to install Docker Compose. If you're running Windows or Mac, Docker Compose is already installed as it comes in the Docker Toolbox. Linux users can easily get their hands on Docker Compose by following the instructions on the docs. Since Compose is written in Python, you can also simply do <code>pip install docker-compose</code>. Test your installation with -</p> <pre><code>$ docker-compose --version\ndocker-compose version 1.21.2, build a133471\n</code></pre> <p>Now that we have it installed, we can jump on the next step i.e. the Docker Compose file <code>docker-compose.yml</code>. The syntax for YAML is quite simple and the repo already contains the docker-compose file that we'll be using.</p> <pre><code>version: \"3\"\nservices:\n  es:\n    image: docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n    container_name: es\n    environment:\n      - discovery.type=single-node\n    ports:\n      - 9200:9200\n    volumes:\n      - esdata1:/usr/share/elasticsearch/data\n  web:\n    image: yourusername/foodtrucks-web\n    command: python3 app.py\n    depends_on:\n      - es\n    ports:\n      - 5000:5000\n    volumes:\n      - ./flask-app:/opt/flask-app\nvolumes:\n  esdata1:\n    driver: local\n</code></pre> <p>Let me breakdown what the file above means. At the parent level, we define the names of our services - <code>es</code> and <code>web</code>. The <code>image</code> parameter is always required, and for each service that we want Docker to run, we can add additional parameters. For <code>es</code>, we just refer to the <code>elasticsearch</code> image available on Elastic registry. For our Flask app, we refer to the image that we built at the beginning of this section.</p> <p>Other parameters such as <code>command</code> and <code>ports</code> provide more information about the container. The <code>volumes</code> parameter specifies a mount point in our <code>web</code> container where the code will reside. This is purely optional and is useful if you need access to logs, etc. We'll later see how this can be useful during development. Refer to the online reference to learn more about the parameters this file supports. We also add volumes for the <code>es</code> container so that the data we load persists between restarts. We also specify <code>depends_on</code>, which tells docker to start the <code>es</code> container before <code>web</code>. You can read more about it on docker compose docs.</p> <p>Note: You must be inside the directory with the <code>docker-compose.yml</code> file in order to execute most Compose commands.</p> <p>Great! Now the file is ready, let's see <code>docker-compose</code> in action. But before we start, we need to make sure the ports and names are free. So if you have the Flask and ES containers running, lets turn them off.</p> <pre><code>$ docker stop es foodtrucks-web\nes\nfoodtrucks-web\n\n$ docker rm es foodtrucks-web\nes\nfoodtrucks-web\n</code></pre> <p>Now we can run <code>docker-compose</code>. Navigate to the food trucks directory and run <code>docker-compose up</code>.</p> <pre><code>$ docker-compose up\nCreating network \"foodtrucks_default\" with the default driver\nCreating foodtrucks_es_1\nCreating foodtrucks_web_1\nAttaching to foodtrucks_es_1, foodtrucks_web_1\nes_1  | [2016-01-11 03:43:50,300][INFO ][node                     ] [Comet] version[2.1.1], pid[1], build[40e2c53/2015-12-15T13:05:55Z]\nes_1  | [2016-01-11 03:43:50,307][INFO ][node                     ] [Comet] initializing ...\nes_1  | [2016-01-11 03:43:50,366][INFO ][plugins                  ] [Comet] loaded [], sites []\nes_1  | [2016-01-11 03:43:50,421][INFO ][env                      ] [Comet] using [1] data paths, mounts [[/usr/share/elasticsearch/data (/dev/sda1)]], net usable_space [16gb], net total_space [18.1gb], spins? [possibly], types [ext4]\nes_1  | [2016-01-11 03:43:52,626][INFO ][node                     ] [Comet] initialized\nes_1  | [2016-01-11 03:43:52,632][INFO ][node                     ] [Comet] starting ...\nes_1  | [2016-01-11 03:43:52,703][WARN ][common.network           ] [Comet] publish address: {0.0.0.0} is a wildcard address, falling back to first non-loopback: {172.17.0.2}\nes_1  | [2016-01-11 03:43:52,704][INFO ][transport                ] [Comet] publish_address {172.17.0.2:9300}, bound_addresses {[::]:9300}\nes_1  | [2016-01-11 03:43:52,721][INFO ][discovery                ] [Comet] elasticsearch/cEk4s7pdQ-evRc9MqS2wqw\nes_1  | [2016-01-11 03:43:55,785][INFO ][cluster.service          ] [Comet] new_master {Comet}{cEk4s7pdQ-evRc9MqS2wqw}{172.17.0.2}{172.17.0.2:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)\nes_1  | [2016-01-11 03:43:55,818][WARN ][common.network           ] [Comet] publish address: {0.0.0.0} is a wildcard address, falling back to first non-loopback: {172.17.0.2}\nes_1  | [2016-01-11 03:43:55,819][INFO ][http                     ] [Comet] publish_address {172.17.0.2:9200}, bound_addresses {[::]:9200}\nes_1  | [2016-01-11 03:43:55,819][INFO ][node                     ] [Comet] started\nes_1  | [2016-01-11 03:43:55,826][INFO ][gateway                  ] [Comet] recovered [0] indices into cluster_state\nes_1  | [2016-01-11 03:44:01,825][INFO ][cluster.metadata         ] [Comet] [sfdata] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings [truck]\nes_1  | [2016-01-11 03:44:02,373][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,510][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,593][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,708][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:03,047][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nweb_1 |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n</code></pre> <p>Head over to the IP to see your app live. That was amazing wasn't it? Just a few lines of configuration and we have two Docker containers running successfully in unison. Let's stop the services and re-run in detached mode.</p> <pre><code>web_1 |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nKilling foodtrucks_web_1 ... done\nKilling foodtrucks_es_1 ... done\n\n$ docker-compose up -d\nCreating es               ... done\nCreating foodtrucks_web_1 ... done\n\n$ docker-compose ps\n      Name                    Command               State                Ports\n--------------------------------------------------------------------------------------------\nes                 /usr/local/bin/docker-entr ...   Up      0.0.0.0:9200-&gt;9200/tcp, 9300/tcp\nfoodtrucks_web_1   python3 app.py                   Up      0.0.0.0:5000-&gt;5000/tcp\n</code></pre> <p>Unsurprisingly, we can see both the containers running successfully. Where do the names come from? Those were created automatically by Compose. But does Compose also create the network automatically? Good question! Let's find out.</p> <p>First off, let us stop the services from running. We can always bring them back up in just one command. Data volumes will persist, so it\u2019s possible to start the cluster again with the same data using docker-compose up. To destroy the cluster and the data volumes, just type <code>docker-compose down -v</code>.</p> <pre><code>$ docker-compose down -v\nStopping foodtrucks_web_1 ... done\nStopping es               ... done\nRemoving foodtrucks_web_1 ... done\nRemoving es               ... done\nRemoving network foodtrucks_default\nRemoving volume foodtrucks_esdata1\n</code></pre> <p>While we're are at it, we'll also remove the <code>foodtrucks</code> network that we created last time.</p> <pre><code>$ docker network rm foodtrucks-net\n$ docker network ls\nNETWORK ID          NAME                 DRIVER              SCOPE\nc2c695315b3a        bridge               bridge              local\na875bec5d6fd        host                 host                local\nead0e804a67b        none                 null                local\n</code></pre> <p>Great! Now that we have a clean slate, let's re-run our services and see if Compose does its magic.</p> <pre><code>$ docker-compose up -d\nRecreating foodtrucks_es_1\nRecreating foodtrucks_web_1\n\n$ docker container ls\nCONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS                    NAMES\nf50bb33a3242        yourusername/foodtrucks-web  \"python3 app.py\"         14 seconds ago      Up 13 seconds       0.0.0.0:5000-&gt;5000/tcp   foodtrucks_web_1\ne299ceeb4caa        elasticsearch                \"/docker-entrypoint.s\"   14 seconds ago      Up 14 seconds       9200/tcp, 9300/tcp       foodtrucks_es_1\n</code></pre> <p>So far, so good. Time to see if any networks were created.</p> <pre><code>$ docker network ls\nNETWORK ID          NAME                 DRIVER\nc2c695315b3a        bridge               bridge              local\nf3b80f381ed3        foodtrucks_default   bridge              local\na875bec5d6fd        host                 host                local\nead0e804a67b        none                 null                local\n</code></pre> <p>You can see that compose went ahead and created a new network called <code>foodtrucks_default</code> and attached both the new services in that network so that each of these are discoverable to the other. Each container for a service joins the default network and is both reachable by other containers on that network, and discoverable by them at a hostname identical to the container name.</p> <pre><code>$ docker ps\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED              STATUS              PORTS                              NAMES\n8c6bb7e818ec        docker.elastic.co/elasticsearch/elasticsearch:6.3.2   \"/usr/local/bin/dock\u2026\"   About a minute ago   Up About a minute   0.0.0.0:9200-&gt;9200/tcp, 9300/tcp   es\n7640cec7feb7        yourusername/foodtrucks-web                           \"python3 app.py\"         About a minute ago   Up About a minute   0.0.0.0:5000-&gt;5000/tcp             foodtrucks_web_1\n\n$ docker network inspect foodtrucks_default\n[\n    {\n        \"Name\": \"foodtrucks_default\",\n        \"Id\": \"f3b80f381ed3e03b3d5e605e42c4a576e32d38ba24399e963d7dad848b3b4fe7\",\n        \"Created\": \"2018-07-30T03:36:06.0384826Z\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"EnableIPv6\": false,\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Options\": null,\n            \"Config\": [\n                {\n                    \"Subnet\": \"172.19.0.0/16\",\n                    \"Gateway\": \"172.19.0.1\"\n                }\n            ]\n        },\n        \"Internal\": false,\n        \"Attachable\": true,\n        \"Ingress\": false,\n        \"ConfigFrom\": {\n            \"Network\": \"\"\n        },\n        \"ConfigOnly\": false,\n        \"Containers\": {\n            \"7640cec7feb7f5615eaac376271a93fb8bab2ce54c7257256bf16716e05c65a5\": {\n                \"Name\": \"foodtrucks_web_1\",\n                \"EndpointID\": \"b1aa3e735402abafea3edfbba605eb4617f81d94f1b5f8fcc566a874660a0266\",\n                \"MacAddress\": \"02:42:ac:13:00:02\",\n                \"IPv4Address\": \"172.19.0.2/16\",\n                \"IPv6Address\": \"\"\n            },\n            \"8c6bb7e818ec1f88c37f375c18f00beb030b31f4b10aee5a0952aad753314b57\": {\n                \"Name\": \"es\",\n                \"EndpointID\": \"649b3567d38e5e6f03fa6c004a4302508c14a5f2ac086ee6dcf13ddef936de7b\",\n                \"MacAddress\": \"02:42:ac:13:00:03\",\n                \"IPv4Address\": \"172.19.0.3/16\",\n                \"IPv6Address\": \"\"\n            }\n        },\n        \"Options\": {},\n        \"Labels\": {\n            \"com.docker.compose.network\": \"default\",\n            \"com.docker.compose.project\": \"foodtrucks\",\n            \"com.docker.compose.version\": \"1.21.2\"\n        }\n    }\n]\n</code></pre>"},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/#development-workflow","title":"Development Workflow","text":"<p>Before we jump to the next section, there's one last thing I wanted to cover about docker-compose. As stated earlier, docker-compose is really great for development and testing. So let's see how we can configure compose to make our lives easier during development.</p> <p>Throughout this tutorial, we've worked with readymade docker images. While we've built images from scratch, we haven't touched any application code yet and mostly restricted ourselves to editing Dockerfiles and YAML configurations. One thing that you must be wondering is how does the workflow look during development? Is one supposed to keep creating Docker images for every change, then publish it and then run it to see if the changes work as expected? I'm sure that sounds super tedious. There has to be a better way. In this section, that's what we're going to explore.</p> <p>Let's see how we can make a change in the Foodtrucks app we just ran. Make sure you have the app running,</p> <pre><code>$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                              NAMES\n5450ebedd03c        yourusername/foodtrucks-web                           \"python3 app.py\"         9 seconds ago       Up 6 seconds        0.0.0.0:5000-&gt;5000/tcp             foodtrucks_web_1\n05d408b25dfe        docker.elastic.co/elasticsearch/elasticsearch:6.3.2   \"/usr/local/bin/dock\u2026\"   10 hours ago        Up 10 hours         0.0.0.0:9200-&gt;9200/tcp, 9300/tcp   es\n</code></pre> <p>Now let's see if we can change this app to display a <code>Hello world!</code> message when a request is made to <code>/hello</code> route. Currently, the app responds with a 404.</p> <pre><code>$ curl -I 0.0.0.0:5000/hello\nHTTP/1.0 404 NOT FOUND\nContent-Type: text/html\nContent-Length: 233\nServer: Werkzeug/0.11.2 Python/2.7.15rc1\nDate: Mon, 30 Jul 2018 15:34:38 GMT\n</code></pre> <p>Why does this happen? Since ours is a Flask app, we can see <code>app.py</code> (link) for answers. In Flask, routes are defined with @app.route syntax. In the file, you'll see that we only have three routes defined - <code>/</code>,<code>/debug</code>and<code>/search</code>. The<code>/</code>route renders the main app, the<code>debug</code>route is used to return some debug information and finally<code>search</code> is used by the app to query elasticsearch.</p> <pre><code>$ curl 0.0.0.0:5000/debug\n{\n  \"msg\": \"yellow open sfdata Ibkx7WYjSt-g8NZXOEtTMg 5 1 618 0 1.3mb 1.3mb\\n\",\n  \"status\": \"success\"\n}\n</code></pre> <p>Given that context, how would we add a new route for <code>hello</code>? You guessed it! Let's open <code>flask-app/app.py</code> in our favorite editor and make the following change</p> <pre><code>@app.route('/')\ndef index():\n  return render_template(\"index.html\")\n\n# add a new hello route\n@app.route('/hello')\ndef hello():\n  return \"hello world!\"\n</code></pre> <p>Now let's try making a request again</p> <pre><code>$ curl -I 0.0.0.0:5000/hello\nHTTP/1.0 404 NOT FOUND\nContent-Type: text/html\nContent-Length: 233\nServer: Werkzeug/0.11.2 Python/2.7.15rc1\nDate: Mon, 30 Jul 2018 15:34:38 GMT\n</code></pre> <p>Oh no! That didn't work! What did we do wrong? While we did make the change in <code>app.py</code>, the file resides in our machine (or the host machine), but since Docker is running our containers based off the <code>yourusername/foodtrucks-web</code> image, it doesn't know about this change. To validate this, lets try the following -</p> <pre><code>$ docker-compose run web bash\nStarting es ... done\nroot@581e351c82b0:/opt/flask-app# ls\napp.py        package-lock.json  requirements.txt  templates\nnode_modules  package.json       static            webpack.config.js\nroot@581e351c82b0:/opt/flask-app# grep hello app.py\nroot@581e351c82b0:/opt/flask-app# exit\n</code></pre> <p>What we're trying to do here is to validate that our changes are not in the <code>app.py</code> that's running in the container. We do this by running the command <code>docker-compose run</code>, which is similar to its cousin <code>docker run</code> but takes additional arguments for the service (which is <code>web</code> in our case). As soon as we run <code>bash</code>, the shell opens in <code>/opt/flask-app</code> as specified in our Dockerfile. From the grep command we can see that our changes are not in the file.</p> <p>Lets see how we can fix it. First off, we need to tell docker compose to not use the image and instead use the files locally. We'll also set debug mode to <code>true</code> so that Flask knows to reload the server when <code>app.py</code> changes. Replace the <code>web</code> portion of the <code>docker-compose.yml</code> file like so:</p> <pre><code>version: \"3\"\nservices:\n  es:\n    image: docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n    container_name: es\n    environment:\n      - discovery.type=single-node\n    ports:\n      - 9200:9200\n    volumes:\n      - esdata1:/usr/share/elasticsearch/data\n  web:\n    build: . # replaced image with build\n    command: python3 app.py\n    environment:\n      - DEBUG=True # set an env var for flask\n    depends_on:\n      - es\n    ports:\n      - \"5000:5000\"\n    volumes:\n      - ./flask-app:/opt/flask-app\nvolumes:\n  esdata1:\n    driver: local\n</code></pre> <p>With that change (diff), let's stop and start the containers.</p> <pre><code>$ docker-compose down -v\nStopping foodtrucks_web_1 ... done\nStopping es               ... done\nRemoving foodtrucks_web_1 ... done\nRemoving es               ... done\nRemoving network foodtrucks_default\nRemoving volume foodtrucks_esdata1\n\n$ docker-compose up -d\nCreating network \"foodtrucks_default\" with the default driver\nCreating volume \"foodtrucks_esdata1\" with local driver\nCreating es ... done\nCreating foodtrucks_web_1 ... done\n</code></pre> <p>As a final step, lets make the change in <code>app.py</code> by adding a new route. Now we try to curl</p> <pre><code>$ curl 0.0.0.0:5000/hello\nhello world\n</code></pre> <p>Wohoo! We get a valid response! Try playing around by making more changes in the app.</p> <p>That concludes our tour of Docker Compose. With Docker Compose, you can also pause your services, run a one-off command on a container and even scale the number of containers. I also recommend you checkout a few other use-cases of Docker compose. Hopefully, I was able to show you how easy it is to manage multi-container environments with Compose. In the final section, we are going to deploy our app to AWS!</p>"},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/#aws-elastic-container-service","title":"AWS Elastic Container Service","text":"<p>In the last section we used <code>docker-compose</code> to run our app locally with a single command: <code>docker-compose up</code>. Now that we have a functioning app we want to share this with the world, get some users, make tons of money and buy a big house in Miami. Executing the last three are beyond the scope of the tutorial, so we'll spend our time instead on figuring out how we can deploy our multi-container apps on the cloud with AWS.</p> <p>If you've read this far you are pretty much convinced that Docker is a pretty cool technology. And you are not alone. Seeing the meteoric rise of Docker, almost all Cloud vendors started working on adding support for deploying Docker apps on their platform. As of today, you can deploy containers on Google Cloud Platform, AWS, Azure and many others. We already got a primer on deploying single container apps with Elastic Beanstalk and in this section we are going to look at Elastic Container Service (or ECS) by AWS.</p> <p>AWS ECS is a scalable and super flexible container management service that supports Docker containers. It allows you to operate a Docker cluster on top of EC2 instances via an easy-to-use API. Where Beanstalk came with reasonable defaults, ECS allows you to completely tune your environment as per your needs. This makes ECS, in my opinion, quite complex to get started with.</p> <p>Luckily for us, ECS has a friendly CLI tool that understands Docker Compose files and automatically provisions the cluster on ECS! Since we already have a functioning <code>docker-compose.yml</code> it should not take a lot of effort in getting up and running on AWS. So let's get started!</p> <p>The first step is to install the CLI. Instructions to install the CLI on both Mac and Linux are explained very clearly in the official docs. Go ahead, install the CLI and when you are done, verify the install by running</p> <pre><code>$ ecs-cli --version\necs-cli version 1.18.1 (7e9df84)\n</code></pre> <p>Next, we'll be working on configuring the CLI so that we can talk to ECS. We'll be following the steps as detailed in the official guide on AWS ECS docs. In case of any confusion, please feel free to refer to that guide.</p> <p>The first step will involve creating a profile that we'll use for the rest of the tutorial. To continue, you'll need your <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code>. To obtain these, follow the steps as detailed under the section titled Access Key and Secret Access Key on this page.</p> <pre><code>$ ecs-cli configure profile --profile-name ecs-foodtrucks --access-key $AWS_ACCESS_KEY_ID --secret-key $AWS_SECRET_ACCESS_KEY\n</code></pre> <p>Next, we need to get a key pair which we'll be using to log into the instances. Head over to your EC2 Console and create a new key pair. Download the key pair and store it in a safe location. Another thing to note before you move away from this screen is the region name. In my case, I have named my key - <code>ecs</code> and set my region as <code>us-east-1</code>. This is what I'll assume for the rest of this walkthrough.</p> <p></p> <p>The next step is to configure the CLI.</p> <pre><code>$ ecs-cli configure --region us-east-1 --cluster foodtrucks\nINFO[0000] Saved ECS CLI configuration for cluster (foodtrucks)\n</code></pre> <p>We provide the <code>configure</code> command with the region name we want our cluster to reside in and a cluster name. Make sure you provide the same region name that you used when creating the key pair. If you've not configured the AWS CLI on your computer before, you can use the official guide, which explains everything in great detail on how to get everything going.</p> <p>The next step enables the CLI to create a CloudFormation template.</p> <pre><code>$ ecs-cli up --keypair ecs --capability-iam --size 1 --instance-type t2.medium\nINFO[0000] Using recommended Amazon Linux 2 AMI with ECS Agent 1.39.0 and Docker version 18.09.9-ce\nINFO[0000] Created cluster                               cluster=foodtrucks\nINFO[0001] Waiting for your cluster resources to be created\nINFO[0001] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0062] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0122] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0182] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0242] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nVPC created: vpc-0bbed8536930053a6\nSecurity Group created: sg-0cf767fb4d01a3f99\nSubnet created: subnet-05de1db2cb1a50ab8\nSubnet created: subnet-01e1e8bc95d49d0fd\nCluster creation succeeded.\n</code></pre> <p>Here we provide the name of the key pair we downloaded initially (<code>ecs</code> in my case), the number of instances that we want to use (<code>--size</code>) and the type of instances that we want the containers to run on. The <code>--capability-iam</code> flag tells the CLI that we acknowledge that this command may create IAM resources.</p> <p>The last and final step is where we'll use our <code>docker-compose.yml</code> file. We'll need to make a few minor changes, so instead of modifying the original, let's make a copy of it. The contents of this file (after making the changes) look like (below) -</p> <pre><code>version: '2'\nservices:\n  es:\n    image: docker.elastic.co/elasticsearch/elasticsearch:7.6.2\n    cpu_shares: 100\n    mem_limit: 3621440000\n    environment:\n      - discovery.type=single-node\n      - bootstrap.memory_lock=true\n      - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"\n    logging:\n      driver: awslogs\n      options:\n        awslogs-group: foodtrucks\n        awslogs-region: us-east-1\n        awslogs-stream-prefix: es\n  web:\n    image: yourusername/foodtrucks-web\n    cpu_shares: 100\n    mem_limit: 262144000\n    ports:\n      - \"80:5000\"\n    links:\n      - es\n    logging:\n      driver: awslogs\n      options:\n        awslogs-group: foodtrucks\n        awslogs-region: us-east-1\n        awslogs-stream-prefix: web\n</code></pre> <p>The only changes we made from the original <code>docker-compose.yml</code> are of providing the <code>mem_limit</code> (in bytes) and <code>cpu_shares</code> values for each container and adding some logging configuration. This allows us to view logs generated by our containers in AWS CloudWatch. Head over to CloudWatch to create a log group called <code>foodtrucks</code>. Note that since ElasticSearch typically ends up taking more memory, we've given around 3.4 GB of memory limit. Another thing we need to do before we move onto the next step is to publish our image on Docker Hub.</p> <pre><code>$ docker push yourusername/foodtrucks-web\n</code></pre> <p>Great! Now let's run the final command that will deploy our app on ECS!</p> <pre><code>$ cd aws-ecs\n$ ecs-cli compose up\nINFO[0000] Using ECS task definition                     TaskDefinition=ecscompose-foodtrucks:2\nINFO[0000] Starting container...                         container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es\nINFO[0000] Starting container...                         container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web\nINFO[0000] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0000] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0036] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0048] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0048] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0060] Started container...                          container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=RUNNING taskDefinition=ecscompose-foodtrucks:2\nINFO[0060] Started container...                          container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=RUNNING taskDefinition=ecscompose-foodtrucks:2\n</code></pre> <p>It's not a coincidence that the invocation above looks similar to the one we used with Docker Compose. If everything went well, you should see a <code>desiredStatus=RUNNING lastStatus=RUNNING</code> as the last line.</p> <p>Awesome! Our app is live, but how can we access it?</p> <pre><code>ecs-cli ps\nName                                      State    Ports                     TaskDefinition\n845e2368-170d-44a7-bf9f-84c7fcd9ae29/web  RUNNING  54.86.14.14:80-&gt;5000/tcp  ecscompose-foodtrucks:2\n845e2368-170d-44a7-bf9f-84c7fcd9ae29/es   RUNNING                            ecscompose-foodtrucks:2\n</code></pre> <p>Go ahead and open http://54.86.14.14 in your browser and you should see the Food Trucks in all its black-yellow glory! Since we're on the topic, let's see how our AWS ECS console looks.</p> <p> </p> <p>We can see above that our ECS cluster called 'foodtrucks' was created and is now running 1 task with 2 container instances. Spend some time browsing this console to get a hang of all the options that are here.</p>"},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/#cleanup_1","title":"Cleanup","text":"<p>Once you've played around with the deployed app, remember to turn down the cluster -</p> <pre><code>$ ecs-cli down --force\nINFO[0001] Waiting for your cluster resources to be deleted...\nINFO[0001] Cloudformation stack status                   stackStatus=DELETE_IN_PROGRESS\nINFO[0062] Cloudformation stack status                   stackStatus=DELETE_IN_PROGRESS\nINFO[0124] Cloudformation stack status                   stackStatus=DELETE_IN_PROGRESS\nINFO[0155] Deleted cluster                               cluster=foodtrucks\n</code></pre> <p>So there you have it. With just a few commands we were able to deploy our awesome app on the AWS cloud!</p>"},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/#conclusion","title":"Conclusion","text":"<p>And that's a wrap! After a long, exhaustive but fun tutorial you are now ready to take the container world by storm! If you followed along till the very end then you should definitely be proud of yourself. You learned how to setup Docker, run your own containers, play with static and dynamic websites and most importantly got hands on experience with deploying your applications to the cloud!</p> <p>I hope that finishing this tutorial makes you more confident in your abilities to deal with servers. When you have an idea of building your next app, you can be sure that you'll be able to get it in front of people with minimal effort.</p>"},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/#next-steps","title":"Next Steps","text":"<p>Your journey into the container world has just started! My goal with this tutorial was to whet your appetite and show you the power of Docker. In the sea of new technology, it can be hard to navigate the waters alone and tutorials such as this one can provide a helping hand. This is the Docker tutorial I wish I had when I was starting out. Hopefully, it served its purpose of getting you excited about containers so that you no longer have to watch the action from the sides.</p> <p>Below are a few additional resources that will be beneficial. For your next project, I strongly encourage you to use Docker. Keep in mind - practice makes perfect!</p> <p>Additional Resources</p> <ul> <li>Awesome Docker</li> <li>Why Docker</li> <li>Docker Weekly and archives</li> <li>Codeship Blog</li> </ul> <p>Off you go, young padawan!</p>"},{"location":"Reference/devops/A%20Docker%20Tutorial%20for%20Beginners/#give-feedback","title":"Give Feedback","text":"<p>Now that the tutorial is over, it's my turn to ask questions. How did you like the tutorial? Did you find the tutorial to be a complete mess or did you have fun and learn something?</p> <p>Send in your thoughts directly to me or just create an issue. I'm on Twitter, too, so if that's your deal, feel free to holler there!</p> <p>I would totally love to hear about your experience with this tutorial. Give suggestions on how to make this better or let me know about my mistakes. I want this tutorial to be one of the best introductory tutorials on the web and I can't do it without your help.</p>"},{"location":"Reference/linux/12%20Linux%20Terminal%20Shortcuts%20Every%20Power%20Linux%20User%20Must%20Know/","title":"12 Linux Terminal Shortcuts Every Power Linux User Must Know","text":""},{"location":"Reference/linux/12%20Linux%20Terminal%20Shortcuts%20Every%20Power%20Linux%20User%20Must%20Know/#must-know-linux-shortcuts","title":"Must Know Linux Shortcuts","text":"<p>I would like to mention that some of these shortcuts may depend upon the Shell you are using. Bash is the most popular shell, so the list is focused on Bash. If you want, you may call it Bash shortcut list as well.</p>"},{"location":"Reference/linux/12%20Linux%20Terminal%20Shortcuts%20Every%20Power%20Linux%20User%20Must%20Know/#1-tab","title":"1. Tab","text":"<p>Just start typing a command, filename, directory name or even command options and hit the tab key. It will either automatically complete what you were typing or it will show all the possible results for you.</p>"},{"location":"Reference/linux/12%20Linux%20Terminal%20Shortcuts%20Every%20Power%20Linux%20User%20Must%20Know/#2-ctrl-c","title":"2. Ctrl + C","text":"<p>These are the keys you should press in order to break out of a command or process on a terminal. This will stop (terminate) a running program immediately.</p>"},{"location":"Reference/linux/12%20Linux%20Terminal%20Shortcuts%20Every%20Power%20Linux%20User%20Must%20Know/#3-ctrl-z","title":"3. Ctrl + Z","text":"<p>This shortcut will suspend a running program and gives you control of the shell. You can see the stopped program in background jobs and even resume to run it using the fg command.</p>"},{"location":"Reference/linux/12%20Linux%20Terminal%20Shortcuts%20Every%20Power%20Linux%20User%20Must%20Know/#4-ctrl-d","title":"4. Ctrl + D","text":"<p>This keyboard shortcut will log you out of the current terminal. If you are using an SSH connection, it will be closed. If you are using a terminal directly, the application will be closed immediately.</p> <p>Consider it equivalent to the \u2018exit\u2019 command.</p>"},{"location":"Reference/linux/12%20Linux%20Terminal%20Shortcuts%20Every%20Power%20Linux%20User%20Must%20Know/#5-ctrl-l","title":"5. Ctrl + L","text":"<p>How do you clear your terminal screen? I guess using the clear command.</p> <p>Instead of writing C-L-E-A-R, you can simply use Ctrl+L to clear the terminal. Handy, isn\u2019t it?</p>"},{"location":"Reference/linux/12%20Linux%20Terminal%20Shortcuts%20Every%20Power%20Linux%20User%20Must%20Know/#6-ctrl-a","title":"6. Ctrl + A","text":"<p>This shortcut will move the cursor to the beginning of the line.</p> <p>Suppose you typed a long command or path in the terminal and you want to go to the beginning of it, using the arrow key to move the cursor will take plenty of time. Do note that you cannot use the mouse to move the cursor to the beginning of the line.</p> <p>This is where Ctrl+A saves the day.</p>"},{"location":"Reference/linux/12%20Linux%20Terminal%20Shortcuts%20Every%20Power%20Linux%20User%20Must%20Know/#7-ctrl-e","title":"7. Ctrl + E","text":"<p>This shortcut is sort of opposite to Ctrl+A. Ctrl+A sends the cursor to the beginning of the line whereas Ctrl+E moves the cursor to the end of the line.</p> <p>[!NOTE] If you have the Home and End keys on your keyboard, you can also use them. Home is equivalent to Ctrl +A and End is equivalent to Ctrl + E.</p>"},{"location":"Reference/linux/12%20Linux%20Terminal%20Shortcuts%20Every%20Power%20Linux%20User%20Must%20Know/#8-ctrl-u","title":"8. Ctrl + U","text":"<p>Typed a wrong command? Instead of using the backspace to discard the current command, use Ctrl+U shortcut in the Linux terminal. This shortcut erases everything from the current cursor position to the beginning of the line.</p> <p>[!WARNING ] Ubuntu Terminal would erase everything of one line</p>"},{"location":"Reference/linux/12%20Linux%20Terminal%20Shortcuts%20Every%20Power%20Linux%20User%20Must%20Know/#9-ctrl-k","title":"9. Ctrl + K","text":"<p>This one is similar to the Ctrl+U shortcut. The only difference is that instead of the beginning of the line, it erases everything from the current cursor position to the end of the line.</p>"},{"location":"Reference/linux/12%20Linux%20Terminal%20Shortcuts%20Every%20Power%20Linux%20User%20Must%20Know/#10-ctrl-w","title":"10. Ctrl + W","text":"<p>You just learned about erasing text till the beginning and the end of the line. But what if you just need to delete a single word? Use the Ctrl+W shortcut.</p> <p>Using Ctrl+W shortcut, you can erase the word preceding to the cursor position. If the cursor is on a word itself, it will erase all letters from the cursor position to the beginning of the word.</p> <p>The best way to use it to move the cursor to the next space after the targeted word and then use the Ctrl+W keyboard shortcut.</p>"},{"location":"Reference/linux/12%20Linux%20Terminal%20Shortcuts%20Every%20Power%20Linux%20User%20Must%20Know/#11-ctrl-y","title":"11. Ctrl + Y","text":"<p>This will paste the erased text that you saw with Ctrl + W, Ctrl + U and Ctrl + K shortcuts. Comes handy in case you erased wrong text or if you need to use the erased text someplace else.</p>"},{"location":"Reference/linux/12%20Linux%20Terminal%20Shortcuts%20Every%20Power%20Linux%20User%20Must%20Know/#bonus-shortcut-ctrl-r-to-search-in-command-history","title":"Bonus Shortcut: Ctrl + R to search in Command History","text":"<p>You typed some command but cannot remember what it was exactly? Meet Ctrl + R.</p> <p>This keyboard shortcut allows you to perform a search in your command history. Just press Ctrl+R and start typing. It will show the last command that matches the string you typed. Note that the typed string could be anywhere in the command. How cool is that?</p> <p></p> <p>If you want to see more commands for the same string, just keep pressing Ctrl + R.</p> <p>You can press enter to run the command selected or press Esc to come out of the search with the last search result.</p>"},{"location":"Reference/linux/12%20Linux%20Terminal%20Shortcuts%20Every%20Power%20Linux%20User%20Must%20Know/#download-free-terminal-shortcut-cheatsheet","title":"Download FREE Terminal Shortcut Cheatsheet","text":""},{"location":"Reference/linux/Understanding%20Systemd%20Units%20and%20Unit%20Files%20%20DigitalOcean/","title":"Understanding Systemd Units and Unit Files DigitalOcean","text":"","tags":["linux"]},{"location":"Reference/linux/Understanding%20Systemd%20Units%20and%20Unit%20Files%20%20DigitalOcean/#introduction","title":"Introduction","text":"<p>Increasingly, Linux distributions are adopting the <code>systemd</code> init system. This powerful suite of software can manage many aspects of your server, from services to mounted devices and system states.</p> <p>In <code>systemd</code>, a <code>unit</code> refers to any resource that the system knows how to operate on and manage. This is the primary object that the <code>systemd</code> tools know how to deal with. These resources are defined using configuration files called unit files.</p> <p>In this guide, we will introduce you to the different units that <code>systemd</code> can handle. We will also be covering some of the many directives that can be used in unit files in order to shape the way these resources are handled on your system.</p>","tags":["linux"]},{"location":"Reference/linux/Understanding%20Systemd%20Units%20and%20Unit%20Files%20%20DigitalOcean/#what-do-systemd-units-give-you","title":"What Do Systemd Units Give You?","text":"<p>Units are the objects that <code>systemd</code> knows how to manage. These are basically a standardized representation of system resources that can be managed by the suite of daemons and manipulated by the provided utilities.</p> <p>Units can be said to be similar to services or jobs in other init systems. However, a unit has a much broader definition, as these can be used to abstract services, network resources, devices, filesystem mounts, and isolated resource pools.</p> <p>Ideas that in other init systems may be handled with one unified service definition can be broken out into component units according to their focus. This organizes by function and allows you to easily enable, disable, or extend functionality without modifying the core behavior of a unit.</p> <p>Some features that units are able implement easily are:</p> <ul> <li>socket-based activation: Sockets associated with a service are best broken out of the daemon itself in order to be handled separately. This provides a number of advantages, such as delaying the start of a service until the associated socket is first accessed. This also allows the system to create all sockets early in the boot process, making it possible to boot the associated services in parallel.</li> <li>bus-based activation: Units can also be activated on the bus interface provided by <code>D-Bus</code>. A unit can be started when an associated bus is published.</li> <li>path-based activation: A unit can be started based on activity on or the availability of certain filesystem paths. This utilizes <code>inotify</code>.</li> <li>device-based activation: Units can also be started at the first availability of associated hardware by leveraging <code>udev</code> events.</li> <li>implicit dependency mapping: Most of the dependency tree for units can be built by <code>systemd</code> itself. You can still add dependency and ordering information, but most of the heavy lifting is taken care of for you.</li> <li>instances and templates: Template unit files can be used to create multiple instances of the same general unit. This allows for slight variations or sibling units that all provide the same general function.</li> <li>easy security hardening: Units can implement some fairly good security features by adding simple directives. For example, you can specify no or read-only access to part of the filesystem, limit kernel capabilities, and assign private <code>/tmp</code> and network access.</li> <li>drop-ins and snippets: Units can easily be extended by providing snippets that will override parts of the system\u2019s unit file. This makes it easy to switch between vanilla and customized unit implementations.</li> </ul> <p>There are many other advantages that <code>systemd</code> units have over other init systems\u2019 work items, but this should give you an idea of the power that can be leveraged using native configuration directives.</p>","tags":["linux"]},{"location":"Reference/linux/Understanding%20Systemd%20Units%20and%20Unit%20Files%20%20DigitalOcean/#where-are-systemd-unit-files-found","title":"Where Are Systemd Unit Files Found?","text":"<p>The files that define how <code>systemd</code> will handle a unit can be found in many different locations, each of which have different priorities and implications.</p> <p>The system\u2019s copy of unit files are generally kept in the <code>/lib/systemd/system</code> directory. When software installs unit files on the system, this is the location where they are placed by default.</p> <p>Unit files stored here are able to be started and stopped on-demand during a session. This will be the generic, vanilla unit file, often written by the upstream project\u2019s maintainers that should work on any system that deploys <code>systemd</code> in its standard implementation. You should not edit files in this directory. Instead you should override the file, if necessary, using another unit file location which will supersede the file in this location.</p> <p>If you wish to modify the way that a unit functions, the best location to do so is within the <code>/etc/systemd/system</code> directory. Unit files found in this directory location take precedence over any of the other locations on the filesystem. If you need to modify the system\u2019s copy of a unit file, putting a replacement in this directory is the safest and most flexible way to do this.</p> <p>If you wish to override only specific directives from the system\u2019s unit file, you can actually provide unit file snippets within a subdirectory. These will append or modify the directives of the system\u2019s copy, allowing you to specify only the options you want to change.</p> <p>The correct way to do this is to create a directory named after the unit file with <code>.d</code> appended on the end. So for a unit called <code>example.service</code>, a subdirectory called <code>example.service.d</code> could be created. Within this directory a file ending with <code>.conf</code> can be used to override or extend the attributes of the system\u2019s unit file.</p> <p>There is also a location for run-time unit definitions at <code>/run/systemd/system</code>. Unit files found in this directory have a priority landing between those in <code>/etc/systemd/system</code> and <code>/lib/systemd/system</code>. Files in this location are given less weight than the former location, but more weight than the latter.</p> <p>The <code>systemd</code> process itself uses this location for dynamically created unit files created at runtime. This directory can be used to change the system\u2019s unit behavior for the duration of the session. All changes made in this directory will be lost when the server is rebooted.</p>","tags":["linux"]},{"location":"Reference/linux/Understanding%20Systemd%20Units%20and%20Unit%20Files%20%20DigitalOcean/#types-of-units","title":"Types of Units","text":"<p><code>Systemd</code> categories units according to the type of resource they describe. The easiest way to determine the type of a unit is with its type suffix, which is appended to the end of the resource name. The following list describes the types of units available to <code>systemd</code>:</p> <ul> <li><code>.service</code>: A service unit describes how to manage a service or application on the server. This will include how to start or stop the service, under which circumstances it should be automatically started, and the dependency and ordering information for related software.</li> <li><code>.socket</code>: A socket unit file describes a network or IPC socket, or a FIFO buffer that <code>systemd</code> uses for socket-based activation. These always have an associated <code>.service</code> file that will be started when activity is seen on the socket that this unit defines.</li> <li><code>.device</code>: A unit that describes a device that has been designated as needing <code>systemd</code> management by <code>udev</code> or the <code>sysfs</code> filesystem. Not all devices will have <code>.device</code> files. Some scenarios where <code>.device</code> units may be necessary are for ordering, mounting, and accessing the devices.</li> <li><code>.mount</code>: This unit defines a mountpoint on the system to be managed by <code>systemd</code>. These are named after the mount path, with slashes changed to dashes. Entries within <code>/etc/fstab</code> can have units created automatically.</li> <li><code>.automount</code>: An <code>.automount</code> unit configures a mountpoint that will be automatically mounted. These must be named after the mount point they refer to and must have a matching <code>.mount</code> unit to define the specifics of the mount.</li> <li><code>.swap</code>: This unit describes swap space on the system. The name of these units must reflect the device or file path of the space.</li> <li><code>.target</code>: A target unit is used to provide synchronization points for other units when booting up or changing states. They also can be used to bring the system to a new state. Other units specify their relation to targets to become tied to the target\u2019s operations.</li> <li><code>.path</code>: This unit defines a path that can be used for path-based activation. By default, a <code>.service</code> unit of the same base name will be started when the path reaches the specified state. This uses <code>inotify</code> to monitor the path for changes.</li> <li><code>.timer</code>: A <code>.timer</code> unit defines a timer that will be managed by <code>systemd</code>, similar to a <code>cron</code> job for delayed or scheduled activation. A matching unit will be started when the timer is reached.</li> <li><code>.snapshot</code>: A <code>.snapshot</code> unit is created automatically by the <code>systemctl snapshot</code> command. It allows you to reconstruct the current state of the system after making changes. Snapshots do not survive across sessions and are used to roll back temporary states.</li> <li><code>.slice</code>: A <code>.slice</code> unit is associated with Linux Control Group nodes, allowing resources to be restricted or assigned to any processes associated with the slice. The name reflects its hierarchical position within the <code>cgroup</code> tree. Units are placed in certain slices by default depending on their type.</li> <li><code>.scope</code>: Scope units are created automatically by <code>systemd</code> from information received from its bus interfaces. These are used to manage sets of system processes that are created externally.</li> </ul> <p>As you can see, there are many different units that <code>systemd</code> knows how to manage. Many of the unit types work together to add functionality. For instance, some units are used to trigger other units and provide activation functionality.</p> <p>We will mainly be focusing on <code>.service</code> units due to their utility and the consistency in which administrators need to managed these units.</p>","tags":["linux"]},{"location":"Reference/linux/Understanding%20Systemd%20Units%20and%20Unit%20Files%20%20DigitalOcean/#anatomy-of-a-unit-file","title":"Anatomy of a Unit File","text":"<p>The internal structure of unit files are organized with sections. Sections are denoted by a pair of square brackets \u201c<code>[</code>\u201d and \u201c<code>]</code>\u201d with the section name enclosed within. Each section extends until the beginning of the subsequent section or until the end of the file.</p>","tags":["linux"]},{"location":"Reference/linux/Understanding%20Systemd%20Units%20and%20Unit%20Files%20%20DigitalOcean/#general-characteristics-of-unit-files","title":"General Characteristics of Unit Files","text":"<p>Section names are well-defined and case-sensitive. So, the section <code>[Unit]</code> will not be interpreted correctly if it is spelled like <code>[UNIT]</code>. If you need to add non-standard sections to be parsed by applications other than <code>systemd</code>, you can add a <code>X-</code> prefix to the section name.</p> <p>Within these sections, unit behavior and metadata is defined through the use of simple directives using a key-value format with assignment indicated by an equal sign, like this:</p> <pre><code>[Section]\nDirective1=value\nDirective2=value\n\n. . .\n</code></pre> <p>In the event of an override file (such as those contained in a <code>==unit==.==type==.d</code> directory), directives can be reset by assigning them to an empty string. For example, the system\u2019s copy of a unit file may contain a directive set to a value like this:</p> <pre><code>Directive1=default_value\n</code></pre> <p>The <code>default_value</code> can be eliminated in an override file by referencing <code>==Directive1==</code> without a value, like this:</p> <pre><code>Directive1=\n</code></pre> <p>In general, <code>systemd</code> allows for easy and flexible configuration. For example, multiple boolean expressions are accepted (<code>1</code>, <code>yes</code>, <code>on</code>, and <code>true</code> for affirmative and <code>0</code>, <code>no</code> <code>off</code>, and <code>false</code> for the opposite answer). Times can be intelligently parsed, with seconds assumed for unit-less values and combining multiple formats accomplished internally.</p>","tags":["linux"]},{"location":"Reference/linux/Understanding%20Systemd%20Units%20and%20Unit%20Files%20%20DigitalOcean/#unit-section-directives","title":"[Unit] Section Directives","text":"<p>The first section found in most unit files is the <code>[Unit]</code> section. This is generally used for defining metadata for the unit and configuring the relationship of the unit to other units.</p> <p>Although section order does not matter to <code>systemd</code> when parsing the file, this section is often placed at the top because it provides an overview of the unit. Some common directives that you will find in the <code>[Unit]</code> section are:</p> <ul> <li><code>Description=</code>: This directive can be used to describe the name and basic functionality of the unit. It is returned by various <code>systemd</code> tools, so it is good to set this to something short, specific, and informative.</li> <li><code>Documentation=</code>: This directive provides a location for a list of URIs for documentation. These can be either internally available <code>man</code> pages or web accessible URLs. The <code>systemctl status</code> command will expose this information, allowing for easy discoverability.</li> <li><code>Requires=</code>: This directive lists any units upon which this unit essentially depends. If the current unit is activated, the units listed here must successfully activate as well, else this unit will fail. These units are started in parallel with the current unit by default.</li> <li><code>Wants=</code>: This directive is similar to <code>Requires=</code>, but less strict. <code>Systemd</code> will attempt to start any units listed here when this unit is activated. If these units are not found or fail to start, the current unit will continue to function. This is the recommended way to configure most dependency relationships. Again, this implies a parallel activation unless modified by other directives.</li> <li><code>BindsTo=</code>: This directive is similar to <code>Requires=</code>, but also causes the current unit to stop when the associated unit terminates.</li> <li><code>Before=</code>: The units listed in this directive will not be started until the current unit is marked as started if they are activated at the same time. This does not imply a dependency relationship and must be used in conjunction with one of the above directives if this is desired.</li> <li><code>After=</code>: The units listed in this directive will be started before starting the current unit. This does not imply a dependency relationship and one must be established through the above directives if this is required.</li> <li><code>Conflicts=</code>: This can be used to list units that cannot be run at the same time as the current unit. Starting a unit with this relationship will cause the other units to be stopped.</li> <li><code>Condition\u2026=</code>: There are a number of directives that start with <code>Condition</code> which allow the administrator to test certain conditions prior to starting the unit. This can be used to provide a generic unit file that will only be run when on appropriate systems. If the condition is not met, the unit is gracefully skipped.</li> <li><code>Assert\u2026=</code>: Similar to the directives that start with <code>Condition</code>, these directives check for different aspects of the running environment to decide whether the unit should activate. However, unlike the <code>Condition</code> directives, a negative result causes a failure with this directive.</li> </ul> <p>Using these directives and a handful of others, general information about the unit and its relationship to other units and the operating system can be established.</p>","tags":["linux"]},{"location":"Reference/linux/Understanding%20Systemd%20Units%20and%20Unit%20Files%20%20DigitalOcean/#install-section-directives","title":"[Install] Section Directives","text":"<p>On the opposite side of unit file, the last section is often the <code>[Install]</code> section. This section is optional and is used to define the behavior or a unit if it is enabled or disabled. Enabling a unit marks it to be automatically started at boot. In essence, this is accomplished by latching the unit in question onto another unit that is somewhere in the line of units to be started at boot.</p> <p>Because of this, only units that can be enabled will have this section. The directives within dictate what should happen when the unit is enabled:</p> <ul> <li><code>WantedBy=</code>: The <code>WantedBy=</code> directive is the most common way to specify how a unit should be enabled. This directive allows you to specify a dependency relationship in a similar way to the <code>Wants=</code> directive does in the <code>[Unit]</code> section. The difference is that this directive is included in the ancillary unit allowing the primary unit listed to remain relatively clean. When a unit with this directive is enabled, a directory will be created within <code>/etc/systemd/system</code> named after the specified unit with <code>.wants</code> appended to the end. Within this, a symbolic link to the current unit will be created, creating the dependency. For instance, if the current unit has <code>WantedBy=multi-user.target</code>, a directory called <code>multi-user.target.wants</code> will be created within <code>/etc/systemd/system</code> (if not already available) and a symbolic link to the current unit will be placed within. Disabling this unit removes the link and removes the dependency relationship.</li> <li><code>RequiredBy=</code>: This directive is very similar to the <code>WantedBy=</code> directive, but instead specifies a required dependency that will cause the activation to fail if not met. When enabled, a unit with this directive will create a directory ending with <code>.requires</code>.</li> <li><code>Alias=</code>: This directive allows the unit to be enabled under another name as well. Among other uses, this allows multiple providers of a function to be available, so that related units can look for any provider of the common aliased name.</li> <li><code>Also=</code>: This directive allows units to be enabled or disabled as a set. Supporting units that should always be available when this unit is active can be listed here. They will be managed as a group for installation tasks.</li> <li><code>DefaultInstance=</code>: For template units (covered later) which can produce unit instances with unpredictable names, this can be used as a fallback value for the name if an appropriate name is not provided.</li> </ul>","tags":["linux"]},{"location":"Reference/linux/Understanding%20Systemd%20Units%20and%20Unit%20Files%20%20DigitalOcean/#unit-specific-section-directives","title":"Unit-Specific Section Directives","text":"<p>Sandwiched between the previous two sections, you will likely find unit type-specific sections. Most unit types offer directives that only apply to their specific type. These are available within sections named after their type. We will cover those briefly here.</p> <p>The <code>device</code>, <code>target</code>, <code>snapshot</code>, and <code>scope</code> unit types have no unit-specific directives, and thus have no associated sections for their type.</p>","tags":["linux"]},{"location":"Reference/linux/Understanding%20Systemd%20Units%20and%20Unit%20Files%20%20DigitalOcean/#the-service-section","title":"The [Service] Section","text":"<p>The <code>[Service]</code> section is used to provide configuration that is only applicable for services.</p> <p>One of the basic things that should be specified within the <code>[Service]</code> section is the <code>Type=</code> of the service. This categorizes services by their process and daemonizing behavior. This is important because it tells <code>systemd</code> how to correctly manage the service and find out its state.</p> <p>The <code>Type=</code> directive can be one of the following:</p> <ul> <li>simple: The main process of the service is specified in the start line. This is the default if the <code>Type=</code> and <code>Busname=</code> directives are not set, but the <code>ExecStart=</code> is set. Any communication should be handled outside of the unit through a second unit of the appropriate type (like through a <code>.socket</code> unit if this unit must communicate using sockets).</li> <li>forking: This service type is used when the service forks a child process, exiting the parent process almost immediately. This tells <code>systemd</code> that the process is still running even though the parent exited.</li> <li>oneshot: This type indicates that the process will be short-lived and that <code>systemd</code> should wait for the process to exit before continuing on with other units. This is the default <code>Type=</code> and <code>ExecStart=</code> are not set. It is used for one-off tasks.</li> <li>dbus: This indicates that unit will take a name on the D-Bus bus. When this happens, <code>systemd</code> will continue to process the next unit.</li> <li>notify: This indicates that the service will issue a notification when it has finished starting up. The <code>systemd</code> process will wait for this to happen before proceeding to other units.</li> <li>idle: This indicates that the service will not be run until all jobs are dispatched.</li> </ul> <p>Some additional directives may be needed when using certain service types. For instance:</p> <ul> <li><code>RemainAfterExit=</code>: This directive is commonly used with the <code>oneshot</code> type. It indicates that the service should be considered active even after the process exits.</li> <li><code>PIDFile=</code>: If the service type is marked as \u201cforking\u201d, this directive is used to set the path of the file that should contain the process ID number of the main child that should be monitored.</li> <li><code>BusName=</code>: This directive should be set to the D-Bus bus name that the service will attempt to acquire when using the \u201cdbus\u201d service type.</li> <li><code>NotifyAccess=</code>: This specifies access to the socket that should be used to listen for notifications when the \u201cnotify\u201d service type is selected This can be \u201cnone\u201d, \u201cmain\u201d, or \"all. The default, \u201cnone\u201d, ignores all status messages. The \u201cmain\u201d option will listen to messages from the main process and the \u201call\u201d option will cause all members of the service\u2019s control group to be processed.</li> </ul> <p>So far, we have discussed some pre-requisite information, but we haven\u2019t actually defined how to manage our services. The directives to do this are:</p> <ul> <li><code>ExecStart=</code>: This specifies the full path and the arguments of the command to be executed to start the process. This may only be specified once (except for \u201coneshot\u201d services). If the path to the command is preceded by a dash \u201c-\u201d character, non-zero exit statuses will be accepted without marking the unit activation as failed.</li> <li><code>ExecStartPre=</code>: This can be used to provide additional commands that should be executed before the main process is started. This can be used multiple times. Again, commands must specify a full path and they can be preceded by \u201c-\u201d to indicate that the failure of the command will be tolerated.</li> <li><code>ExecStartPost=</code>: This has the same exact qualities as <code>ExecStartPre=</code> except that it specifies commands that will be run after the main process is started.</li> <li><code>ExecReload=</code>: This optional directive indicates the command necessary to reload the configuration of the service if available.</li> <li><code>ExecStop=</code>: This indicates the command needed to stop the service. If this is not given, the process will be killed immediately when the service is stopped.</li> <li><code>ExecStopPost=</code>: This can be used to specify commands to execute following the stop command.</li> <li><code>RestartSec=</code>: If automatically restarting the service is enabled, this specifies the amount of time to wait before attempting to restart the service.</li> <li><code>Restart=</code>: This indicates the circumstances under which <code>systemd</code> will attempt to automatically restart the service. This can be set to values like \u201calways\u201d, \u201con-success\u201d, \u201con-failure\u201d, \u201con-abnormal\u201d, \u201con-abort\u201d, or \u201con-watchdog\u201d. These will trigger a restart according to the way that the service was stopped.</li> <li><code>TimeoutSec=</code>: This configures the amount of time that <code>systemd</code> will wait when stopping or stopping the service before marking it as failed or forcefully killing it. You can set separate timeouts with <code>TimeoutStartSec=</code> and <code>TimeoutStopSec=</code> as well.</li> </ul>","tags":["linux"]},{"location":"Reference/linux/Understanding%20Systemd%20Units%20and%20Unit%20Files%20%20DigitalOcean/#the-socket-section","title":"The [Socket] Section","text":"<p>Socket units are very common in <code>systemd</code> configurations because many services implement socket-based activation to provide better parallelization and flexibility. Each socket unit must have a matching service unit that will be activated when the socket receives activity.</p> <p>By breaking socket control outside of the service itself, sockets can be initialized early and the associated services can often be started in parallel. By default, the socket name will attempt to start the service of the same name upon receiving a connection. When the service is initialized, the socket will be passed to it, allowing it to begin processing any buffered requests.</p> <p>To specify the actual socket, these directives are common:</p> <ul> <li><code>ListenStream=</code>: This defines an address for a stream socket which supports sequential, reliable communication. Services that use TCP should use this socket type.</li> <li><code>ListenDatagram=</code>: This defines an address for a datagram socket which supports fast, unreliable communication packets. Services that use UDP should set this socket type.</li> <li><code>ListenSequentialPacket=</code>: This defines an address for sequential, reliable communication with max length datagrams that preserves message boundaries. This is found most often for Unix sockets.</li> <li><code>ListenFIFO</code>: Along with the other listening types, you can also specify a FIFO buffer instead of a socket.</li> </ul> <p>There are more types of listening directives, but the ones above are the most common.</p> <p>Other characteristics of the sockets can be controlled through additional directives:</p> <ul> <li><code>Accept=</code>: This determines whether an additional instance of the service will be started for each connection. If set to false (the default), one instance will handle all connections.</li> <li><code>SocketUser=</code>: With a Unix socket, specifies the owner of the socket. This will be the root user if left unset.</li> <li><code>SocketGroup=</code>: With a Unix socket, specifies the group owner of the socket. This will be the root group if neither this or the above are set. If only the <code>SocketUser=</code> is set, <code>systemd</code> will try to find a matching group.</li> <li><code>SocketMode=</code>: For Unix sockets or FIFO buffers, this sets the permissions on the created entity.</li> <li><code>Service=</code>: If the service name does not match the <code>.socket</code> name, the service can be specified with this directive.</li> </ul>","tags":["linux"]},{"location":"Reference/linux/Understanding%20Systemd%20Units%20and%20Unit%20Files%20%20DigitalOcean/#the-mount-section","title":"The [Mount] Section","text":"<p>Mount units allow for mount point management from within <code>systemd</code>. Mount points are named after the directory that they control, with a translation algorithm applied.</p> <p>For example, the leading slash is removed, all other slashes are translated into dashes \u201c-\u201d, and all dashes and unprintable characters are replaced with C-style escape codes. The result of this translation is used as the mount unit name. Mount units will have an implicit dependency on other mounts above it in the hierarchy.</p> <p>Mount units are often translated directly from <code>/etc/fstab</code> files during the boot process. For the unit definitions automatically created and those that you wish to define in a unit file, the following directives are useful:</p> <ul> <li><code>What=</code>: The absolute path to the resource that needs to be mounted.</li> <li><code>Where=</code>: The absolute path of the mount point where the resource should be mounted. This should be the same as the unit file name, except using conventional filesystem notation.</li> <li><code>Type=</code>: The filesystem type of the mount.</li> <li><code>Options=</code>: Any mount options that need to be applied. This is a comma-separated list.</li> <li><code>SloppyOptions=</code>: A boolean that determines whether the mount will fail if there is an unrecognized mount option.</li> <li><code>DirectoryMode=</code>: If parent directories need to be created for the mount point, this determines the permission mode of these directories.</li> <li><code>TimeoutSec=</code>: Configures the amount of time the system will wait until the mount operation is marked as failed.</li> </ul>","tags":["linux"]},{"location":"Reference/linux/Understanding%20Systemd%20Units%20and%20Unit%20Files%20%20DigitalOcean/#the-automount-section","title":"The [Automount] Section","text":"<p>This unit allows an associated <code>.mount</code> unit to be automatically mounted at boot. As with the <code>.mount</code> unit, these units must be named after the translated mount point\u2019s path.</p> <p>The <code>[Automount]</code> section is pretty simple, with only the following two options allowed:</p> <ul> <li><code>Where=</code>: The absolute path of the automount point on the filesystem. This will match the filename except that it uses conventional path notation instead of the translation.</li> <li><code>DirectoryMode=</code>: If the automount point or any parent directories need to be created, this will determine the permissions settings of those path components.</li> </ul>","tags":["linux"]},{"location":"Reference/linux/Understanding%20Systemd%20Units%20and%20Unit%20Files%20%20DigitalOcean/#the-swap-section","title":"The [Swap] Section","text":"<p>Swap units are used to configure swap space on the system. The units must be named after the swap file or the swap device, using the same filesystem translation that was discussed above.</p> <p>Like the mount options, the swap units can be automatically created from <code>/etc/fstab</code> entries, or can be configured through a dedicated unit file.</p> <p>The <code>[Swap]</code> section of a unit file can contain the following directives for configuration:</p> <ul> <li><code>What=</code>: The absolute path to the location of the swap space, whether this is a file or a device.</li> <li><code>Priority=</code>: This takes an integer that indicates the priority of the swap being configured.</li> <li><code>Options=</code>: Any options that are typically set in the <code>/etc/fstab</code> file can be set with this directive instead. A comma-separated list is used.</li> <li><code>TimeoutSec=</code>: The amount of time that <code>systemd</code> waits for the swap to be activated before marking the operation as a failure.</li> </ul>","tags":["linux"]},{"location":"Reference/linux/Understanding%20Systemd%20Units%20and%20Unit%20Files%20%20DigitalOcean/#the-path-section","title":"The [Path] Section","text":"<p>A path unit defines a filesystem path that <code>systmed</code> can monitor for changes. Another unit must exist that will be be activated when certain activity is detected at the path location. Path activity is determined through <code>inotify</code> events.</p> <p>The <code>[Path]</code> section of a unit file can contain the following directives:</p> <ul> <li><code>PathExists=</code>: This directive is used to check whether the path in question exists. If it does, the associated unit is activated.</li> <li><code>PathExistsGlob=</code>: This is the same as the above, but supports file glob expressions for determining path existence.</li> <li><code>PathChanged=</code>: This watches the path location for changes. The associated unit is activated if a change is detected when the watched file is closed.</li> <li><code>PathModified=</code>: This watches for changes like the above directive, but it activates on file writes as well as when the file is closed.</li> <li><code>DirectoryNotEmpty=</code>: This directive allows <code>systemd</code> to activate the associated unit when the directory is no longer empty.</li> <li><code>Unit=</code>: This specifies the unit to activate when the path conditions specified above are met. If this is omitted, <code>systemd</code> will look for a <code>.service</code> file that shares the same base unit name as this unit.</li> <li><code>MakeDirectory=</code>: This determines if <code>systemd</code> will create the directory structure of the path in question prior to watching.</li> <li><code>DirectoryMode=</code>: If the above is enabled, this will set the permission mode of any path components that must be created.</li> </ul>","tags":["linux"]},{"location":"Reference/linux/Understanding%20Systemd%20Units%20and%20Unit%20Files%20%20DigitalOcean/#the-timer-section","title":"The [Timer] Section","text":"<p>Timer units are used to schedule tasks to operate at a specific time or after a certain delay. This unit type replaces or supplements some of the functionality of the <code>cron</code> and <code>at</code> daemons. An associated unit must be provided which will be activated when the timer is reached.</p> <p>The <code>[Timer]</code> section of a unit file can contain some of the following directives:</p> <ul> <li><code>OnActiveSec=</code>: This directive allows the associated unit to be activated relative to the <code>.timer</code> unit\u2019s activation.</li> <li><code>OnBootSec=</code>: This directive is used to specify the amount of time after the system is booted when the associated unit should be activated.</li> <li><code>OnStartupSec=</code>: This directive is similar to the above timer, but in relation to when the <code>systemd</code> process itself was started.</li> <li><code>OnUnitActiveSec=</code>: This sets a timer according to when the associated unit was last activated.</li> <li><code>OnUnitInactiveSec=</code>: This sets the timer in relation to when the associated unit was last marked as inactive.</li> <li><code>OnCalendar=</code>: This allows you to activate the associated unit by specifying an absolute instead of relative to an event.</li> <li><code>AccuracySec=</code>: This unit is used to set the level of accuracy with which the timer should be adhered to. By default, the associated unit will be activated within one minute of the timer being reached. The value of this directive will determine the upper bounds on the window in which <code>systemd</code> schedules the activation to occur.</li> <li><code>Unit=</code>: This directive is used to specify the unit that should be activated when the timer elapses. If unset, <code>systemd</code> will look for a <code>.service</code> unit with a name that matches this unit.</li> <li><code>Persistent=</code>: If this is set, <code>systemd</code> will trigger the associated unit when the timer becomes active if it would have been triggered during the period in which the timer was inactive.</li> <li><code>WakeSystem=</code>: Setting this directive allows you to wake a system from suspend if the timer is reached when in that state.</li> </ul>","tags":["linux"]},{"location":"Reference/linux/Understanding%20Systemd%20Units%20and%20Unit%20Files%20%20DigitalOcean/#the-slice-section","title":"The [Slice] Section","text":"<p>The <code>[Slice]</code> section of a unit file actually does not have any <code>.slice</code> unit specific configuration. Instead, it can contain some resource management directives that are actually available to a number of the units listed above.</p> <p>Some common directives in the <code>[Slice]</code> section, which may also be used in other units can be found in the <code>systemd.resource-control</code> man page. These are valid in the following unit-specific sections:</p> <ul> <li><code>[Slice]</code></li> <li><code>[Scope]</code></li> <li><code>[Service]</code></li> <li><code>[Socket]</code></li> <li><code>[Mount]</code></li> <li><code>[Swap]</code></li> </ul>","tags":["linux"]},{"location":"Reference/linux/Understanding%20Systemd%20Units%20and%20Unit%20Files%20%20DigitalOcean/#creating-instance-units-from-template-unit-files","title":"Creating Instance Units from Template Unit Files","text":"<p>We mentioned earlier in this guide the idea of template unit files being used to create multiple instances of units. In this section, we can go over this concept in more detail.</p> <p>Template unit files are, in most ways, no different than regular unit files. However, these provide flexibility in configuring units by allowing certain parts of the file to utilize dynamic information that will be available at runtime.</p>","tags":["linux"]},{"location":"Reference/linux/Understanding%20Systemd%20Units%20and%20Unit%20Files%20%20DigitalOcean/#template-and-instance-unit-names","title":"Template and Instance Unit Names","text":"<p>Template unit files can be identified because they contain an <code>@</code> symbol after the base unit name and before the unit type suffix. A template unit file name may look like this:</p> <pre><code>example@.service\n</code></pre> <p>When an instance is created from a template, an instance identifier is placed between the <code>@</code> symbol and the period signifying the start of the unit type. For example, the above template unit file could be used to create an instance unit that looks like this:</p> <pre><code>example@instance1.service\n</code></pre> <p>An instance file is usually created as a symbolic link to the template file, with the link name including the instance identifier. In this way, multiple links with unique identifiers can point back to a single template file. When managing an instance unit, <code>systemd</code> will look for a file with the exact instance name you specify on the command line to use. If it cannot find one, it will look for an associated template file.</p>","tags":["linux"]},{"location":"Reference/linux/Understanding%20Systemd%20Units%20and%20Unit%20Files%20%20DigitalOcean/#template-specifiers","title":"Template Specifiers","text":"<p>The power of template unit files is mainly seen through its ability to dynamically substitute appropriate information within the unit definition according to the operating environment. This is done by setting the directives in the template file as normal, but replacing certain values or parts of values with variable specifiers.</p> <p>The following are some of the more common specifiers will be replaced when an instance unit is interpreted with the relevant information:</p> <ul> <li><code>%n</code>: Anywhere where this appears in a template file, the full resulting unit name will be inserted.</li> <li><code>%N</code>: This is the same as the above, but any escaping, such as those present in file path patterns, will be reversed.</li> <li><code>%p</code>: This references the unit name prefix. This is the portion of the unit name that comes before the <code>@</code> symbol.</li> <li><code>%P</code>: This is the same as above, but with any escaping reversed.</li> <li><code>%i</code>: This references the instance name, which is the identifier following the <code>@</code> in the instance unit. This is one of the most commonly used specifiers because it will be guaranteed to be dynamic. The use of this identifier encourages the use of configuration significant identifiers. For example, the port that the service will be run at can be used as the instance identifier and the template can use this specifier to set up the port specification.</li> <li><code>%I</code>: This specifier is the same as the above, but with any escaping reversed.</li> <li><code>%f</code>: This will be replaced with the unescaped instance name or the prefix name, prepended with a <code>/</code>.</li> <li><code>%c</code>: This will indicate the control group of the unit, with the standard parent hierarchy of <code>/sys/fs/cgroup/ssytemd/</code> removed.</li> <li><code>%u</code>: The name of the user configured to run the unit.</li> <li><code>%U</code>: The same as above, but as a numeric <code>UID</code> instead of name.</li> <li><code>%H</code>: The host name of the system that is running the unit.</li> <li><code>%%</code>: This is used to insert a literal percentage sign.</li> </ul> <p>By using the above identifiers in a template file, <code>systemd</code> will fill in the correct values when interpreting the template to create an instance unit.</p>","tags":["linux"]},{"location":"Reference/linux/Understanding%20Systemd%20Units%20and%20Unit%20Files%20%20DigitalOcean/#conclusion","title":"Conclusion","text":"<p>When working with <code>systemd</code>, understanding units and unit files can make administration easier. Unlike many other init systems, you do not have to know a scripting language to interpret the init files used to boot services or the system. The unit files use a fairly straightforward declarative syntax that allows you to see at a glance the purpose and effects of a unit upon activation.</p> <p>Breaking functionality such as activation logic into separate units not only allows the internal <code>systemd</code> processes to optimize parallel initialization, it also keeps the configuration rather simple and allows you to modify and restart some units without tearing down and rebuilding their associated connections. Leveraging these abilities can give you more flexibility and power during administration.</p>","tags":["linux"]},{"location":"Reference/obsidian/My%2010%2B%20Obsidian%20Plugins%20As%20a%20Medium%20Writer/","title":"My 10+ Obsidian Plugins As a Medium Writer","text":"","tags":["plugin"]},{"location":"Reference/obsidian/My%2010%2B%20Obsidian%20Plugins%20As%20a%20Medium%20Writer/#streamline-your-workflow-top-plugins-for-obsidian-writers","title":"Streamline Your Workflow: Top Plugins for Obsidian Writers","text":"<p>[</p> <p></p> <p>](https://wesley-wei.medium.com/?source=post_page-----6f440119587f--------------------------------)[</p> <p></p> <p>](https://medium.programmerscareer.com/?source=post_page-----6f440119587f--------------------------------)</p> <p></p> <p>Photo by Claudio Schwarz on Unsplash</p> <p>Hello, here is Wesley, Today\u2019s article is about Obsidian Plugins. Without further ado, let\u2019s get started.\ud83d\udcaa</p>","tags":["plugin"]},{"location":"Reference/obsidian/My%2010%2B%20Obsidian%20Plugins%20As%20a%20Medium%20Writer/#overview","title":"Overview","text":"<p>I highly recommend Richard Feynman\u2019s approach to promoting self-learning through output. First, unreflected output is useless and won\u2019t attract anyone\u2019s attention; second, when you decide to write an article introducing a piece of knowledge, you need to convince yourself that you\u2019ve learned something before wanting to share it with others.</p> <p>Unconsciously, output will become a huge driving force, allowing you to enter the learning state faster and helping you learn more deeply. Moreover, each time you output, it\u2019s like reviewing and refining your understanding of the knowledge for yourself, and sharing it with others through records and exchanges.</p> <p>I\u2019ve been writing on Medium for some time now, and I think there are many good feedback mechanisms here because there are many readers and a paid subscription wall. So why not continue writing on Medium?</p> <p>In my journey as a writer, I\u2019ve used many note-taking software tools, including OneNote and Notion, but they all have their limitations. Now, I\u2019m using Obsidian as my main note-taking tool, which has native support for local files and a wide range of plugins that make me love it even more. This allows me to write on Medium with greater ease, and this article will share some simple tips.</p> <p>This article may be suitable for those who have used Obsidian before. Here, I\u2019ll introduce the plugins I commonly use when writing blog posts or outputting content.</p> <p>The focus of this article is on showcasing the effectiveness of using these plugins, rather than step-by-step instructions. If you think these plugins might be helpful to you, feel free to explore them yourself.</p>","tags":["plugin"]},{"location":"Reference/obsidian/My%2010%2B%20Obsidian%20Plugins%20As%20a%20Medium%20Writer/#1-content-related","title":"1. Content-related","text":"","tags":["plugin"]},{"location":"Reference/obsidian/My%2010%2B%20Obsidian%20Plugins%20As%20a%20Medium%20Writer/#11-templater","title":"1.1 Templater","text":"<p>This plugin helps me reduce repetitive work when creating new series articles. Whenever I create a new article, I consider using Templater to lower my workload.</p> <p>When you set up the template, it can generate content according to your settings, such as:</p> <p></p> <p>The corresponding Templater code is as follows:</p> <pre><code>---&lt;%*  let title = tp.file.title  if (title.startsWith(\"Untitled\")) {    title = await tp.system.prompt(\"Title\");    await tp.file.rename(title);  }%&gt;title: &lt;%* tR += title %&gt;author: Wesley Weidate: &lt;% tp.file.creation_date() %&gt;tags: stayaheadcategories: efficiency---&lt;% tp.web.random_picture(\"500x500\", \"landscape, nature\") %&gt;&lt;% tp.user.quote(tp) %&gt;\n</code></pre> <p>Note that the quote function is customized according to my own blog format:</p> <pre><code>async function quote(tp) {      str = await tp.web.daily_quote()    let newStr = str.replace(/&gt;\\s\\[!quote]\\s/g, \"\");    newStr = newStr.replace(/&gt;/g, \"\");  newStr = \"{% colorquote success %}\\n\" + newStr;  newStr += \"\\n{% endcolorquote %}\";  return newStr}module.exports = quote;\n</code></pre> <p>If you\u2019re interested in setting up more customized content, you can refer to Templater\u2019s official documentation.</p>","tags":["plugin"]},{"location":"Reference/obsidian/My%2010%2B%20Obsidian%20Plugins%20As%20a%20Medium%20Writer/#12-auto-link-title","title":"1.2 Auto Link Title","text":"<p>This plugin is very convenient and intuitive, so I directly quoted the official content:</p> <p></p> <p>This plugin automatically fetches the webpage to extract link titles when they\u2019re pasted, creating a markdown link with the correct title set.</p> <p>For example: When pasting https://github.com/zolrath/obsidian-auto-link-title, the plugin fetches the page and retrieves the title, resulting in a paste of: zolrath/obsidian-auto-link-title: Automatically fetch the titles of pasted links</p>","tags":["plugin"]},{"location":"Reference/obsidian/My%2010%2B%20Obsidian%20Plugins%20As%20a%20Medium%20Writer/#13-copilot","title":"1.3 Copilot","text":"<p>Writing processes can always benefit from AI assistance, such as generating medium article subtitles.</p> <p>For my article: Timer changes in Go 1.23: Enhancing Accuracy and Simplifying Concurrency | by Wesley Wei | Jul, 2024 | Programmer\u2019s Career, I used the Copilot plugin to ask Ollama for its opinion, and it provided the following response:</p> <p></p> <p>If you\u2019re familiar with this content, you\u2019ll find that its response is indeed worth referencing. The corresponding prompt is as follows:</p> <p></p> <p>AI is still rapidly developing, and its potential is limitless. If you input the corresponding prompt and question, it can provide relevant responses. We can fully utilize localized AI to boost productivity. Why not use it?</p> <p>Free AI tools are already very useful today. I welcome you to read my previous article:</p> <p>More about My AI Tools: Welcome to the AI Revolution: My Guide to Essential Tools and Concepts | by Wesley Wei | Jun, 2024 | Programmer\u2019s Career</p>","tags":["plugin"]},{"location":"Reference/obsidian/My%2010%2B%20Obsidian%20Plugins%20As%20a%20Medium%20Writer/#14-linter","title":"1.4 Linter","text":"<p>This plugin can help us standardize article formatting. I\u2019ve made some customizations, and whenever I trigger the save article shortcut key, this plugin can automatically format my article according to my customized style, as shown below:</p> <p></p> <p>If you\u2019re someone who writes frequently, you should understand what this means: time-saving.</p> <p>When I finish writing an article, all I need to do is input the shortcut key, and the article formatting will be basically done, which will definitely save a lot of time.</p>","tags":["plugin"]},{"location":"Reference/obsidian/My%2010%2B%20Obsidian%20Plugins%20As%20a%20Medium%20Writer/#2-usage-related","title":"2. Usage-related","text":"","tags":["plugin"]},{"location":"Reference/obsidian/My%2010%2B%20Obsidian%20Plugins%20As%20a%20Medium%20Writer/#21-another-quick-switcher-better-command-palette-omnisearch","title":"2.1 Another Quick Switcher, Better Command Palette, Omnisearch","text":"<p>What I use most often in Another Quick Switcher is \u2014 Recent search. By setting a shortcut key, I can quickly know which articles I\u2019ve recently opened.</p> <p></p> <p>Better Command Palette helps me find commands more easily. Similarly, by setting a shortcut key, I can quickly open the search and use the command.</p> <p></p> <p>Omnisearch performs faster and more detailed searches.</p>","tags":["plugin"]},{"location":"Reference/obsidian/My%2010%2B%20Obsidian%20Plugins%20As%20a%20Medium%20Writer/#22-excalibrain-excalidraw","title":"2.2 ExcaliBrain, Excalidraw","text":"<p>ExcaliBrain can help me view my articles from various angles and visualize them as images. It also allows me to see relationships between this article and other articles.</p> <p></p> <p>Excalidraw enables me to create diagrams locally, and with ExcaliBrain, I can modify the structure diagram of my article and generate an image that represents the structure I want.</p>","tags":["plugin"]},{"location":"Reference/obsidian/My%2010%2B%20Obsidian%20Plugins%20As%20a%20Medium%20Writer/#23-style-settings-supercharged-links","title":"2.3 Style Settings, Supercharged Links","text":"<p>Style Settings and Supercharged Links allow me to categorize my articles based on tags and other dimensions, which helps me manage my articles. For example, I set different colors for articles with different tags:</p> <p></p>","tags":["plugin"]},{"location":"Reference/obsidian/My%2010%2B%20Obsidian%20Plugins%20As%20a%20Medium%20Writer/#24-commander","title":"2.4 Commander","text":"<p>This plugin enables me to customize some frequently used commands. For instance, I can create a command to enter or exit focus mode and place it on the Tab Bar:</p> <p></p> <p></p> <p></p> <p>By clicking on the icon in the top-right corner, I can easily toggle the status bar on or off. Of course, this is just one small example, and there are many more things to explore.</p>","tags":["plugin"]},{"location":"Reference/obsidian/My%2010%2B%20Obsidian%20Plugins%20As%20a%20Medium%20Writer/#3-tools-related","title":"3. Tools-Related","text":"","tags":["plugin"]},{"location":"Reference/obsidian/My%2010%2B%20Obsidian%20Plugins%20As%20a%20Medium%20Writer/#31-image-auto-upload-plugin","title":"3.1 Image Auto Upload Plugin","text":"<p>This plugin helps us use PicGo to upload images to third-party services, without saving them locally. The uploading process is as follows:</p> <p></p>","tags":["plugin"]},{"location":"Reference/obsidian/My%2010%2B%20Obsidian%20Plugins%20As%20a%20Medium%20Writer/#32-git","title":"3.2 Git","text":"<p>When I publish my articles on Medium and my blog, I use this plugin to push them to GitHub with one click, then deploy the blog content to my server using GitHub Actions.</p>","tags":["plugin"]},{"location":"Reference/obsidian/My%2010%2B%20Obsidian%20Plugins%20As%20a%20Medium%20Writer/#33-terminal","title":"3.3 Terminal","text":"<p>We all know that Obsidian\u2019s default files are in Markdown format, but sometimes I need to create other types of files, such as a custom JavaScript file for the Template plugin. This requires me to switch to another window to create these types of files, which is inconvenient and distracts from my workflow.</p> <p>I solved this problem by using the Terminal plugin, which allows us to open a terminal in the same Obsidian instance and create a JavaScript file through the terminal.</p> <p></p> <p>Of course, if you\u2019re familiar with terminals, you\u2019ll understand that it can do many more things.</p>","tags":["plugin"]},{"location":"Reference/obsidian/My%2010%2B%20Obsidian%20Plugins%20As%20a%20Medium%20Writer/#4-conclusion","title":"4. Conclusion","text":"<p>All plugins can be downloaded through Community plugins, and you can choose the ones that are helpful to you based on the recommendations above.</p> <p></p> <p>I think there are too many plugins on Obsidian, which can be overwhelming. I believe we only need to select the ones that best suit our needs. I hope my recommended plugins include what you\u2019re looking for.</p> <p>The plugins mentioned in this article are all those I\u2019ve accumulated over time while using Obsidian to write slowly. They\u2019re just a small part of the iceberg when it comes to Obsidian plugins, and the usage scenarios are only personal examples. I welcome your comments telling me which plugins you like and how you use them.</p> <p>More Series Articles about Stay Ahead of The Curve\uff1a</p> <p></p>","tags":["plugin"]},{"location":"Reference/obsidian/My%2010%2B%20Obsidian%20Plugins%20As%20a%20Medium%20Writer/#stay-ahead-of-the-curve","title":"Stay Ahead of The Curve","text":"<p>And I\u2019m Wesley, delighted to share knowledge from the world of programming.</p> <p>Don\u2019t forget to follow me for more informative content, or feel free to share this with others who may also find it beneficial. it would be a great help to me.</p> <p>Give me some free applauds, highlights, or replies, and I\u2019ll pay attention to those reactions, which will determine whether or not I continue to post this type of article.</p> <p>See you in the next article. \ud83d\udc4b</p> <p>\u4e2d\u6587\u6587\u7ae0: https://programmerscareer.com/zh-cn/obsidian-plugins/Author: Wesley Wei \u2014 Twitter Wesley Wei \u2014 MediumNote: Originally written at https://programmerscareer.com/obsidian-plugins/ at 2024\u201307\u201314 21:38. If you choose to repost or use this article, please cite the original source.</p>","tags":["plugin"]},{"location":"VCS/git/","title":"Git","text":""},{"location":"VCS/git/#git_1","title":"Git","text":"<p>Git game in Learn Git Branching</p>"},{"location":"VCS/git/#init","title":"Init","text":""},{"location":"VCS/git/#auth","title":"Auth","text":"<pre><code>git config --global user.name \"A.J.Zeller\"\ngit config --global user.email \"hello@atticux.me\"\ngit config --global init.defaultBranch main\ngit config credential.helper\n</code></pre>"},{"location":"VCS/git/#ssh","title":"SSH","text":"<p>in admin powershell</p>"},{"location":"VCS/git/#start-ssh-agent","title":"Start Ssh-agent","text":"<pre><code>Get-Service -Name ssh-agent | Set-Service -StartupType Manual\nStart-Service ssh-agent\n</code></pre>"},{"location":"VCS/git/#generate-key-pair","title":"Generate Key-pair","text":"<pre><code>ssh-keygen -t ed25519 -C \"atticus.zeller@pm.me\"\n</code></pre> <p>in normal powershell</p>"},{"location":"VCS/git/#add-ssh-key-pair-into-ssh-agent","title":"Add Ssh-key-pair into Ssh-agent","text":"<pre><code>ssh-add C:\\Users\\18317\\.ssh\\id_ed25519\n</code></pre> <pre><code>eval $(ssh-agent -s)\nssh-add ~/.ssh/id_ed25519\n</code></pre>"},{"location":"VCS/git/#copy-id_ed25519pub-into-github_ssh_setting-set-auth-key-type","title":"Copy id_ed25519.pub into github_ssh_setting Set Auth Key Type","text":"<pre><code>cat ~/.ssh/id_ed25519.pub | clip\n</code></pre> <pre><code>cat ~/.ssh/id_ed25519.pub\n</code></pre>"},{"location":"VCS/git/#test-ssh-connect-by-https","title":"Test Ssh Connect by Https","text":"<pre><code>ssh -T -p 443 git@ssh.github.com\n</code></pre>"},{"location":"VCS/git/#add-config","title":"Add Config","text":"<p>add the following text into <code>~/.ssh/config</code></p> <pre><code>nano ~/.ssh/config\n</code></pre> <p>windows in <code>c:\\user\\your_user_name\\.ssh\\config</code></p> <pre><code>Host github.com\n    Hostname ssh.github.com\n    Port 443\n    User git\n    # 30327 is your local proxy port\n    ProxyCommand nc -X connect -x 127.0.0.1:30327 %h %p\n</code></pre>"},{"location":"VCS/git/#test-test-ssh-connect","title":"Test Test Ssh Connect","text":"<p>you may need to enter yes as it's requiring to trust connection to github</p> <pre><code>ssh -T git@github.com\n</code></pre>"},{"location":"VCS/git/#remote","title":"Remote","text":"<p>list remote repo</p> <pre><code>git remote -v\n</code></pre> <p>add remote</p> <pre><code>git init\ngit add .\ngit commit -m \"Initial commit\"\ngit remote add origin &lt;remote_repo_URL&gt;\n</code></pre> <p>associate remote</p> <pre><code>git push -u origin main\n</code></pre> <p>set https to ssh</p> <pre><code>git remote set-url origin &lt;remote_repo_URL&gt;\n</code></pre>"},{"location":"VCS/git/#remove-track","title":"Remove Track","text":"<pre><code>git rm --cached &lt;file_path_or_folder&gt;\n# if folder add -r\n</code></pre>"},{"location":"VCS/git/#roll-back","title":"Roll back","text":"<pre><code># force roll back some commit\ngit reset --hard &lt;commit hash&gt;\n# force push remote\ngit push origin main -f\n</code></pre>"},{"location":"VCS/git/#sync-from-github","title":"Sync from Github","text":"<pre><code>git fetch origin\ngit reset --hard origin/main\ngit clean -fd\n</code></pre>"},{"location":"VCS/git/#submodule","title":"Submodule","text":"<p>clone with submodule</p> <pre><code>git clone --recurse-submodules &lt;repository-url&gt;\n# if forget to with params --recurse-submodules try following\ngit submodule update --init --recursive\n</code></pre> <p>add submodule or clone in main as submodule</p> <pre><code>git submodule add &lt;repository-url&gt; &lt;sub_repo_relative_path_to_root&gt;\n# if no .gitsubmodules appears try it\ngit submodule update --init --recursive\n</code></pre> <p>remove</p> <pre><code>rm -rf third_party/GS_ICP_SLAM\nrm -rf .git/modules/third_party/GS_ICP_SLAM\ngit config --remove-section submodule.third_party/GS_ICP_SLAM\n</code></pre>"},{"location":"VCS/git/#modify-git-commit-messages","title":"Modify Git Commit Messages","text":"<p>open rebase editor with 4 latest commit</p> <pre><code>git rebase -i HEAD~4\n</code></pre> <p>replace <code>pick</code> with <code>edit</code> and save. start to edit commit message.</p> <pre><code>git commit --amend\n</code></pre> <p>save the results</p> <pre><code>git rebase --continue\ngit push --force\n</code></pre>"},{"location":"VCS/git/#update-pr-from-main","title":"Update PR from Main","text":"<pre><code>git checkout -b backup-branch\n# under your-feature-branch\ngit checkout your-feature-branch\n# rest to main\ngit fetch upstream\ngit reset --hard upstream/dev-next\n# pick new commit\ngit cherry-pick backup-branch\ngit push -f origin\n</code></pre>"},{"location":"VCS/git/#tags","title":"Tags","text":"<p>delete latest tag</p> <pre><code>git tag -d $(git describe --abbrev=0 --tags)\n# check if deleted\ngit tag -l\ngit push origin main --tags -f\n</code></pre> <p>add tags via <code>vscode</code> git plugin</p>"},{"location":"VCS/git/#lfs","title":"LFS","text":"<pre><code>git lfs install\ngit lfs track \"*.onnx\"\ngit add .gitattributes\ngit add  \"&lt;file_path&gt;.onnx\"\ngit commit -m \"add extractor model .onnx\"\ngit push origin main\n</code></pre>"},{"location":"VCS/github/SSH_keypair_setup_for_GitHub/","title":"SSH","text":"<p>system: win11 in admin powershell</p>"},{"location":"VCS/github/SSH_keypair_setup_for_GitHub/#start-ssh-agent","title":"Start Ssh-agent","text":"<pre><code>Get-Service -Name ssh-agent | Set-Service -StartupType Manual\nStart-Service ssh-agent\n</code></pre>"},{"location":"VCS/github/SSH_keypair_setup_for_GitHub/#generate-key-pair","title":"Generate Key-pair","text":"<pre><code>ssh-keygen -t ed25519 -C \"1831768457@qq.com\"\n</code></pre> <p>in normal powershell</p>"},{"location":"VCS/github/SSH_keypair_setup_for_GitHub/#add-ssh-key-pair-into-ssh-agent","title":"Add Ssh-key-pair into Ssh-agent","text":"<pre><code>ssh-add C:\\Users\\18317\\.ssh\\id_ed25519\n</code></pre>"},{"location":"VCS/github/SSH_keypair_setup_for_GitHub/#copy-id_ed25519pub-into-github_ssh_setting-set-auth-key-type","title":"Copy id_ed25519.pub into github_ssh_setting Set Auth Key Type","text":"<pre><code>cat ~/.ssh/id_ed25519.pub | clip\n</code></pre>"},{"location":"VCS/github/SSH_keypair_setup_for_GitHub/#test-ssh-connect-by-https","title":"Test Ssh Connect by Https","text":"<pre><code>ssh -T -p 443 git@ssh.github.com\n</code></pre>"},{"location":"VCS/github/SSH_keypair_setup_for_GitHub/#add-config","title":"Add Config","text":"<p>add the following text into <code>~/.ssh/config</code></p> <pre><code>Host github.com\n    Hostname ssh.github.com\n    Port 443\n    User git\n</code></pre>"},{"location":"VCS/github/SSH_keypair_setup_for_GitHub/#test-test-ssh-connect","title":"Test Test Ssh Connect","text":"<p>you may need to enter yes as it's requiring to trust connection to github</p> <pre><code>ssh -T git@github.com\n</code></pre> <p>Generating a new SSH key and adding it to the ssh-agent - GitHub Docs</p>"},{"location":"tools/IDE/JetBrains/","title":"JetBrains","text":""},{"location":"tools/IDE/JetBrains/#_1","title":"\u6fc0\u6d3b\u5de5\u5177","text":"<p>Linux\u6fc0\u6d3b\u6559\u7a0b - Feishu Docs</p> <ul> <li>\u4e0b\u8f7d\u5730\u5740  https://wwkh.lanzout.com/b03jufqhg  \u5bc6\u7801:7bou</li> </ul> <pre><code>chmod +x scripts/install.sh\n./scripts/install.sh\n</code></pre> <ul> <li>win \u7cfb\u7edf\u51fa\u73b0\u201cKey is invalid\u201d\u89e3\u51b3\u65b9\u6cd5,Linux \u7cfb\u7edf\u6309\u6559\u7a0b\u6b65\u9aa4\u627e Linux \u6587\u4ef6\u5939\u91cc\u6587\u4ef6  \u51fa\u73b0\u201cKey is invalid\u201d\u89e3\u51b3\u65b9\u6cd5 - Feishu Docs</li> </ul>"},{"location":"tools/IDE/JetBrains/#_2","title":"\u65ad\u7f51\u6fc0\u6d3b","text":"<p>\u6b63\u7248\u6fc0\u6d3b\u7801\uff1a https://hwmgu1yf37x.feishu.cn/docx/Y0PGdrvC2orriUxrFBxcavPxnqH</p> <p></p> <pre><code>KQ8KMJ77TY-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-QKJUeHFkc+NaPWlEFZFGpoBJBYjehR5cGPezKK9BKHVnVaydzLV4YSAnILt8mz9twXw9lIh0k/HivsPKrffP8F9gZkWA/rfjieSI0jziDr9WBARPzYKRlQHSw/iZn5VUn6zIR9U7uJC6Kd/jiaeLumn+dzL/ia9B/1dBUIg5WQlIOtld4xx2xR0gb4JCNBd4kQMV4SAC3Og13/APGkDiP7KzDz7T3DxmpSKvjAfG1Hg1jn2pt5B/3gmhOK5lmJKbGBDRW40f4sqyDpzXsA5DaPAAaFT07GSL5FlfdKfngGjcQdwQ18k1iFET6wSwWkk+p+OySDqegpFw3wx2Kzj+Ow==-MIIETDCCAjSgAwIBAgIBDzANBgkqhkiG9w0BAQsFADAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBMB4XDTIyMTAxMDE2MDU0NFoXDTI0MTAxMTE2MDU0NFowHzEdMBsGA1UEAwwUcHJvZDJ5LWZyb20tMjAyMjEwMTAwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC/W3uCpU5M2y48rUR/3fFR6y4xj1nOm3rIuGp2brELVGzdgK2BezjnDXpAxVDw5657hBkAUMoyByiDs2MgmVi9IcqdAwpk988/Daaajq9xuU1of59jH9eQ9c3BmsEtdA4boN3VpenYKATwmpKYkJKVc07ZKoXL6kSyZuF7Jq7HoQZcclChbF75QJPGbri3cw9vDk/e46kuzfwpGftvl6+vKibpInO6Dv0ocwImDbOutyZC7E+BwpEm1TJZW4XovMBegHhWC04cJvpH1u98xoR94ichw0jKhdppywARe43rGU96163RckIuFmFDQKZV9SMUrwpQFu4Z2D5yTNqnlLRfAgMBAAGjgZkwgZYwCQYDVR0TBAIwADAdBgNVHQ4EFgQU5FZqQ4gnVc+inIeZF+o3ID+VhcEwSAYDVR0jBEEwP4AUo562SGdCEjZBvW3gubSgUouX8bOhHKQaMBgxFjAUBgNVBAMMDUpldFByb2ZpbGUgQ0GCCQDSbLGDsoN54TATBgNVHSUEDDAKBggrBgEFBQcDATALBgNVHQ8EBAMCBaAwDQYJKoZIhvcNAQELBQADggIBANLG1anEKid4W87vQkqWaQTkRtFKJ2GFtBeMhvLhIyM6Cg3FdQnMZr0qr9mlV0w289pf/+M14J7S7SgsfwxMJvFbw9gZlwHvhBl24N349GuthshGO9P9eKmNPgyTJzTtw6FedXrrHV99nC7spaY84e+DqfHGYOzMJDrg8xHDYLLHk5Q2z5TlrztXMbtLhjPKrc2+ZajFFshgE5eowfkutSYxeX8uA5czFNT1ZxmDwX1KIelbqhh6XkMQFJui8v8Eo396/sN3RAQSfvBd7Syhch2vlaMP4FAB11AlMKO2x/1hoKiHBU3oU3OKRTfoUTfy1uH3T+t03k1Qkr0dqgHLxiv6QU5WrarR9tx/dapqbsSmrYapmJ7S5+ghc4FTWxXJB1cjJRh3X+gwJIHjOVW+5ZVqXTG2s2Jwi2daDt6XYeigxgL2SlQpeL5kvXNCcuSJurJVcRZFYUkzVv85XfDauqGxYqaehPcK2TzmcXOUWPfxQxLJd2TrqSiO+mseqqkNTb3ZDiYS/ZqdQoGYIUwJqXo+EDgqlmuWUhkWwCkyo4rtTZeAj+nP00v3n8JmXtO30Fip+lxpfsVR3tO1hk4Vi2kmVjXyRkW2G7D7WAVt+91ahFoSeRWlKyb4KcvGvwUaa43fWLem2hyI4di2pZdr3fcYJ3xvL5ejL3m14bKsfoOv\n</code></pre> <p>Code completion | PyCharm Documentation</p>"},{"location":"tools/IDE/VSCode/","title":"VSCode","text":""},{"location":"tools/IDE/VSCode/#install","title":"Install","text":"<pre><code>sudo apt-get install wget gpg\nwget -qO- https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor &gt; packages.microsoft.gpg\nsudo install -D -o root -g root -m 644 packages.microsoft.gpg /etc/apt/keyrings/packages.microsoft.gpg\necho \"deb [arch=amd64,arm64,armhf signed-by=/etc/apt/keyrings/packages.microsoft.gpg] https://packages.microsoft.com/repos/code stable main\" |sudo tee /etc/apt/sources.list.d/vscode.list &gt; /dev/null\nrm -f packages.microsoft.gpg\nsudo apt install apt-transport-https\nsudo apt update\nsudo apt install code\n</code></pre>"},{"location":"tools/IDE/VSCode/#shortcut","title":"Shortcut","text":""},{"location":"tools/IDE/VSCode/#terminal","title":"Terminal","text":"<ol> <li>open|close terminal ctrl+`</li> <li>create and open new terminal ctrl+shift +`</li> </ol>"},{"location":"tools/IDE/VSCode/#variables-reference","title":"variables-reference","text":""},{"location":"tools/IDE/VSCode/#common","title":"Common","text":"<pre><code>${env:HOME}\n</code></pre>"},{"location":"tools/IDE/VSCode/#predefined-variables","title":"Predefined Variables","text":"<p>The following predefined variables are supported:</p> <ul> <li>${userHome} - the path of the user's home folder</li> <li>${workspaceFolder} - the path of the folder opened in VS Code</li> <li>${workspaceFolderBasename} - the name of the folder opened in VS Code without any slashes (/)</li> <li>${file} - the current opened file</li> <li>${fileWorkspaceFolder} - the current opened file's workspace folder</li> <li>${relativeFile} - the current opened file relative to <code>workspaceFolder</code></li> <li>${relativeFileDirname} - the current opened file's dirname relative to <code>workspaceFolder</code></li> <li>${fileBasename} - the current opened file's basename</li> <li>${fileBasenameNoExtension} - the current opened file's basename with no file extension</li> <li>${fileExtname} - the current opened file's extension</li> <li>${fileDirname} - the current opened file's folder path</li> <li>${fileDirnameBasename} - the current opened file's folder name</li> <li>${cwd} - the task runner's current working directory upon the startup of VS Code</li> <li>${lineNumber} - the current selected line number in the active file</li> <li>${selectedText} - the current selected text in the active file</li> <li>${execPath} - the path to the running VS Code executable</li> <li>${defaultBuildTask} - the name of the default build task</li> <li>${pathSeparator} - the character used by the operating system to separate components in file paths</li> <li>${/} - shorthand for ${pathSeparator}</li> </ul>"},{"location":"tools/IDE/VSCode/#predefined-variables-examples","title":"Predefined Variables Examples","text":"<p>Supposing that you have the following requirements:</p> <ol> <li>A file located at <code>/home/your-username/your-project/folder/file.ext</code> opened in your editor;</li> <li>The directory <code>/home/your-username/your-project</code> opened as your root workspace.</li> </ol> <p>So you will have the following values for each variable:</p> <ul> <li>${userHome} - <code>/home/your-username</code></li> <li>${workspaceFolder} - <code>/home/your-username/your-project</code></li> <li>${workspaceFolderBasename} - <code>your-project</code></li> <li>${file} - <code>/home/your-username/your-project/folder/file.ext</code></li> <li>${fileWorkspaceFolder} - <code>/home/your-username/your-project</code></li> <li>${relativeFile} - <code>folder/file.ext</code></li> <li>${relativeFileDirname} - <code>folder</code></li> <li>${fileBasename} - <code>file.ext</code></li> <li>${fileBasenameNoExtension} - <code>file</code></li> <li>${fileDirname} - <code>/home/your-username/your-project/folder</code></li> <li>${fileExtname} - <code>.ext</code></li> <li>${lineNumber} - line number of the cursor</li> <li>${selectedText} - text selected in your code editor</li> <li>${execPath} - location of Code.exe</li> <li>${pathSeparator} - <code>/</code> on macOS or linux, <code>\\</code> on Windows</li> </ul> <p>Tip</p> <p>Use IntelliSense inside string values for <code>tasks.json</code> and <code>launch.json</code> to get a full list of predefined variables.</p>"},{"location":"tools/IDE/VSCode/#variables-scoped-per-workspace-folder","title":"Variables Scoped per Workspace Folder","text":"<p>By appending the root folder's name to a variable (separated by a colon), it is possible to reach into sibling root folders of a workspace. Without the root folder name, the variable is scoped to the same folder where it is used.</p> <p>For example, in a multi root workspace with folders <code>Server</code> and <code>Client</code>, a <code>${workspaceFolder:Client}</code> refers to the path of the <code>Client</code> root.</p>"},{"location":"tools/IDE/VSCode/#environment-variables","title":"Environment Variables","text":"<p>You can also reference environment variables through the <code>${env:Name}</code> syntax (for example, <code>${env:USERNAME}</code>).</p> <pre><code>{\n  \"type\": \"node\",\n  \"request\": \"launch\",\n  \"name\": \"Launch Program\",\n  \"program\": \"${workspaceFolder}/app.js\",\n  \"cwd\": \"${workspaceFolder}\",\n  \"args\": [\"${env:USERNAME}\"]\n}\n</code></pre>"},{"location":"tools/IDE/VSCode/#configuration-variables","title":"Configuration Variables","text":"<p>You can reference VS Code settings (\"configurations\") through <code>${config:Name}</code> syntax (for example, <code>${config:editor.fontSize}</code>).</p>"}]}